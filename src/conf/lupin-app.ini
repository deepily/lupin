[default]

foo = bar

#######################################################################################################################
[Lupin: Production]
#######################################################################################################################
inherits                          = Lupin: Baseline

tts_generation_strategy           = openai

#######################################################################################################################
[Lupin: Development]
#######################################################################################################################
inherits                          = Lupin: Baseline

app_config_server_name            = 127.0.0.1:7999

agent_function_mapping_tools_path_wo_root = /src/conf/tools/search.xml

;agent_model_name_for_calendaring     = Groq/mixtral-8x7b-32768
agent_model_name_for_calendaring      = Deepily/kaitchup/Phi-4-AutoRound-GPTQ-4bit
agent_model_name_for_date_and_time    = Deepily/kaitchup/Phi-4-AutoRound-GPTQ-4bit
agent_model_name_for_math             = Deepily/kaitchup/Phi-4-AutoRound-GPTQ-4bit
;agent_model_name_for_math              = Groq/llama-3.1-8b-instant
;agent_model_name_for_math             = Deepily/kaitchup/Qwen2.5-Coder-32B-Instruct-AutoRound-GPTQ-4bit
;agent_model_name_for_math             = Groq/llama-3.1-70b-versatile
;agent_model_name_for_debugger        = Deepily/kaitchup/Phi-4-AutoRound-GPTQ-4bit
agent_model_name_for_debugger         = Groq/llama2-70b-4096
;agent_model_name_for_weather          = OpenAI/gpt-4-0613
;agent_model_name_for_weather          = OpenAI/gpt-3.5-turbo-1106
agent_model_name_for_weather          = Deepily/kaitchup/Phi-4-AutoRound-GPTQ-4bit
agent_model_name_for_todo_list        = Deepily/kaitchup/Phi-4-AutoRound-GPTQ-4bit
agent_model_name_for_receptionist     = Deepily/kaitchup/Phi-4-AutoRound-GPTQ-4bit
;agent_model_name_for_receptionist     = Groq/mixtral-8x7b-32768
;agent_model_name_for_todo_list        = OpenAI/gpt-3.5-turbo-1106
agent_model_name_for_bug_injector     = Deepily/kaitchup/Phi-4-AutoRound-GPTQ-4bit
;agent_model_name_for_function_mapping = Deepily/kaitchup/Phi-4-AutoRound-GPTQ-4bit
agent_model_name_for_function_mapping  = OpenAI/gpt-4-turbo-2024-04-09
agent_model_name_for_math_refactoring  = Groq/llama-3.1-70b-versatile

agent_prompt_for_calendaring         = /src/conf/prompts/agents/calendaring.txt
agent_prompt_for_date_and_time       = /src/conf/prompts/agents/date-and-time.txt
agent_prompt_for_math                = /src/conf/prompts/agents/math.txt
agent_prompt_for_receptionist        = /src/conf/prompts/agents/receptionist.txt
agent_prompt_for_weather             = /src/conf/prompts/agents/weather.txt
agent_prompt_for_todo_list           = /src/conf/prompts/agents/todo-lists.txt
agent_prompt_for_debugger            = /src/conf/prompts/agents/debugger.txt
agent_prompt_for_debugger_minimalist = /src/conf/prompts/agents/debugger-minimalist.txt
agent_prompt_for_bug_injector        = /src/conf/prompts/agents/bug-injector.txt
agent_prompt_for_function_mapping    = /src/conf/prompts/agents/function-mapping.txt
agent_prompt_for_math_refactoring    = /src/conf/prompts/agents/math-refactoring.txt

agent_todo_list_serialize_prompt_to_json = False
agent_todo_list_serialize_code_to_json   = False

agent_receptionist_serialize_prompt_to_json = False
;agent_receptionist_serialize_code_to_json   = False

database_path_wo_root            = /src/conf/long-term-memory/lupin.lancedb

;formatter_model_name_for_calendaring   = OpenAI/gpt-4-0613
;formatter_model_name_for_calendaring   = Groq/llama2-70b-4096
;formatter_model_name_for_calendaring     = Groq/llama3-70b-8192
formatter_model_name_for_calendaring      = Deepily/kaitchup/Phi-4-AutoRound-GPTQ-4bit
formatter_model_name_for_date_and_time    = Deepily/kaitchup/Phi-4-AutoRound-GPTQ-4bit
;formatter_model_name_for_date_and_time  = Groq/llama3-70b-8192
;formatter_model_name_for_math           = Groq/llama3-70b-8192
formatter_model_name_for_math            = Deepily/kaitchup/Phi-4-AutoRound-GPTQ-4bit
;formatter_model_name_for_weather        = OpenAI/gpt-3.5-turbo-1106
;formatter_model_name_for_weather        = Deepily/kaitchup/Phi-4-AutoRound-GPTQ-4bit
;formatter_model_name_for_weather        = Groq/mixtral-8x7b-32768
;formatter_model_name_for_weather         = Groq/llama3-70b-8192
formatter_model_name_for_weather          = Deepily/kaitchup/Phi-4-AutoRound-GPTQ-4bit
;formatter_model_name_for_todo_list     = Deepily/kaitchup/Phi-4-AutoRound-GPTQ-4bit
;formatter_model_name_for_todo_list      = OpenAI/gpt-3.5-turbo-1106
;formatter_model_name_for_todo_list       = Groq/llama3-70b-8192
formatter_model_name_for_todo_list        = Deepily/kaitchup/Phi-4-AutoRound-GPTQ-4bit
;formatter_model_name_for_receptionist   = Groq/mixtral-8x7b-32768
;formatter_model_name_for_receptionist    = Groq/llama3-70b-8192
formatter_model_name_for_receptionist     = Deepily/kaitchup/Phi-4-AutoRound-GPTQ-4bit

formatter_prompt_for_math_terse    = True
formatter_prompt_for_calendaring   = /src/conf/prompts/formatters/calendaring.txt
formatter_prompt_for_date_and_time = /src/conf/prompts/formatters/date-and-time.txt
formatter_prompt_for_math          = /src/conf/prompts/formatters/math.txt
formatter_prompt_for_weather       = /src/conf/prompts/formatters/weather.txt
formatter_prompt_for_todo_list     = /src/conf/prompts/formatters/todo-list.txt
formatter_prompt_for_receptionist  = /src/conf/prompts/formatters/receptionist.txt

# TODO: add more models for the debugger!
llm_model_keys_for_debugger        = [ "kaitchup/phi_4_14b" ]

######################################
# v010 LLM configurations here!
######################################
deepily/ministral_8b_2410_ft_lora        = vllm://192.168.1.21:3000@/mnt/DATA01/include/www.deepily.ai/projects/models/Ministral-8B-Instruct-2410.lora/merged-on-2025-02-12-at-02-05/autoround-4-bits-sym.gptq/2025-02-12-at-02-27
deepily/ministral_8b_2410_ft_lora_params = { "prompt_format": "instruction_completion", "model_name": "Ministral-8B-Instruct-2410", "temperature": 0.25, "top_k": 10, "top_p": 0.25, "max_new_tokens": 254, "stop_sequence": [ "</s>", "</response>" ], "stream": True }

mistralai/Ministral-8B-Instruct-2410        = vllm://192.168.1.21:3000@mistralai/Ministral-8B-Instruct-2410
mistralai/Ministral-8B-Instruct-2410_params = { "prompt_format": "instruction_completion", "model_name": "Ministral-8B-Instruct-2410", "temperature": 0.25, "top_k": 10, "top_p": 0.25, "max_new_tokens": 254, "stop_sequence": [ "</s>", "</response>" ], "stream": True }

kaitchup/phi_4_14b                        = vllm://192.168.1.21:3001@kaitchup/Phi-4-AutoRound-GPTQ-4bit
kaitchup/phi_4_14b_params                 = { "prompt_format": "instruction_completion", "model_name": "Phi-4-AutoRound-GPTQ-4bit", "temperature": 0.25, "top_k": 10, "top_p": 0.25, "max_tokens": 4096, "stop_sequence": [ "</s>", "</response>" ], "stream": True }

######################################
# v010 LLM agent configurations here!
######################################

# Prompt template paths for agent routing commands
prompt template for agent router                        = /src/conf/prompts/agent-router-template-completion.txt
prompt template for agent router go to date and time    = /src/conf/prompts/agents/date-and-time.txt
prompt template for agent router go to math             = /src/conf/prompts/agents/math.txt
prompt template for agent router go to calendar         = /src/conf/prompts/agents/calendaring.txt
prompt template for agent router go to weather          = /src/conf/prompts/agents/weather.txt
prompt template for agent router go to todo list        = /src/conf/prompts/agents/todo-lists.txt
prompt template for agent router go to receptionist     = /src/conf/prompts/agents/receptionist.txt
prompt template for agent router go to debugger         = /src/conf/prompts/agents/debugger.txt
prompt template for agent router go to bug injector     = /src/conf/prompts/agents/bug-injector.txt
prompt template for agent router go to function mapping = /src/conf/prompts/agents/function-mapping.txt
prompt template for agent router go to math refactoring = /src/conf/prompts/agents/math-refactoring.txt

# Other prompt template paths
prompt template for confirmation dialog                 = /src/conf/prompts/agents/confirmation-yes-no.txt
prompt template for gist generation                     = /src/conf/prompts/agents/gist.txt

# models to use for agents reached via `agent router`
llm spec key for agent router                        = deepily/ministral_8b_2410_ft_lora
llm spec key for agent router go to date and time    = kaitchup/phi_4_14b
llm spec key for agent router go to math             = kaitchup/phi_4_14b
llm spec key for agent router go to calendar         = kaitchup/phi_4_14b
llm spec key for agent router go to weather          = kaitchup/phi_4_14b
llm spec key for agent router go to todo list        = kaitchup/phi_4_14b
llm spec key for agent router go to receptionist     = groq:llama-3.1-8b-instant
llm spec key for agent router go to debugger         = kaitchup/phi_4_14b
llm spec key for agent router go to bug injector     = kaitchup/phi_4_14b
llm spec key for agent router go to function mapping = kaitchup/phi_4_14b
llm spec key for agent router go to math refactoring = kaitchup/phi_4_14b

# other LLM spec keys
llm spec key for gist generation                     = kaitchup/phi_4_14b
llm spec key for confirmation dialog                 = groq:llama-3.3-70b-versatile

# topics for `agent router` serialization
serialization topic for agent router go to date and time    = date-and-time
serialization topic for agent router go to math             = math
serialization topic for agent router go to calendar         = calendar
serialization topic for agent router go to weather          = weather
serialization topic for agent router go to todo list        = todo-list
serialization topic for agent router go to receptionist     = receptionist
serialization topic for agent router go to debugger         = code-debugger
serialization topic for agent router go to bug injector     = bug-injector
serialization topic for agent router go to function mapping = function-mapping
serialization topic for agent router go to refactoring      = refactoring

# formatter templates for agent routing commands
formatter template for agent router go to date and time     = /src/conf/prompts/formatters/date-and-time.txt
formatter template for agent router go to math              = /src/conf/prompts/formatters/math.txt
formatter template for agent router go to calendar          = /src/conf/prompts/formatters/calendaring.txt
formatter template for agent router go to weather           = /src/conf/prompts/formatters/weather.txt
formatter template for agent router go to todo list         = /src/conf/prompts/formatters/todo-list.txt
formatter template for agent router go to receptionist      = /src/conf/prompts/formatters/receptionist.txt

# formatter model specs for agent routing commands
formatter llm spec for agent router go to date and time     = kaitchup/phi_4_14b
formatter llm spec for agent router go to math              = groq:llama-3.1-8b-instant
formatter llm spec for agent router go to calendar          = groq:llama-3.1-8b-instant
formatter llm spec for agent router go to weather           = groq:llama-3.1-8b-instant
formatter llm spec for agent router go to todo list         = groq:llama-3.1-8b-instant
formatter llm spec for agent router go to receptionist      = groq:llama-3.1-8b-instant


prompt_format_template_directory            = src/conf/prompts/v1/llms

# Default format if not specified
prompt_format_default                       = json_message

# LLM family default formats
prompt_format_default_openai                = json_message
prompt_format_default_groq                  = json_message
prompt_format_default_anthropic             = json_message
prompt_format_default_phi                   = special_token
prompt_format_default_mistral               = instruction_completion
prompt_format_default_llama                 = instruction_completion

# Model-specific prompt formats
prompt_format_llm_deepily_ministral_8b_2410 = instruction_completion
prompt_format_llm_deepily_phi_4_14b         = special_token
prompt_format_groq_llama_3_1_8b             = json_message




path_to_debugger_prompts_wo_root  = /src/conf/prompts/iterative-agents/debugger/

path_to_events_df_wo_root         = /src/conf/long-term-memory/events.csv
path_to_event_prompts_wo_root     = /src/conf/prompts/incremental-agents/events/

path_to_snapshots_dir_wo_root     = /src/conf/long-term-memory/solutions/

path_to_todolist_df_wo_root       = /src/conf/long-term-memory/todo.csv
path_to_todolist_prompts_wo_root  = /src/conf/prompts/incremental-agents/todo-lists/

# Toggling between these two values will change the behavior: plain text will be processed to create the XML's response object, while XML will be used as is.
path_to_search_function_mapping_data_wo_root = /src/ephemera/prompts/data/synthetic-data-agent-search-static-vs-dynamic.txt
;path_to_search_function_mapping_data_wo_root = /src/ephemera/prompts/data/synthetic-data-agent-search-static-vs-dynamic.xml

# Code execution file path
code_execution_file_path         = /io/code.py

# These values are used for the similarity threshold calculations
similarity_threshold_question      = 95.0
similarity_threshold_question_gist = 90.0
similarity_threshold_confirmation  = 98.0

stt_device_id                     = cuda:0
stt_model_id                      = distil-whisper/distil-large-v3

tts_local_url_template            = http://192.168.0.188:5002/api/tts?text={tts_text}
tts_generation_strategy           = openai

;deepily_inference_chat_url       = http://172.17.0.3:3001
;deepily_inference_chat_url       = http://172.17.0.3:3000
;deepily_inference_chat_url        = http://192.168.1.21:3001/v1/chat/completions
;deepily_inference_completions_url = http://192.168.1.21:3001/v1/completions

;vox_command_llm_name              = Mistral-7B-Instruct-v0.2.AWQ (2024.09.22)
;vox_command_llm_path_wo_root      = /models/Mistral-7B-Instruct-v0.2/merged-00-2024.09.22.awq/
;vox_command_llm_device_map        = cuda:0
;vox_command_prompt_path_wo_root   = /src/conf/prompts/vox-command-template.txt
vox_command_prompt_path_wo_root   = /src/conf/prompts/vox-command-template-completion.txt

router_and_vox_command_llm_name              = Mistral-8B-Instruct-2410.lora (2025.02.12-at-02-27)
router_and_vox_command_is_completion         = True
# router_and_vox_command_model                 = Deepily//mnt/DATA01/include/www.deepily.ai/projects/models/Ministral-8B-Instruct-2410.lora/merged-on-2025-02-12-at-02-05/autoround-4-bits-sym.gptq/2025-02-12-at-02-27
router_and_vox_command_model                 = deepily/ministral_8b_2410_ft_lora
router_and_vox_command_prompt_path_wo_root   = /src/conf/prompts/vox-command-template-completion.txt

embedding model name = text-embedding-3-small

# Normalizer settings
spacy model name = en_core_web_sm
expand symbols to words = False
async embedding generation = True
debug text truncation length = 48


#######################################################################################################################
[Lupin: Baseline]
#######################################################################################################################

app_debug                         = True
app_verbose                       = True
app_silent                        = False
app_timezone                      = America/New_York

auto_debug                        = True
inject_bugs                       = False

tts_generation_strategy           = local