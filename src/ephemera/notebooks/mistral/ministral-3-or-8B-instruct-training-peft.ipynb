{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbf3ea6b1a88c0ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T02:01:31.361705Z",
     "start_time": "2025-02-14T02:01:20.324201Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/var/model/genie-in-the-box/src/ephemera/notebooks/mistral\n",
      "/var/model/genie-in-the-box/src\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-14 02:01:31,317 INFO schemas.py L1274: Include schema from 'file:///usr/local/lib/python3.10/dist-packages/xmlschema/schemas/XSD_1.1/xsd11-extra.xsd'\n"
     ]
    }
   ],
   "source": [
    "# auto reloads source code modules\n",
    "%load_ext autoreload\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import gc\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
    "\n",
    "models_root = \"/var/model/models\"\n",
    "gib_root    = \"/var/model/genie-in-the-box\"\n",
    "\n",
    "print( os.getcwd() )\n",
    "os.chdir( f\"/{gib_root}/src\" )\n",
    "print( os.getcwd() )\n",
    "\n",
    "import cosa.utils.util          as du\n",
    "import cosa.utils.util_xml      as dux\n",
    "import cosa.utils.util_pytorch  as dupt\n",
    "from cosa.agents.llm            import Llm\n",
    "from cosa.training.peft_trainer import PeftTrainer\n",
    "\n",
    "os.environ[ \"NCCL_P2P_DISABLE\" ] = \"1\"\n",
    "os.environ[ \"NCCL_IB_DISABLE\"  ] = \"1\"\n",
    "os.environ[ \"WANDB_DISABLED\"   ] = \"true\"\n",
    "\n",
    "def reset_models( models ):\n",
    "\n",
    "    for model in models:\n",
    "        del model\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "def reset_notebook_kernel():\n",
    "\n",
    "    from IPython import get_ipython\n",
    "    get_ipython().kernel.do_shutdown( restart=True )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48975a85a6e74a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_models( [] )\n",
    "reset_notebook_kernel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4334895984e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e918a40dc02318",
   "metadata": {},
   "source": [
    "## Instantiate trainer for Ministral-8B-Instruct-2410"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba48805b2eaf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "\n",
    "# model_id_or_path = f\"{models_root}/Ministral-8B-Instruct-2410-autoround-4-bits-sym.gptq/2025-01-24-at-20-48\"\n",
    "model_id_or_path   = \"mistralai/Ministral-8B-Instruct-2410\"\n",
    "model_name         = \"Ministral-8B-Instruct-2410\"\n",
    "test_train_path    = f\"{gib_root}/src/ephemera/prompts/data\"\n",
    "lora_dir           = f\"{models_root}/{model_name}.lora\"\n",
    "\n",
    "trainer = PeftTrainer( model_id_or_path, model_name, test_train_path, lora_dir=lora_dir, debug=True )\n",
    "\n",
    "trainer.set_hf_env_vars()\n",
    "trainer.set_gib_env_vars( gib_root=gib_root )\n",
    "trainer.login_to_hf()\n",
    "\n",
    "gib_root"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59911265fcf27aef",
   "metadata": {},
   "source": [
    "### get training prompt stats, _ONCE_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daff148faf757865",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "token_stats, _ = trainer.get_training_prompt_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b557e6cdb5aba1a7",
   "metadata": {},
   "source": [
    "{'min': 373, 'max': 683, 'mean': 589.1221287097386}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0635651b82b0143",
   "metadata": {},
   "source": [
    "## Fine tune it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ffcaf83ac9c4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "# baby batch...\n",
    "trainer.fine_tune( sample_size=0.005, batch_size=2, gradient_accumulation_steps=8, logging_steps=0.05, eval_steps=0.20, device_map=\"auto\", output_dir=lora_dir )\n",
    "# Half batch (sample_size=0.5) should really be full batch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a968e636cfbb5fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.checkpoint_dir = \"/var/model/models/Ministral-8B-Instruct-2410.lora/training-2025-02-11-at-18-33/checkpoint-9\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db4b39f8f7171ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.lora_dir = lora_dir\n",
    "trainer.lora_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a10fa9c0b62d1fd",
   "metadata": {},
   "source": [
    "## load an quantized model and merge with adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf5ab03a373baa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "\n",
    "# checkpoint_dir = f\"{models_root}/Ministral-8B-Instruct-2410.lora/training-2025-02-06-at-21-28/checkpoint-987\"\n",
    "# du.print_simple_file_list( lora_dir )\n",
    "\n",
    "# we can get the last checkpoint directory only after a fine-tuning run has finished, Leslie:\n",
    "# checkpoint_dir = trainer.get_last_checkpoint_dir()\n",
    "\n",
    "trainer.load_and_merge_adapter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde8e4b4a060c663",
   "metadata": {},
   "source": [
    "## Save the merged model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6643f5136c374157",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "merged_adapter_dir = trainer.save_merged_adapter( lora_dir )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deba191b375cc987",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls -alh /var/model/models/Ministral-8B-Instruct-2410.lora/merged-on-2025-02-08-at-16-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b769c167ff78c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear models from memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4fc0340ed81ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "merged_adapter_dir = \"/var/model/models/Ministral-8B-Instruct-2410.lora/merged-on-2025-02-08-at-16-16\"\n",
    "# release_gpus( [] )\n",
    "# reset_notebook_kernel()\n",
    "# trainer.quantize_merged_adapter()\n",
    "trainer.quantize_merged_adapter( merged_adapter_dir=merged_adapter_dir )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b50f3d92a16ddf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model_dir = \"/var/model/models/Ministral-8B-Instruct-2410.lora/merged-on-2025-02-08-at-16-16/autoround-4-bits-sym.gptq/2025-02-11-at-21-12\"\n",
    "quantized_model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4837a1083f009000",
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_notebook_kernel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ce19f282d769fe",
   "metadata": {},
   "source": [
    "## Validate merged model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b2c66c7a751969b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T02:40:08.842278Z",
     "start_time": "2025-02-14T02:40:08.196135Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Initializing PEFT Trainer for Ministral-8B-Instruct-2410\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Model ID: /var/model/models/Ministral-8B-Instruct-2410.lora/merged-on-2025-02-08-at-16-16/autoround-4-bits-sym.gptq/2025-02-11-at-21-12\n",
      "Path to test/train data: /var/model/genie-in-the-box/src/ephemera/prompts/data\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "\n",
    "# model_id         = \"/var/model/models/Ministral-8B-Instruct-2410.lora/merged-on-2025-02-08-at-16-16\"\n",
    "model_id         = \"/var/model/models/Ministral-8B-Instruct-2410.lora/merged-on-2025-02-08-at-16-16/autoround-4-bits-sym.gptq/2025-02-11-at-21-12\"\n",
    "model_name       = \"Ministral-8B-Instruct-2410\"\n",
    "test_train_path  = f\"{gib_root}/src/ephemera/prompts/data\"\n",
    "# lora_dir         = f\"{models_root}/{model_name}.lora\"\n",
    "\n",
    "trainer = PeftTrainer( model_id, model_name, test_train_path, debug=True )\n",
    "\n",
    "trainer.set_gib_env_vars( gib_root=gib_root )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125a49e104605ff4",
   "metadata": {},
   "source": [
    "## run validation, using standalone server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5a31b7cd8462dff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T02:11:34.971199Z",
     "start_time": "2025-02-14T02:11:34.966925Z"
    }
   },
   "outputs": [],
   "source": [
    "from cosa.agents.llm import Llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f2c3a2218d88e2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T02:20:29.811608Z",
     "start_time": "2025-02-14T02:20:29.376782Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Deepily//mnt/foo'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%autoreload\n",
    "# llm = Llm( switch=\"deepily\" )\n",
    "Llm.DEEPILY_PREFIX\n",
    "\n",
    "# llm = Llm( default_url=\"http://blah.blah.com\" )\n",
    "Llm.get_model( \"/mnt/foo\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d418a3f109309dcc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T02:41:12.375627Z",
     "start_time": "2025-02-14T02:41:12.372443Z"
    }
   },
   "outputs": [],
   "source": [
    "quantized_model_dir = \"/mnt/DATA01/include/www.deepily.ai/projects/models/Ministral-8B-Instruct-2410.lora/merged-on-2025-02-12-at-02-05/autoround-4-bits-sym.gptq/2025-02-12-at-02-27\"\n",
    "model = Llm.get_model( quantized_model_dir )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9abdbd4c4c96d7b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T02:41:18.574050Z",
     "start_time": "2025-02-14T02:41:17.003092Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Querying an LLM server w/ model [Deepily//mnt/DATA01/include/www.deepily.ai/projects/models/Ministral-8B-Instruct-2410.lora/merged-on-2025-02-12-at-02-05/autoround-4-bits-sym.gptq/2025-02-12-at-02-27]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Updating the prompt field for [100] rows...\n",
      "Updating the prompt field for [100] rows... Done!\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Validating Ministral-8B-Instruct-2410 w/ 100 samples...\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "command\n",
      "go to new tab                                        8\n",
      "search google new tab                                7\n",
      "search perplexity new tab                            7\n",
      "agent router go to calendar                          7\n",
      "search perplexity current tab                        7\n",
      "go to current tab                                    6\n",
      "search kagi new tab                                  6\n",
      "search google scholar current tab                    6\n",
      "search google current tab                            5\n",
      "search kagi current tab                              5\n",
      "search phind new tab                                 5\n",
      "agent router go to date and time                     4\n",
      "agent router go to receptionist                      4\n",
      "agent router go to weather                           3\n",
      "search google scholar new tab                        3\n",
      "search new tab                                       3\n",
      "agent router go to todo list                         3\n",
      "search current tab                                   2\n",
      "search google scholar using clipboard current tab    2\n",
      "agent router go to math                              2\n",
      "search phind current tab                             2\n",
      "search phind using clipboard new tab                 2\n",
      "search phind using clipboard current tab             1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Instantiating ConfigurationManager() singleton...\n",
      "\n",
      "Using environment variables to instantiate configuration manager\n",
      "[0]th arg = [config_path=/src/conf/gib-app.ini]... done!\n",
      "[1]th arg = [splainer_path=/src/conf/gib-app-splainer.ini]... done!\n",
      "[2]th arg = [config_block_id=Genie+in+the+Box:+Development]... done!\n",
      "\n",
      "Name value dictionary pairs:\n",
      "\n",
      "[ config_block_id] = [Genie in the Box: Development]\n",
      "[     config_path] = [/src/conf/gib-app.ini]\n",
      "[   splainer_path] = [/src/conf/gib-app-splainer.ini]\n",
      "\n",
      "File exists! [/var/model/genie-in-the-box/src/conf/gib-app.ini]\n",
      "File exists! [/var/model/genie-in-the-box/src/conf/gib-app-splainer.ini]\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Initializing configuration_manager [/var/model/genie-in-the-box/src/conf/gib-app.ini]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Splainer path [/var/model/genie-in-the-box/src/conf/gib-app-splainer.ini]\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Sections, '*' = current block ID\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "  Genie in the Box: Baseline\n",
      "* Genie in the Box: Development\n",
      "  Genie in the Box: Production\n",
      "  default\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Calculating inheritance... * = parent block\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "* [Genie in the Box: Development] inherits from [Genie in the Box: Baseline]\n",
      "Scanning for immutable keys...\n",
      "Scanning for immutable keys... Done!\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Calculating defaults...\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Loading splainer file [/var/model/genie-in-the-box/src/conf/gib-app-splainer.ini]...\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Inserting DUPLICATE placeholders into the list. Requested length [500] > list length [74]\n",
      "Inserting DUPLICATE placeholders into the list. Requested length [500] > list length [148]\n",
      "Inserting DUPLICATE placeholders into the list. Requested length [500] > list length [296]\n",
      "Commands file for command [go to current tab] exists: True\n",
      "Commands file for command [go to new tab] exists: True\n",
      "Commands file for command [search current tab] exists: True\n",
      "Commands file for command [search new tab] exists: True\n",
      "Commands file for command [search google current tab] exists: True\n",
      "Commands file for command [search google new tab] exists: True\n",
      "Commands file for command [search google scholar current tab] exists: True\n",
      "Commands file for command [search google scholar new tab] exists: True\n",
      "Commands file for command [search kagi new tab] exists: True\n",
      "Commands file for command [search kagi current tab] exists: True\n",
      "Commands file for command [search perplexity current tab] exists: True\n",
      "Commands file for command [search perplexity new tab] exists: True\n",
      "Commands file for command [search phind current tab] exists: True\n",
      "Commands file for command [search phind new tab] exists: True\n",
      "\n",
      "Commands file for command [search using clipboard current tab] exists: True\n",
      "Commands file for command [search using clipboard new tab] exists: True\n",
      "Commands file for command [search google using clipboard current tab] exists: True\n",
      "Commands file for command [search google using clipboard new tab] exists: True\n",
      "Commands file for command [search google scholar using clipboard current tab] exists: True\n",
      "Commands file for command [search google scholar using clipboard new tab] exists: True\n",
      "Commands file for command [search kagi using clipboard current tab] exists: True\n",
      "Commands file for command [search kagi using clipboard new tab] exists: True\n",
      "Commands file for command [search perplexity using clipboard current tab] exists: True\n",
      "Commands file for command [search perplexity using clipboard new tab] exists: True\n",
      "Commands file for command [search phind using clipboard current tab] exists: True\n",
      "Commands file for command [search phind using clipboard new tab] exists: True\n",
      "Commands file for command [none] exists: True\n",
      "\n",
      "Commands file for command [agent router go to search function mapping] exists: True\n",
      "\n",
      "Commands file for command [agent router go to date and time] exists: True\n",
      "Commands file for command [agent router go to weather] exists: True\n",
      "Commands file for command [agent router go to calendar] exists: True\n",
      "Commands file for command [agent router go to receptionist] exists: True\n",
      "\n",
      "Commands file for command [agent router go to todo list] exists: True\n",
      "Commands file for command [agent router go to math] exists: True\n",
      "Commands file for command [none] exists: True\n",
      "\n",
      "Generating responses for 100 rows...\n",
      "Using DEEPILY w/ model_name [Ministral-8B-Instruct-2410]...\n",
      "Processing call [001] out of [100] = [1.0%]... ETA: 0 seconds\n",
      "Deepily//mnt/DATA01/include/www.deepily.ai/projects/models/Ministral-8B-Instruct-2410.lora/merged-on-2025-02-12-at-02-05/autoround-4-bits-sym.gptq/2025-02-12-at-02-27\n",
      "Skipping sanity check for prompt and preamble!\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "HTTPConnectionPool(host='127.0.0.1', port=3000): Max retries exceeded with url: /v1/completions (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7efaf3157cd0>: Failed to establish a new connection: [Errno 111] Connection refused'))",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mConnectionRefusedError\u001B[0m                    Traceback (most recent call last)",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connection.py:199\u001B[0m, in \u001B[0;36mHTTPConnection._new_conn\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    198\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 199\u001B[0m     sock \u001B[38;5;241m=\u001B[39m \u001B[43mconnection\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate_connection\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    200\u001B[0m \u001B[43m        \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dns_host\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mport\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    201\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    202\u001B[0m \u001B[43m        \u001B[49m\u001B[43msource_address\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msource_address\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    203\u001B[0m \u001B[43m        \u001B[49m\u001B[43msocket_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msocket_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    204\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    205\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m socket\u001B[38;5;241m.\u001B[39mgaierror \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/util/connection.py:85\u001B[0m, in \u001B[0;36mcreate_connection\u001B[0;34m(address, timeout, source_address, socket_options)\u001B[0m\n\u001B[1;32m     84\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 85\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m err\n\u001B[1;32m     86\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     87\u001B[0m     \u001B[38;5;66;03m# Break explicitly a reference cycle\u001B[39;00m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/util/connection.py:73\u001B[0m, in \u001B[0;36mcreate_connection\u001B[0;34m(address, timeout, source_address, socket_options)\u001B[0m\n\u001B[1;32m     72\u001B[0m     sock\u001B[38;5;241m.\u001B[39mbind(source_address)\n\u001B[0;32m---> 73\u001B[0m \u001B[43msock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconnect\u001B[49m\u001B[43m(\u001B[49m\u001B[43msa\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     74\u001B[0m \u001B[38;5;66;03m# Break explicitly a reference cycle\u001B[39;00m\n",
      "\u001B[0;31mConnectionRefusedError\u001B[0m: [Errno 111] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mNewConnectionError\u001B[0m                        Traceback (most recent call last)",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py:789\u001B[0m, in \u001B[0;36mHTTPConnectionPool.urlopen\u001B[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001B[0m\n\u001B[1;32m    788\u001B[0m \u001B[38;5;66;03m# Make the request on the HTTPConnection object\u001B[39;00m\n\u001B[0;32m--> 789\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    790\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    791\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    792\u001B[0m \u001B[43m    \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    793\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout_obj\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    794\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    795\u001B[0m \u001B[43m    \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    796\u001B[0m \u001B[43m    \u001B[49m\u001B[43mchunked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunked\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    797\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mretries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    798\u001B[0m \u001B[43m    \u001B[49m\u001B[43mresponse_conn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresponse_conn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    799\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpreload_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpreload_content\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    800\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdecode_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecode_content\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    801\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mresponse_kw\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    802\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    804\u001B[0m \u001B[38;5;66;03m# Everything went great!\u001B[39;00m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py:495\u001B[0m, in \u001B[0;36mHTTPConnectionPool._make_request\u001B[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001B[0m\n\u001B[1;32m    494\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 495\u001B[0m     \u001B[43mconn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    496\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    497\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    498\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    499\u001B[0m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    500\u001B[0m \u001B[43m        \u001B[49m\u001B[43mchunked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunked\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    501\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpreload_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpreload_content\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    502\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdecode_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecode_content\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    503\u001B[0m \u001B[43m        \u001B[49m\u001B[43menforce_content_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43menforce_content_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    504\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    506\u001B[0m \u001B[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001B[39;00m\n\u001B[1;32m    507\u001B[0m \u001B[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001B[39;00m\n\u001B[1;32m    508\u001B[0m \u001B[38;5;66;03m# With this behaviour, the received response is still readable.\u001B[39;00m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connection.py:441\u001B[0m, in \u001B[0;36mHTTPConnection.request\u001B[0;34m(self, method, url, body, headers, chunked, preload_content, decode_content, enforce_content_length)\u001B[0m\n\u001B[1;32m    440\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mputheader(header, value)\n\u001B[0;32m--> 441\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mendheaders\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    443\u001B[0m \u001B[38;5;66;03m# If we're given a body we start sending that in chunks.\u001B[39;00m\n",
      "File \u001B[0;32m/usr/lib/python3.10/http/client.py:1278\u001B[0m, in \u001B[0;36mHTTPConnection.endheaders\u001B[0;34m(self, message_body, encode_chunked)\u001B[0m\n\u001B[1;32m   1277\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m CannotSendHeader()\n\u001B[0;32m-> 1278\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_send_output\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmessage_body\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencode_chunked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencode_chunked\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/lib/python3.10/http/client.py:1038\u001B[0m, in \u001B[0;36mHTTPConnection._send_output\u001B[0;34m(self, message_body, encode_chunked)\u001B[0m\n\u001B[1;32m   1037\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_buffer[:]\n\u001B[0;32m-> 1038\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmsg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1040\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m message_body \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1041\u001B[0m \n\u001B[1;32m   1042\u001B[0m     \u001B[38;5;66;03m# create a consistent interface to message_body\u001B[39;00m\n",
      "File \u001B[0;32m/usr/lib/python3.10/http/client.py:976\u001B[0m, in \u001B[0;36mHTTPConnection.send\u001B[0;34m(self, data)\u001B[0m\n\u001B[1;32m    975\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mauto_open:\n\u001B[0;32m--> 976\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconnect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    977\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connection.py:279\u001B[0m, in \u001B[0;36mHTTPConnection.connect\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    278\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mconnect\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 279\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msock \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_new_conn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    280\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tunnel_host:\n\u001B[1;32m    281\u001B[0m         \u001B[38;5;66;03m# If we're tunneling it means we're connected to our proxy.\u001B[39;00m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connection.py:214\u001B[0m, in \u001B[0;36mHTTPConnection._new_conn\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    213\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m--> 214\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m NewConnectionError(\n\u001B[1;32m    215\u001B[0m         \u001B[38;5;28mself\u001B[39m, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFailed to establish a new connection: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    216\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[1;32m    218\u001B[0m \u001B[38;5;66;03m# Audit hooks are only available in Python 3.8+\u001B[39;00m\n",
      "\u001B[0;31mNewConnectionError\u001B[0m: <urllib3.connection.HTTPConnection object at 0x7efaf3157cd0>: Failed to establish a new connection: [Errno 111] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mMaxRetryError\u001B[0m                             Traceback (most recent call last)",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/requests/adapters.py:667\u001B[0m, in \u001B[0;36mHTTPAdapter.send\u001B[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001B[0m\n\u001B[1;32m    666\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 667\u001B[0m     resp \u001B[38;5;241m=\u001B[39m \u001B[43mconn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43murlopen\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    668\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    669\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    670\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    671\u001B[0m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    672\u001B[0m \u001B[43m        \u001B[49m\u001B[43mredirect\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    673\u001B[0m \u001B[43m        \u001B[49m\u001B[43massert_same_host\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    674\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpreload_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    675\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdecode_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    676\u001B[0m \u001B[43m        \u001B[49m\u001B[43mretries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_retries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    677\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    678\u001B[0m \u001B[43m        \u001B[49m\u001B[43mchunked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunked\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    679\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    681\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (ProtocolError, \u001B[38;5;167;01mOSError\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py:843\u001B[0m, in \u001B[0;36mHTTPConnectionPool.urlopen\u001B[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001B[0m\n\u001B[1;32m    841\u001B[0m     new_e \u001B[38;5;241m=\u001B[39m ProtocolError(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mConnection aborted.\u001B[39m\u001B[38;5;124m\"\u001B[39m, new_e)\n\u001B[0;32m--> 843\u001B[0m retries \u001B[38;5;241m=\u001B[39m \u001B[43mretries\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mincrement\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    844\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43merror\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnew_e\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_pool\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_stacktrace\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msys\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexc_info\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m]\u001B[49m\n\u001B[1;32m    845\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    846\u001B[0m retries\u001B[38;5;241m.\u001B[39msleep()\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/util/retry.py:519\u001B[0m, in \u001B[0;36mRetry.increment\u001B[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001B[0m\n\u001B[1;32m    518\u001B[0m     reason \u001B[38;5;241m=\u001B[39m error \u001B[38;5;129;01mor\u001B[39;00m ResponseError(cause)\n\u001B[0;32m--> 519\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m MaxRetryError(_pool, url, reason) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mreason\u001B[39;00m  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[1;32m    521\u001B[0m log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIncremented Retry for (url=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m): \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, url, new_retry)\n",
      "\u001B[0;31mMaxRetryError\u001B[0m: HTTPConnectionPool(host='127.0.0.1', port=3000): Max retries exceeded with url: /v1/completions (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7efaf3157cd0>: Failed to establish a new connection: [Errno 111] Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mConnectionError\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[27], line 5\u001B[0m\n\u001B[1;32m      1\u001B[0m get_ipython()\u001B[38;5;241m.\u001B[39mrun_line_magic(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mautoreload\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# this one calls an LLM server, like vLLM or TGI\u001B[39;00m\n\u001B[0;32m----> 5\u001B[0m stats_df \u001B[38;5;241m=\u001B[39m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_validation_with_server\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mswitch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdeepily\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcuda:0\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdebug\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\n\u001B[1;32m      7\u001B[0m \u001B[43m)\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28mprint\u001B[39m()\n\u001B[1;32m      9\u001B[0m stats_df\n",
      "File \u001B[0;32m/var/model/genie-in-the-box/src/cosa/training/peft_trainer.py:313\u001B[0m, in \u001B[0;36mPeftTrainer.run_validation_with_server\u001B[0;34m(self, model, switch, path_prefix, device_map, sample_size, debug, verbose)\u001B[0m\n\u001B[1;32m    310\u001B[0m xml_ftp_generator \u001B[38;5;241m=\u001B[39m XmlFineTuningPromptGenerator( path_prefix\u001B[38;5;241m=\u001B[39mpath_prefix, debug\u001B[38;5;241m=\u001B[39mdebug, verbose\u001B[38;5;241m=\u001B[39mverbose )\n\u001B[1;32m    312\u001B[0m \u001B[38;5;66;03m# generate responses...\u001B[39;00m\n\u001B[0;32m--> 313\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[43mxml_ftp_generator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate_responses\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    314\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mswitch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mswitch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    315\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice_map\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_new_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m128\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdebug\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdebug\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\n\u001B[1;32m    316\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    317\u001B[0m \u001B[38;5;66;03m# validate responses...\u001B[39;00m\n\u001B[1;32m    318\u001B[0m df \u001B[38;5;241m=\u001B[39m xml_ftp_generator\u001B[38;5;241m.\u001B[39mvalidate_responses( df )\n",
      "File \u001B[0;32m/var/model/genie-in-the-box/src/cosa/training/xml_fine_tuning_prompt_generator.py:1147\u001B[0m, in \u001B[0;36mXmlFineTuningPromptGenerator.generate_responses\u001B[0;34m(self, df, tokenizer, model, switch, model_name, max_new_tokens, temperature, top_k, top_p, device, debug, verbose, silent)\u001B[0m\n\u001B[1;32m   1145\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m switch \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdeepily\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m   1146\u001B[0m     \u001B[38;5;28mprint\u001B[39m( \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUsing DEEPILY w/ model_name [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m]...\u001B[39m\u001B[38;5;124m\"\u001B[39m )\n\u001B[0;32m-> 1147\u001B[0m     df[ \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mresponse\u001B[39m\u001B[38;5;124m\"\u001B[39m ]  \u001B[38;5;241m=\u001B[39m \u001B[43mdf\u001B[49m\u001B[43m[\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mprompt\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mcell\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_response_to_prompt\u001B[49m\u001B[43m(\u001B[49m\u001B[43m \u001B[49m\u001B[43mcell\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrows\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mswitch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mswitch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_new_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_new_tokens\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtemperature\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_k\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtop_k\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_p\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtop_p\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msilent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msilent\u001B[49m\u001B[43m \u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1148\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m switch \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mopenai\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m   1149\u001B[0m     \u001B[38;5;28mprint\u001B[39m( \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUsing OPENAI w/ model_name [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m]...\u001B[39m\u001B[38;5;124m\"\u001B[39m )\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py:4924\u001B[0m, in \u001B[0;36mSeries.apply\u001B[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001B[0m\n\u001B[1;32m   4789\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply\u001B[39m(\n\u001B[1;32m   4790\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   4791\u001B[0m     func: AggFuncType,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   4796\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m   4797\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame \u001B[38;5;241m|\u001B[39m Series:\n\u001B[1;32m   4798\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   4799\u001B[0m \u001B[38;5;124;03m    Invoke function on values of Series.\u001B[39;00m\n\u001B[1;32m   4800\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   4915\u001B[0m \u001B[38;5;124;03m    dtype: float64\u001B[39;00m\n\u001B[1;32m   4916\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m   4917\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mSeriesApply\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   4918\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4919\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4920\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconvert_dtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert_dtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4921\u001B[0m \u001B[43m        \u001B[49m\u001B[43mby_row\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mby_row\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4922\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4923\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m-> 4924\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py:1427\u001B[0m, in \u001B[0;36mSeriesApply.apply\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1424\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply_compat()\n\u001B[1;32m   1426\u001B[0m \u001B[38;5;66;03m# self.func is Callable\u001B[39;00m\n\u001B[0;32m-> 1427\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_standard\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py:1507\u001B[0m, in \u001B[0;36mSeriesApply.apply_standard\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1501\u001B[0m \u001B[38;5;66;03m# row-wise access\u001B[39;00m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m \u001B[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001B[39;00m\n\u001B[1;32m   1504\u001B[0m \u001B[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001B[39;00m\n\u001B[1;32m   1505\u001B[0m \u001B[38;5;66;03m#  Categorical (GH51645).\u001B[39;00m\n\u001B[1;32m   1506\u001B[0m action \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mignore\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(obj\u001B[38;5;241m.\u001B[39mdtype, CategoricalDtype) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 1507\u001B[0m mapped \u001B[38;5;241m=\u001B[39m \u001B[43mobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_map_values\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1508\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmapper\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcurried\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mna_action\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maction\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert_dtype\u001B[49m\n\u001B[1;32m   1509\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1511\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(mapped) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(mapped[\u001B[38;5;241m0\u001B[39m], ABCSeries):\n\u001B[1;32m   1512\u001B[0m     \u001B[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001B[39;00m\n\u001B[1;32m   1513\u001B[0m     \u001B[38;5;66;03m#  See also GH#25959 regarding EA support\u001B[39;00m\n\u001B[1;32m   1514\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\u001B[38;5;241m.\u001B[39m_constructor_expanddim(\u001B[38;5;28mlist\u001B[39m(mapped), index\u001B[38;5;241m=\u001B[39mobj\u001B[38;5;241m.\u001B[39mindex)\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/base.py:921\u001B[0m, in \u001B[0;36mIndexOpsMixin._map_values\u001B[0;34m(self, mapper, na_action, convert)\u001B[0m\n\u001B[1;32m    918\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(arr, ExtensionArray):\n\u001B[1;32m    919\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m arr\u001B[38;5;241m.\u001B[39mmap(mapper, na_action\u001B[38;5;241m=\u001B[39mna_action)\n\u001B[0;32m--> 921\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43malgorithms\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43marr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapper\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mna_action\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mna_action\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/algorithms.py:1743\u001B[0m, in \u001B[0;36mmap_array\u001B[0;34m(arr, mapper, na_action, convert)\u001B[0m\n\u001B[1;32m   1741\u001B[0m values \u001B[38;5;241m=\u001B[39m arr\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mobject\u001B[39m, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m na_action \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1743\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_infer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapper\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1745\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m lib\u001B[38;5;241m.\u001B[39mmap_infer_mask(\n\u001B[1;32m   1746\u001B[0m         values, mapper, mask\u001B[38;5;241m=\u001B[39misna(values)\u001B[38;5;241m.\u001B[39mview(np\u001B[38;5;241m.\u001B[39muint8), convert\u001B[38;5;241m=\u001B[39mconvert\n\u001B[1;32m   1747\u001B[0m     )\n",
      "File \u001B[0;32mlib.pyx:2972\u001B[0m, in \u001B[0;36mpandas._libs.lib.map_infer\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m/var/model/genie-in-the-box/src/cosa/training/xml_fine_tuning_prompt_generator.py:1147\u001B[0m, in \u001B[0;36mXmlFineTuningPromptGenerator.generate_responses.<locals>.<lambda>\u001B[0;34m(cell)\u001B[0m\n\u001B[1;32m   1145\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m switch \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdeepily\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m   1146\u001B[0m     \u001B[38;5;28mprint\u001B[39m( \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUsing DEEPILY w/ model_name [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m]...\u001B[39m\u001B[38;5;124m\"\u001B[39m )\n\u001B[0;32m-> 1147\u001B[0m     df[ \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mresponse\u001B[39m\u001B[38;5;124m\"\u001B[39m ]  \u001B[38;5;241m=\u001B[39m df[ \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprompt\u001B[39m\u001B[38;5;124m\"\u001B[39m ]\u001B[38;5;241m.\u001B[39mapply( \u001B[38;5;28;01mlambda\u001B[39;00m cell: \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_response_to_prompt\u001B[49m\u001B[43m(\u001B[49m\u001B[43m \u001B[49m\u001B[43mcell\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrows\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mswitch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mswitch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_new_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_new_tokens\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtemperature\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_k\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtop_k\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_p\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtop_p\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msilent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msilent\u001B[49m\u001B[43m \u001B[49m\u001B[43m)\u001B[49m )\n\u001B[1;32m   1148\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m switch \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mopenai\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m   1149\u001B[0m     \u001B[38;5;28mprint\u001B[39m( \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUsing OPENAI w/ model_name [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m]...\u001B[39m\u001B[38;5;124m\"\u001B[39m )\n",
      "File \u001B[0;32m/var/model/genie-in-the-box/src/cosa/training/xml_fine_tuning_prompt_generator.py:1127\u001B[0m, in \u001B[0;36mXmlFineTuningPromptGenerator._get_response_to_prompt\u001B[0;34m(self, prompt, rows, model, switch, model_name, timer, tokenizer, max_new_tokens, temperature, top_k, top_p, device, silent, debug, verbose)\u001B[0m\n\u001B[1;32m   1125\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mquery_llm_tgi( prompt, model_name\u001B[38;5;241m=\u001B[39mmodel_name, max_new_tokens\u001B[38;5;241m=\u001B[39mmax_new_tokens, temperature\u001B[38;5;241m=\u001B[39mtemperature, top_k\u001B[38;5;241m=\u001B[39mtop_k, top_p\u001B[38;5;241m=\u001B[39mtop_p, silent\u001B[38;5;241m=\u001B[39msilent )\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m switch \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdeepily\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m-> 1127\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_query_llm_deepily\u001B[49m\u001B[43m(\u001B[49m\u001B[43m \u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdebug\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdebug\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m switch \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mopenai\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m   1129\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_query_llm_openai( prompt[ \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m ], model_name\u001B[38;5;241m=\u001B[39mmodel_name )\n",
      "File \u001B[0;32m/var/model/genie-in-the-box/src/cosa/training/xml_fine_tuning_prompt_generator.py:1067\u001B[0m, in \u001B[0;36mXmlFineTuningPromptGenerator._query_llm_deepily\u001B[0;34m(self, prompt, model, model_name, debug, verbose)\u001B[0m\n\u001B[1;32m   1065\u001B[0m \u001B[38;5;28mprint\u001B[39m(model)\n\u001B[1;32m   1066\u001B[0m llm \u001B[38;5;241m=\u001B[39m Llm( model\u001B[38;5;241m=\u001B[39mmodel, default_url\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttp://192.168.1.21:3000/v1\u001B[39m\u001B[38;5;124m\"\u001B[39m, debug\u001B[38;5;241m=\u001B[39mdebug, verbose\u001B[38;5;241m=\u001B[39mverbose )\n\u001B[0;32m-> 1067\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[43mllm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mquery_llm\u001B[49m\u001B[43m(\u001B[49m\u001B[43m \u001B[49m\u001B[43mprompt\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprompt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_k\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m50\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_p\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1.0\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1068\u001B[0m \u001B[38;5;66;03m# du.print_banner( \"DEEPILY Response\", prepend_nl=True )\u001B[39;00m\n\u001B[1;32m   1069\u001B[0m \u001B[38;5;66;03m# print( results )\u001B[39;00m\n\u001B[1;32m   1070\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m results\n",
      "File \u001B[0;32m/var/model/genie-in-the-box/src/cosa/agents/llm.py:126\u001B[0m, in \u001B[0;36mLlm.query_llm\u001B[0;34m(self, model, prompt_yaml, prompt, preamble, question, max_new_tokens, temperature, top_k, top_p, stop_sequences, debug, verbose)\u001B[0m\n\u001B[1;32m    121\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_query_llm_google(\n\u001B[1;32m    122\u001B[0m         prompt, max_new_tokens\u001B[38;5;241m=\u001B[39mmax_new_tokens, temperature\u001B[38;5;241m=\u001B[39mtemperature, top_k\u001B[38;5;241m=\u001B[39mtop_k, top_p\u001B[38;5;241m=\u001B[39mtop_p, stop_sequences\u001B[38;5;241m=\u001B[39mstop_sequences, debug\u001B[38;5;241m=\u001B[39mdebug, verbose\u001B[38;5;241m=\u001B[39mverbose\n\u001B[1;32m    123\u001B[0m     )\n\u001B[1;32m    125\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mstartswith( \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDeepily/\u001B[39m\u001B[38;5;124m\"\u001B[39m ):\n\u001B[0;32m--> 126\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_query_vllm_openai\u001B[49m\u001B[43m(\u001B[49m\u001B[43m \u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_new_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_new_tokens\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtemperature\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_k\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtop_k\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_p\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtop_p\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop_sequences\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop_sequences\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdebug\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdebug\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    127\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    128\u001B[0m     \u001B[38;5;66;03m# Test for divisibility if receiving an \"all in one\" non chatbot type prompt\u001B[39;00m\n\u001B[1;32m    129\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m prompt \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m/var/model/genie-in-the-box/src/cosa/agents/llm.py:329\u001B[0m, in \u001B[0;36mLlm._query_vllm_openai\u001B[0;34m(self, prompt, max_new_tokens, temperature, top_k, top_p, stop_sequences, debug, verbose)\u001B[0m\n\u001B[1;32m    316\u001B[0m payload \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    317\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m      : \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mextract_model_name( \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel ),\n\u001B[1;32m    318\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprompt\u001B[39m\u001B[38;5;124m\"\u001B[39m     : prompt,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    323\u001B[0m     \u001B[38;5;66;03m# \"stop\"       : stop_sequences\u001B[39;00m\n\u001B[1;32m    324\u001B[0m }\n\u001B[1;32m    325\u001B[0m \u001B[38;5;66;03m# du.print_banner( \"Payload:\" )\u001B[39;00m\n\u001B[1;32m    326\u001B[0m \u001B[38;5;66;03m# print( payload )\u001B[39;00m\n\u001B[1;32m    327\u001B[0m \n\u001B[1;32m    328\u001B[0m \u001B[38;5;66;03m# Send the request\u001B[39;00m\n\u001B[0;32m--> 329\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[43mrequests\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpost\u001B[49m\u001B[43m(\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mjson\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdumps\u001B[49m\u001B[43m(\u001B[49m\u001B[43m \u001B[49m\u001B[43mpayload\u001B[49m\u001B[43m \u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    331\u001B[0m \u001B[38;5;66;03m# Parse the response\u001B[39;00m\n\u001B[1;32m    332\u001B[0m completion \u001B[38;5;241m=\u001B[39m response\u001B[38;5;241m.\u001B[39mjson()\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/requests/api.py:115\u001B[0m, in \u001B[0;36mpost\u001B[0;34m(url, data, json, **kwargs)\u001B[0m\n\u001B[1;32m    103\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpost\u001B[39m(url, data\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, json\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    104\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Sends a POST request.\u001B[39;00m\n\u001B[1;32m    105\u001B[0m \n\u001B[1;32m    106\u001B[0m \u001B[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;124;03m    :rtype: requests.Response\u001B[39;00m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 115\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpost\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mjson\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mjson\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/requests/api.py:59\u001B[0m, in \u001B[0;36mrequest\u001B[0;34m(method, url, **kwargs)\u001B[0m\n\u001B[1;32m     55\u001B[0m \u001B[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001B[39;00m\n\u001B[1;32m     56\u001B[0m \u001B[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001B[39;00m\n\u001B[1;32m     57\u001B[0m \u001B[38;5;66;03m# cases, and look like a memory leak in others.\u001B[39;00m\n\u001B[1;32m     58\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m sessions\u001B[38;5;241m.\u001B[39mSession() \u001B[38;5;28;01mas\u001B[39;00m session:\n\u001B[0;32m---> 59\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43msession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py:589\u001B[0m, in \u001B[0;36mSession.request\u001B[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001B[0m\n\u001B[1;32m    584\u001B[0m send_kwargs \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    585\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtimeout\u001B[39m\u001B[38;5;124m\"\u001B[39m: timeout,\n\u001B[1;32m    586\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mallow_redirects\u001B[39m\u001B[38;5;124m\"\u001B[39m: allow_redirects,\n\u001B[1;32m    587\u001B[0m }\n\u001B[1;32m    588\u001B[0m send_kwargs\u001B[38;5;241m.\u001B[39mupdate(settings)\n\u001B[0;32m--> 589\u001B[0m resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprep\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43msend_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    591\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m resp\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py:703\u001B[0m, in \u001B[0;36mSession.send\u001B[0;34m(self, request, **kwargs)\u001B[0m\n\u001B[1;32m    700\u001B[0m start \u001B[38;5;241m=\u001B[39m preferred_clock()\n\u001B[1;32m    702\u001B[0m \u001B[38;5;66;03m# Send the request\u001B[39;00m\n\u001B[0;32m--> 703\u001B[0m r \u001B[38;5;241m=\u001B[39m \u001B[43madapter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    705\u001B[0m \u001B[38;5;66;03m# Total elapsed time of the request (approximately)\u001B[39;00m\n\u001B[1;32m    706\u001B[0m elapsed \u001B[38;5;241m=\u001B[39m preferred_clock() \u001B[38;5;241m-\u001B[39m start\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/requests/adapters.py:700\u001B[0m, in \u001B[0;36mHTTPAdapter.send\u001B[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001B[0m\n\u001B[1;32m    696\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(e\u001B[38;5;241m.\u001B[39mreason, _SSLError):\n\u001B[1;32m    697\u001B[0m         \u001B[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001B[39;00m\n\u001B[1;32m    698\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m SSLError(e, request\u001B[38;5;241m=\u001B[39mrequest)\n\u001B[0;32m--> 700\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mConnectionError\u001B[39;00m(e, request\u001B[38;5;241m=\u001B[39mrequest)\n\u001B[1;32m    702\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m ClosedPoolError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    703\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mConnectionError\u001B[39;00m(e, request\u001B[38;5;241m=\u001B[39mrequest)\n",
      "\u001B[0;31mConnectionError\u001B[0m: HTTPConnectionPool(host='127.0.0.1', port=3000): Max retries exceeded with url: /v1/completions (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7efaf3157cd0>: Failed to establish a new connection: [Errno 111] Connection refused'))"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "\n",
    "# this one calls an LLM server, like vLLM or TGI\n",
    "\n",
    "stats_df = trainer.run_validation_with_server(\n",
    "    model=model, switch=\"deepily\", device_map=\"cuda:0\", sample_size=100, debug=True, verbose=True\n",
    ")\n",
    "print()\n",
    "stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c29fb571f2bee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df = trainer.run_validation_in_memory( switch=\"huggingface\", device_map=\"cuda:0\", sample_size=10, debug=False, verbose=False )\n",
    "stats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4e9623708e0ba1",
   "metadata": {},
   "source": [
    "## run validation, using in memory LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0d4c9461618920",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "\n",
    "# trainer.set_gib_env_vars( gib_root=gib_root )\n",
    "# checkpoint_dir = \"/var/model/models/Ministral-8B-Instruct-2410.lora/training-2025-02-06-at-21-28/checkpoint-987\"\n",
    "\n",
    "# this one calls in LLM in memory\n",
    "stats_df = trainer.run_validation_in_memory( switch=\"huggingface\", device_map=\"cuda:0\", sample_size=100, debug=True, verbose=False )\n",
    "stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5102cd20c2a76761",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ddfdd75c-bacc-44d5-a408-aa6201802b6c",
   "metadata": {},
   "source": [
    "## run this after tokenizer is initialized below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e34ef54107ef3e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T17:46:56.839070Z",
     "start_time": "2025-01-20T17:46:56.779304Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "%autoreload\n",
    "\n",
    "def run_validation( model, tokenizer, model_name=\"ministral/Ministral-3b-instruct\", device=\"cuda:1\", sample_size=1000, debug=False, verbose=False ):\n",
    "\n",
    "    df = pd.read_json(\n",
    "        \"/var/model/genie-in-the-box/src/ephemera/prompts/data/voice-commands-xml-validate.jsonl\", lines=True\n",
    "    ).sample( sample_size, random_state=42 )\n",
    "\n",
    "    du.print_banner( f\"Validating {model_name} w/ {sample_size} samples\" )\n",
    "    # Print value counts for the command column to see how many unique commands we have\n",
    "    print( df.command.value_counts(), end=\"\\n\\n\" )\n",
    "\n",
    "    xml_ftp_generator = XmlFineTuningPromptGenerator( path_prefix=\"/var/model/genie-in-the-box\", debug=debug, verbose=verbose )\n",
    "\n",
    "    df = xml_ftp_generator.generate_responses(\n",
    "        df, tokenizer=tokenizer, model=model, switch=\"huggingface\", model_name=model_name, device=device, debug=debug, verbose=verbose\n",
    "    )\n",
    "    df = xml_ftp_generator.validate_responses( df )\n",
    "\n",
    "    xml_ftp_generator.print_validation_stats( df, title=f\"Validation stats for model {model_name}\" )\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "644a6196802f8630",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T01:03:44.071141Z",
     "start_time": "2025-02-06T01:03:43.926401Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 48K\r\n",
      "drwxrwxr-x 11 1001 1001 4.0K Jan 25 01:52 .\r\n",
      "drwxrwxr-x 15 1001 1001 4.0K Jan 25 02:42 ..\r\n",
      "drwxrwxr-x  5 1001 1001 4.0K Dec 20 23:09 .venv\r\n",
      "drwxrwxr-x  6 1001 1001 4.0K Jan 15 19:14 datasets--NeelNanda--pile-10k\r\n",
      "drwxrwxr-x  6 1001 1001 4.0K Dec 20 23:21 models--kaitchup--Phi-4-AutoRound-GPTQ-4bit\r\n",
      "drwxrwxr-x  6 1001 1001 4.0K Dec 18 03:51 models--kaitchup--Qwen2.5-Coder-32B-Instruct-AutoRound-GPTQ-4bit\r\n",
      "drwxrwxr-x  6 1001 1001 4.0K Jan 15 18:34 models--meta-llama--Llama-3.2-3B-Instruct\r\n",
      "drwxrwxr-x  6 1001 1001 4.0K Dec 13 22:17 models--ministral--Ministral-3b-instruct\r\n",
      "drwxrwxr-x  6 1001 1001 4.0K Jan 25 01:26 models--mistralai--Ministral-8B-Instruct-2410\r\n",
      "drwxrwxr-x  6 1001 1001 4.0K Jan 24  2024 models--mistralai--Mistral-7B-Instruct-v0.2\r\n",
      "drwxr-xr-x  2 1001 1001 4.0K Dec 13 19:41 models--mistralai--Mistral-7B-v0.1\r\n",
      "-rw-rw-r--  1 1001 1001    1 Dec 18 03:43 version.txt\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# ! ls -alh /var/model/models/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/b70aa86578567ba3301b21c8a27bea4e8f6d6d61\n",
    "! ls -alh /var/model/models/hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9fedda738ba991",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Load model and tokenizer Using bits and bites quantization or bfloat16?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9a5401a806b903b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T01:18:48.373378Z",
     "start_time": "2025-02-06T01:18:48.366782Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/var/model/models'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get HF_HOME from environment\n",
    "os.environ[ \"HF_HOME\" ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1bc8cfb9781119a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:44:54.684471Z",
     "start_time": "2025-01-20T18:44:54.679223Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def get_base_model_and_tokenizer( model_path=\".\", tokenizer_path=\".\", torch_dtype=torch.bfloat16, use_bnb_quantization=False, device_map=\"auto\", cache_dir=f\"{models_root}/hub\" ):\n",
    "\n",
    "    compute_dtype = getattr( torch, \"float16\" )\n",
    "\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=compute_dtype\n",
    "    )\n",
    "    if use_bnb_quantization:\n",
    "\n",
    "        print( bnb_config )\n",
    "\n",
    "        # OJO! Why were we turning off the cash here? It makes a big performance difference: 21 vs 14 tokens per second\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path, quantization_config=bnb_config, device_map=device_map, low_cpu_mem_usage=True, use_cache=True,\n",
    "            local_files_only=True, cache_dir=cache_dir,\n",
    "            attn_implementation=\"flash_attention_2\",\n",
    "            # use_auth_token=auth_token,\n",
    "            # token=auth_token\n",
    "        )\n",
    "    else:\n",
    "        print( \"Loading without BitsAndBytesConfig...\" )\n",
    "        print( \"HF_HOME: \" + os.environ[ \"HF_HOME\" ] )\n",
    "        print( \"HF_HUB_ETAG_TIMEOUT: \" + os.environ[ \"HF_HUB_ETAG_TIMEOUT\" ] )\n",
    "        print( \"HF_HUB_DOWNLOAD_TIMEOUT: \" + os.environ[ \"HF_HUB_DOWNLOAD_TIMEOUT\" ] )\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path, device_map=device_map, low_cpu_mem_usage=True, use_cache=True,\n",
    "            torch_dtype=torch_dtype, local_files_only=True, cache_dir=cache_dir,\n",
    "            attn_implementation=\"flash_attention_2\",\n",
    "            # use_auth_token=auth_token,\n",
    "            # token=auth_token\n",
    "        )\n",
    "\n",
    "    tokenizer              = AutoTokenizer.from_pretrained( tokenizer_path, force_download=True, from_slow=False )\n",
    "    tokenizer.pad_token    = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "    return base_model, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b54b74fad9678285",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T01:46:00.256197Z",
     "start_time": "2025-02-06T01:46:00.252029Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. PyTorch can use the GPU.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. PyTorch can use the GPU.\")\n",
    "else:\n",
    "    print(\"CUDA is not available. PyTorch will use the CPU.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ee5642b83e85d51",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T20:13:33.540161Z",
     "start_time": "2025-01-13T20:13:33.535659Z"
    }
   },
   "outputs": [],
   "source": [
    "os.chdir( f\"{models_root}/\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c28f4e26e951738c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T01:47:04.398705Z",
     "start_time": "2025-01-14T01:47:04.395300Z"
    }
   },
   "outputs": [],
   "source": [
    "model_path = \"mistralai/Ministral-8B-Instruct-2410\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926b612bcb0c1fdf",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print( os.getcwd() )\n",
    "base_model, tokenizer = get_base_model_and_tokenizer(\n",
    "    model_path=model_path,\n",
    "    tokenizer_path=model_path,\n",
    "    use_bnb_quantization=False,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7d6ee0b2dd06101",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T18:57:48.391004Z",
     "start_time": "2025-01-09T18:57:48.262503Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/var/model/models\r\n"
     ]
    }
   ],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "42d0f6e85a7c7639",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T15:35:07.819018Z",
     "start_time": "2025-01-09T15:35:07.812093Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(131072, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-35): 36 x MistralDecoderLayer(\n",
       "        (self_attn): MistralSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (down_proj): Linear(in_features=12288, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=131072, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcd418053c3052b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Get training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cfdca43d-dd7d-428a-97ab-fc18e3257eec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T15:40:32.389373Z",
     "start_time": "2025-01-09T15:40:32.384421Z"
    }
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae76c88a9a5e791d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T15:40:33.640363Z",
     "start_time": "2025-01-09T15:40:32.927995Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31606"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"/var/model/genie-in-the-box/src/ephemera/prompts/data/voice-commands-xml-train.jsonl\"\n",
    "deepily_dataset_train = du.get_file_as_list( path )#[ 0:10000 ]\n",
    "deepily_dataset_train = [ json.loads( line ) for line in deepily_dataset_train ]\n",
    "len( deepily_dataset_train )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f1c868ec60880dcc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T15:40:34.561110Z",
     "start_time": "2025-01-09T15:40:34.472679Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3951"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"/var/model/genie-in-the-box/src/ephemera/prompts/data/voice-commands-xml-test.jsonl\"\n",
    "deepily_dataset_test = du.get_file_as_list( path )#[ 0:1000 ]\n",
    "deepily_dataset_test = [ json.loads( line ) for line in deepily_dataset_test ]\n",
    "len( deepily_dataset_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e66898ac4c85e976",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T15:40:36.795404Z",
     "start_time": "2025-01-09T15:40:36.792208Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# for line in prompt_instruction_format( deepily_dataset_test[ 0 ] ).split( \"\\n\" ): print( line )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528b5d0d39d9b5df",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Set up training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "393f4bbf9c6c3ca4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T15:40:41.859438Z",
     "start_time": "2025-01-09T15:40:41.788027Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_config, PeftModel, PeftConfig, get_peft_model, AutoPeftModelForCausalLM\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=64, \n",
    "    lora_alpha=32, \n",
    "    # When target_modules was disabled, it was causing detention layers to be assigned to the CPU, throwing this runtime error:\n",
    "    # RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! \n",
    "    # (when checking argument for argument mat2 in method wrapper_CUDA_mm)\n",
    "    target_modules=[ \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\", \"lm_head\" ], \n",
    "    lora_dropout=0.10, \n",
    "    bias=\"none\", \n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abccd9b1f7ce3be5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T20:56:21.258627Z",
     "start_time": "2025-01-13T20:56:21.251895Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/var/model/models/Ministral-8B-Instruct-2410'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"Ministral-8B-Instruct-2410\"\n",
    "\n",
    "os.chdir( f\"{models_root}/{model_name}\" )\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28ebce79b18f6f",
   "metadata": {},
   "source": [
    "## we need to disable peer to peer communication until the RTX 4090 drivers are updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0ceec6ca8797372",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T20:13:54.379074Z",
     "start_time": "2025-01-13T20:13:54.373991Z"
    }
   },
   "outputs": [],
   "source": [
    "os.environ[ \"NCCL_P2P_DISABLE\" ] = \"1\"\n",
    "os.environ[ \"NCCL_IB_DISABLE\" ] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e0ad2d859cefb9c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T15:41:09.214606Z",
     "start_time": "2025-01-09T15:41:01.622129Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, packing. Will not be supported from version '0.13.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:212: UserWarning: You passed a `packing` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "# Define the training arguments\n",
    "trainingArgs = TrainingArguments(\n",
    "    output_dir=\"./training-results\", # Output directory where the model predictions and checkpoints will be stored\n",
    "    num_train_epochs=1, # Number of training epochs\n",
    "    per_device_train_batch_size=2, # Batch size per GPU for training. https://kaitchup.substack.com/p/fine-tune-a-mixture-of-experts-on Says that using even batch size is best\n",
    "    per_device_eval_batch_size=2,  # Batch size per GPU for evaluation. https://kaitchup.substack.com/p/fine-tune-a-mixture-of-experts-on Says that using even batch size is best\n",
    "    gradient_accumulation_steps=8, # Number of update steps to accumulate the gradients for\n",
    "    gradient_checkpointing=True,# Enable gradient checkpointing\n",
    "    # optim=\"paged_adamw_32bit\", # Optimizer to use: see kaitchup for more details: https://kaitchup.substack.com/p/fine-tuning-llms-with-32-bit-8-bit\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    #save_steps=save_steps,\n",
    "    logging_steps=5,\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    \n",
    "    # Setting this may help with the warning message: The input hidden states seems to be silently casted in float32, \n",
    "    # this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.float16.\n",
    "    fp16=False,\n",
    "    # Test to confirm that this works!\n",
    "    # BTW: according to PHIND, this may actually improve fine-tuning performance as well: https://www.phind.com/search?cache=ygn9dbyl0ij4kotmgns2nsrw\n",
    "    \n",
    "    bf16=True,\n",
    "    # tf32=True,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    #max_steps=max_steps,\n",
    "    group_by_length=False,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    disable_tqdm=True,\n",
    "    # report_to=\"wandb\",\n",
    "    report_to=\"none\",\n",
    "    seed=42\n",
    ")\n",
    "# Create the trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=base_model,\n",
    "    train_dataset=deepily_dataset_train,\n",
    "    eval_dataset=deepily_dataset_test,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=2184, # Calculated by get_training_prompt_stats( tokenizer ), max = 728 * 3 # was: 2,048 or 4,096\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True,\n",
    "    formatting_func=prompt_instruction_format,\n",
    "    args=trainingArgs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8275555ac7a5093d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T15:41:09.356246Z",
     "start_time": "2025-01-09T15:41:09.346158Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 183,238,656 || all params: 8,203,046,912 || trainable%: 2.23\n"
     ]
    }
   ],
   "source": [
    "def print_trainable_parameters( model ):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params:,} || all params: {all_param:,} || trainable%: {100 * trainable_params / all_param:.2f}\"\n",
    "    )\n",
    "    \n",
    "print_trainable_parameters( base_model )\n",
    "# trainable params: 170,082,304 || all params: 7,411,814,400 || trainable%: 2.29"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d13c347871afcd",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a40e7bb621f64d47",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T20:13:59.550627Z",
     "start_time": "2025-01-13T20:13:59.544968Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/var/model/models/Ministral-8B-Instruct-2410\n"
     ]
    }
   ],
   "source": [
    "os.chdir( f\"{models_root}/{model_name}\" )\n",
    "print( os.getcwd() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597fd4a8fa0f143f",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-01-09T15:41:21.045952Z"
    },
    "collapsed": false,
    "jupyter": {
     "is_executing": true,
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6379, 'grad_norm': 3.7572505474090576, 'learning_rate': 6.25e-05, 'epoch': 0.00951927653498334}\n",
      "{'loss': 0.4318, 'grad_norm': 2.073525905609131, 'learning_rate': 0.000125, 'epoch': 0.01903855306996668}\n",
      "{'loss': 0.1912, 'grad_norm': 1.2309727668762207, 'learning_rate': 0.0001875, 'epoch': 0.028557829604950024}\n",
      "{'loss': 0.1159, 'grad_norm': 0.6885170340538025, 'learning_rate': 0.00019996952581438068, 'epoch': 0.03807710613993336}\n",
      "{'loss': 0.0891, 'grad_norm': 1.2253117561340332, 'learning_rate': 0.0001998457562671611, 'epoch': 0.047596382674916705}\n",
      "{'loss': 0.0703, 'grad_norm': 0.9658526182174683, 'learning_rate': 0.00019962690449567912, 'epoch': 0.05711565920990005}\n",
      "{'loss': 0.0631, 'grad_norm': 0.6428528428077698, 'learning_rate': 0.00019931317891052708, 'epoch': 0.06663493574488338}\n",
      "{'loss': 0.0614, 'grad_norm': 0.7227765917778015, 'learning_rate': 0.0001989048782697851, 'epoch': 0.07615421227986673}\n",
      "{'loss': 0.0598, 'grad_norm': 0.5716121196746826, 'learning_rate': 0.0001984023913945162, 'epoch': 0.08567348881485007}\n",
      "{'loss': 0.0514, 'grad_norm': 0.4813480079174042, 'learning_rate': 0.00019780619679849552, 'epoch': 0.09519276534983341}\n",
      "{'loss': 0.0517, 'grad_norm': 0.4667695462703705, 'learning_rate': 0.0001971168622325259, 'epoch': 0.10471204188481675}\n",
      "{'loss': 0.053, 'grad_norm': 0.4471019208431244, 'learning_rate': 0.00019633504414377388, 'epoch': 0.1142313184198001}\n",
      "{'loss': 0.051, 'grad_norm': 0.3964586555957794, 'learning_rate': 0.00019546148705064097, 'epoch': 0.12375059495478344}\n",
      "{'loss': 0.0481, 'grad_norm': 0.41865667700767517, 'learning_rate': 0.00019449702283376517, 'epoch': 0.13326987148976677}\n",
      "{'loss': 0.0459, 'grad_norm': 0.4355333745479584, 'learning_rate': 0.00019344256994382878, 'epoch': 0.14278914802475012}\n",
      "{'loss': 0.0456, 'grad_norm': 0.4354214370250702, 'learning_rate': 0.0001922991325269258, 'epoch': 0.15230842455973345}\n",
      "{'loss': 0.044, 'grad_norm': 0.37702372670173645, 'learning_rate': 0.0001910677994683225, 'epoch': 0.1618277010947168}\n",
      "{'loss': 0.0436, 'grad_norm': 0.342061847448349, 'learning_rate': 0.0001897497433555218, 'epoch': 0.17134697762970014}\n",
      "{'loss': 0.0438, 'grad_norm': 0.3480202853679657, 'learning_rate': 0.0001883462193616187, 'epoch': 0.1808662541646835}\n",
      "{'loss': 0.0435, 'grad_norm': 0.3344561457633972, 'learning_rate': 0.00018685856405000983, 'epoch': 0.19038553069966682}\n",
      "{'loss': 0.0454, 'grad_norm': 0.3861624002456665, 'learning_rate': 0.0001852881941015964, 'epoch': 0.19990480723465018}\n",
      "{'loss': 0.0419, 'grad_norm': 0.3449898362159729, 'learning_rate': 0.00018363660496569127, 'epoch': 0.2094240837696335}\n",
      "{'loss': 0.0393, 'grad_norm': 0.33622458577156067, 'learning_rate': 0.00018190536943591624, 'epoch': 0.21894336030461684}\n",
      "{'loss': 0.0413, 'grad_norm': 0.32727330923080444, 'learning_rate': 0.00018009613615244436, 'epoch': 0.2284626368396002}\n",
      "{'loss': 0.0426, 'grad_norm': 0.31954145431518555, 'learning_rate': 0.0001782106280320147, 'epoch': 0.23798191337458352}\n",
      "{'loss': 0.0416, 'grad_norm': 0.6218836307525635, 'learning_rate': 0.00017625064062721415, 'epoch': 0.24750118990956688}\n",
      "{'loss': 0.0401, 'grad_norm': 0.2987784445285797, 'learning_rate': 0.00017421804041658863, 'epoch': 0.25702046644455023}\n",
      "{'loss': 0.0396, 'grad_norm': 0.35389140248298645, 'learning_rate': 0.0001721147630272123, 'epoch': 0.26653974297953353}\n",
      "{'loss': 0.0411, 'grad_norm': 0.38974466919898987, 'learning_rate': 0.00016994281139140688, 'epoch': 0.2760590195145169}\n",
      "{'loss': 0.0383, 'grad_norm': 0.28629088401794434, 'learning_rate': 0.00016770425383936735, 'epoch': 0.28557829604950025}\n",
      "{'loss': 0.036, 'grad_norm': 0.444154292345047, 'learning_rate': 0.00016540122212950934, 'epoch': 0.2950975725844836}\n",
      "{'loss': 0.0382, 'grad_norm': 0.3330197036266327, 'learning_rate': 0.00016303590941841458, 'epoch': 0.3046168491194669}\n",
      "{'loss': 0.0357, 'grad_norm': 0.2910959720611572, 'learning_rate': 0.00016061056817230754, 'epoch': 0.31413612565445026}\n",
      "{'loss': 0.0362, 'grad_norm': 0.3266875147819519, 'learning_rate': 0.00015812750802205187, 'epoch': 0.3236554021894336}\n",
      "{'loss': 0.0383, 'grad_norm': 0.31103014945983887, 'learning_rate': 0.00015558909356370944, 'epoch': 0.3331746787244169}\n",
      "{'loss': 0.0351, 'grad_norm': 0.28717806935310364, 'learning_rate': 0.00015299774210675657, 'epoch': 0.3426939552594003}\n",
      "{'loss': 0.0364, 'grad_norm': 0.3355398178100586, 'learning_rate': 0.00015035592137210187, 'epoch': 0.35221323179438363}\n",
      "{'loss': 0.035, 'grad_norm': 0.315412312746048, 'learning_rate': 0.0001476661471420975, 'epoch': 0.361732508329367}\n",
      "{'loss': 0.0351, 'grad_norm': 0.2670702040195465, 'learning_rate': 0.00014493098086478196, 'epoch': 0.3712517848643503}\n",
      "{'loss': 0.036, 'grad_norm': 0.27994099259376526, 'learning_rate': 0.00014215302721463623, 'epoch': 0.38077106139933364}\n",
      "{'loss': 0.0359, 'grad_norm': 0.27559319138526917, 'learning_rate': 0.00013933493161217523, 'epoch': 0.390290337934317}\n",
      "{'loss': 0.0365, 'grad_norm': 0.29199907183647156, 'learning_rate': 0.00013647937770473737, 'epoch': 0.39980961446930036}\n",
      "{'loss': 0.0337, 'grad_norm': 0.2851071357727051, 'learning_rate': 0.00013358908481087134, 'epoch': 0.40932889100428366}\n",
      "{'loss': 0.0339, 'grad_norm': 0.29431748390197754, 'learning_rate': 0.0001306668053307531, 'epoch': 0.418848167539267}\n",
      "{'loss': 0.0337, 'grad_norm': 0.2651343047618866, 'learning_rate': 0.00012771532212509974, 'epoch': 0.42836744407425037}\n",
      "{'loss': 0.0333, 'grad_norm': 0.2462613433599472, 'learning_rate': 0.00012473744586507604, 'epoch': 0.43788672060923367}\n",
      "{'loss': 0.0331, 'grad_norm': 0.2778613269329071, 'learning_rate': 0.00012173601235571742, 'epoch': 0.447405997144217}\n",
      "{'loss': 0.0342, 'grad_norm': 0.2680171728134155, 'learning_rate': 0.00011871387983541789, 'epoch': 0.4569252736792004}\n",
      "{'loss': 0.0348, 'grad_norm': 0.2754650413990021, 'learning_rate': 0.0001156739262540552, 'epoch': 0.46644455021418374}\n",
      "{'loss': 0.0361, 'grad_norm': 0.303356409072876, 'learning_rate': 0.00011261904653234485, 'epoch': 0.47596382674916704}\n",
      "{'loss': 0.0334, 'grad_norm': 0.2829611301422119, 'learning_rate': 0.00010955214980503284, 'epoch': 0.4854831032841504}\n",
      "{'loss': 0.0327, 'grad_norm': 0.2828277349472046, 'learning_rate': 0.0001064761566505525, 'epoch': 0.49500237981913375}\n",
      "{'loss': 0.032, 'grad_norm': 0.242219477891922, 'learning_rate': 0.00010339399630978373, 'epoch': 0.504521656354117}\n",
      "{'loss': 0.0345, 'grad_norm': 0.31349000334739685, 'learning_rate': 0.00010030860389656305, 'epoch': 0.5140409328891005}\n",
      "{'loss': 0.0317, 'grad_norm': 0.25605273246765137, 'learning_rate': 9.722291760260077e-05, 'epoch': 0.5235602094240838}\n",
      "{'loss': 0.0311, 'grad_norm': 0.2443927824497223, 'learning_rate': 9.413987589946711e-05, 'epoch': 0.5330794859590671}\n",
      "{'loss': 0.0308, 'grad_norm': 0.22341568768024445, 'learning_rate': 9.106241474031212e-05, 'epoch': 0.5425987624940505}\n",
      "{'loss': 0.0297, 'grad_norm': 0.24119433760643005, 'learning_rate': 8.79934647639835e-05, 'epoch': 0.5521180390290338}\n",
      "{'loss': 0.0312, 'grad_norm': 0.25824546813964844, 'learning_rate': 8.493594850420537e-05, 'epoch': 0.5616373155640171}\n",
      "{'loss': 0.0303, 'grad_norm': 0.25661563873291016, 'learning_rate': 8.189277760647537e-05, 'epoch': 0.5711565920990005}\n",
      "{'loss': 0.0315, 'grad_norm': 0.26515769958496094, 'learning_rate': 7.886685005533072e-05, 'epoch': 0.5806758686339838}\n",
      "{'loss': 0.0333, 'grad_norm': 0.257689505815506, 'learning_rate': 7.586104741462325e-05, 'epoch': 0.5901951451689672}\n",
      "{'loss': 0.0335, 'grad_norm': 0.2499377429485321, 'learning_rate': 7.287823208343192e-05, 'epoch': 0.5997144217039505}\n",
      "{'loss': 0.0316, 'grad_norm': 0.25149062275886536, 'learning_rate': 6.992124457022553e-05, 'epoch': 0.6092336982389338}\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n",
    "trainer.save_model()\n",
    "\n",
    "print( \"Model saved\" )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baf78223621654c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf69f614e0a96699",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## RESTART 1st time & load model and tokenizer in FP16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "775016d75838eede",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T19:47:16.206445Z",
     "start_time": "2025-01-09T19:47:14.419967Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"/usr/local/bin/accelerate\", line 8, in <module>\r\n",
      "    sys.exit(main())\r\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/accelerate_cli.py\", line 48, in main\r\n",
      "    args.func(args)\r\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/estimate.py\", line 286, in estimate_command\r\n",
      "    data = gather_data(args)\r\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/estimate.py\", line 253, in gather_data\r\n",
      "    model = create_empty_model(\r\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/estimate.py\", line 133, in create_empty_model\r\n",
      "    raise ValueError(\r\n",
      "ValueError: Library `vllm` is not supported yet, please open an issue on GitHub for us to add support.\r\n"
     ]
    }
   ],
   "source": [
    "# ! accelerate estimate-memory ministral/Ministral-3b-instruct\n",
    "! accelerate estimate-memory mistralai/Ministral-8B-Instruct-2410"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46ec1294438f94de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T21:09:12.482964Z",
     "start_time": "2025-01-09T21:09:12.476697Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/var/model/models'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir( f\"{models_root}\" )\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ba27f10-cbb6-409b-b811-c96d90732fff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T21:09:14.576592Z",
     "start_time": "2025-01-09T21:09:13.705752Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: transformers\r\n",
      "Version: 4.46.3\r\n",
      "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\r\n",
      "Home-page: https://github.com/huggingface/transformers\r\n",
      "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\r\n",
      "Author-email: transformers@huggingface.co\r\n",
      "License: Apache 2.0 License\r\n",
      "Location: /usr/local/lib/python3.10/dist-packages\r\n",
      "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\r\n",
      "Required-by: autoawq, peft, trl\r\n"
     ]
    }
   ],
   "source": [
    "! pip show transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1cdb4598a75c4f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T21:09:21.273416Z",
     "start_time": "2025-01-09T21:09:16.047514Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading without BitsAndBytesConfig...\n",
      "HF_HOME: /var/model/models\n",
      "HF_HUB_ETAG_TIMEOUT: 60\n",
      "HF_HUB_DOWNLOAD_TIMEOUT: 60\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7df4c7a0a534fffb450c062e32cb547",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We've detected an older driver with an RTX 4000 series GPU. These drivers have issues with P2P. This can affect the multi-gpu inference when using accelerate device_map.Please make sure to update your driver to the latest version which resolves this.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b7dd224f40046f9ba04610a400f6d5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/181k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a438e07b4b44e01bf9d33b1181fb94d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/181k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3309d86c70c74476804985a0c69b0385",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5157d44e04447fba8f3a8238926613e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b38008aa19343658eb84c751eb38af3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/181k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    }
   ],
   "source": [
    "base_model, tokenizer = get_base_model_and_tokenizer( \n",
    "    model_path=model_path,\n",
    "    tokenizer_path=model_path,\n",
    "    use_bnb_quantization=False,\n",
    "    device_map=\"auto\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f27db9a1-083e-4619-9a18-ee47a63381d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T21:09:23.124431Z",
     "start_time": "2025-01-09T21:09:23.118260Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/var/model/models'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "608c8460cbc99d11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T21:09:24.488808Z",
     "start_time": "2025-01-09T21:09:24.344388Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1.7G\r\n",
      "drwxr-xr-x 3 root root 4.0K Jan  9 18:37 .\r\n",
      "drwxrwxr-x 3 1001 1001 4.0K Jan  9 18:44 ..\r\n",
      "-rw-r--r-- 1 root root 5.0K Jan  9 18:37 README.md\r\n",
      "-rw-r--r-- 1 root root  824 Jan  9 18:37 adapter_config.json\r\n",
      "-rw-r--r-- 1 root root 1.7G Jan  9 18:37 adapter_model.safetensors\r\n",
      "drwxr-xr-x 2 root root 4.0K Jan  9 18:37 checkpoint-525\r\n",
      "-rw-r--r-- 1 root root  437 Jan  9 18:37 special_tokens_map.json\r\n",
      "-rw-r--r-- 1 root root  17M Jan  9 18:37 tokenizer.json\r\n",
      "-rw-r--r-- 1 root root 178K Jan  9 18:37 tokenizer_config.json\r\n",
      "-rw-r--r-- 1 root root 5.5K Jan  9 18:37 training_args.bin\r\n"
     ]
    }
   ],
   "source": [
    "! ls -alh /var/model/models/Ministral-8B-Instruct-2410/training-results-2025.01.09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "65c9b0cb2867d4a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T21:09:26.324032Z",
     "start_time": "2025-01-09T21:09:26.318109Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/var/model/models'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[ \"HF_HOME\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a8477c4609548336",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T21:09:27.706190Z",
     "start_time": "2025-01-09T21:09:27.700170Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import logging\n",
    "\n",
    "logging.set_verbosity_debug()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a709da7c7b6997b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T21:09:30.758562Z",
     "start_time": "2025-01-09T21:09:30.623507Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/var/model/models\r\n"
     ]
    }
   ],
   "source": [
    "! echo $HF_HOME "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4333e760449e1a12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T21:09:38.939524Z",
     "start_time": "2025-01-09T21:09:38.797292Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 20K\r\n",
      "drwxr-xr-x 4 root root 4.0K Jan  9 21:09 .\r\n",
      "drwxr-xr-x 3 root root 4.0K Jan  9 21:08 ..\r\n",
      "drwxr-xr-x 3 root root 4.0K Jan  9 21:09 .locks\r\n",
      "drwxr-xr-x 6 root root 4.0K Jan  9 21:09 models--mistralai--Ministral-8B-Instruct-2410\r\n",
      "-rw-r--r-- 1 root root    1 Jan  9 21:08 version.txt\r\n"
     ]
    }
   ],
   "source": [
    "! ls -alh /root/.cache/huggingface/hub/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34796358a34b99c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T21:09:44.057070Z",
     "start_time": "2025-01-09T21:09:41.045986Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected flash_attn version: 2.7.2.post1\n",
      "Detected flash_attn version: 2.7.2.post1\n",
      "Detected flash_attn version: 2.7.2.post1\n",
      "Detected flash_attn version: 2.7.2.post1\n",
      "Detected flash_attn version: 2.7.2.post1\n",
      "Detected flash_attn version: 2.7.2.post1\n",
      "Detected flash_attn version: 2.7.2.post1\n",
      "Detected flash_attn version: 2.7.2.post1\n",
      "Detected flash_attn version: 2.7.2.post1\n",
      "Detected flash_attn version: 2.7.2.post1\n",
      "Detected flash_attn version: 2.7.2.post1\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel, AutoPeftModelForCausalLM\n",
    "\n",
    "# adapter_plus_model = PeftModel.from_pretrained( base_model, \"Mistral-7B-Instruct-v0.2/training-results-2024.02.05/\", use_flash_attention_2=True )\n",
    "# adapter_plus_model = PeftModel.from_pretrained( base_model, \"Ministral-3b-instruct/training-results-2024.12.14/\", use_flash_attention_2=True )\n",
    "adapter_plus_model = PeftModel.from_pretrained( base_model, \"Ministral-8B-Instruct-2410/training-results-2025.01.09/\", use_flash_attention_2=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a4d57dcf172749f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T21:09:46.071630Z",
     "start_time": "2025-01-09T21:09:46.057714Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.model.embed_tokens.weight: cuda:0\n",
      "base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.0.mlp.gate_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.0.mlp.up_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.0.mlp.down_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.0.input_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.0.post_attention_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.1.mlp.gate_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.1.mlp.up_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.1.mlp.down_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.1.input_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.1.post_attention_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.2.mlp.gate_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.2.mlp.up_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.2.mlp.down_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.2.input_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.2.post_attention_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.3.mlp.gate_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.3.mlp.up_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.3.mlp.down_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.3.input_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.3.post_attention_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.4.mlp.gate_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.4.mlp.up_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.4.mlp.down_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.4.input_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.4.post_attention_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.5.mlp.gate_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.5.mlp.up_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.5.mlp.down_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.5.input_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.5.post_attention_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.6.mlp.gate_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.6.mlp.up_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.6.mlp.down_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.6.input_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.6.post_attention_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.7.mlp.gate_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.7.mlp.up_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.7.mlp.down_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.7.input_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.7.post_attention_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.8.mlp.gate_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.8.mlp.up_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.8.mlp.down_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.8.input_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.8.post_attention_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.9.mlp.gate_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.9.mlp.up_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.9.mlp.down_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.9.input_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.9.post_attention_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.10.mlp.gate_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.10.mlp.up_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.10.mlp.down_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.10.input_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.10.post_attention_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.11.mlp.gate_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.11.mlp.up_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.11.mlp.down_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.11.input_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.11.post_attention_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.12.mlp.gate_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.12.mlp.up_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.12.mlp.down_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.12.input_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.12.post_attention_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.13.mlp.gate_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.13.mlp.up_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.13.mlp.down_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.13.input_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.13.post_attention_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.14.mlp.gate_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.14.mlp.up_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.14.mlp.down_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.14.input_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.14.post_attention_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.15.mlp.gate_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.15.mlp.up_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.15.mlp.down_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.15.input_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.15.post_attention_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.16.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.16.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.16.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.16.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.16.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.16.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.16.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.17.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.17.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.17.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.17.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.17.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.17.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.17.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.18.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.18.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.18.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.18.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.18.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.18.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.18.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.19.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.19.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.19.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.19.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.19.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.19.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.19.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.20.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.20.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.20.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.20.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.20.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.20.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.20.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.21.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.22.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.23.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.24.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.24.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.24.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.24.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.25.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.25.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.25.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.25.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.26.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.26.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.26.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.26.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.27.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.27.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.27.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.27.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.28.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.28.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.28.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.28.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.29.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.29.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.29.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.29.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.30.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.30.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.30.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.30.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.31.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.31.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.31.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.31.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.32.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.32.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.32.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.32.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.32.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.32.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.32.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.32.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.32.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.32.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.32.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.32.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.32.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.32.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.32.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.32.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.32.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.32.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.32.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.32.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.32.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.32.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.32.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.33.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.33.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.33.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.33.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.33.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.33.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.33.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.33.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.33.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.33.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.33.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.33.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.33.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.33.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.33.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.33.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.33.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.33.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.33.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.33.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.33.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.33.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.33.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.34.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.34.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.34.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.34.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.34.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.34.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.34.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.34.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.34.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.34.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.34.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.34.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.34.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.34.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.34.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.34.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.34.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.34.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.34.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.34.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.34.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.34.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.34.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.35.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.35.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.35.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.35.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.35.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.35.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.35.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.35.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.35.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.35.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.35.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.35.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.35.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.35.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.35.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.35.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.35.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.35.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.35.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.35.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.35.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.35.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.35.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.norm.weight: cuda:1\n",
      "base_model.model.lm_head.base_layer.weight: cuda:1\n",
      "base_model.model.lm_head.lora_A.default.weight: cuda:1\n",
      "base_model.model.lm_head.lora_B.default.weight: cuda:1\n"
     ]
    }
   ],
   "source": [
    "dupt.print_device_allocation( adapter_plus_model )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78308e05900f5eef",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## TEST model on validation dataset using adapter loaded on top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6b4abb04bf27ab82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T21:50:20.465217Z",
     "start_time": "2025-01-09T21:11:10.422085Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Validating Ministral-8B-Instruct-2410 w/ 1000 samples\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "command\n",
      "search google new tab                                58\n",
      "search phind current tab                             58\n",
      "go to current tab                                    57\n",
      "agent router go to weather                           55\n",
      "search google scholar current tab                    55\n",
      "search phind new tab                                 53\n",
      "go to new tab                                        53\n",
      "search kagi current tab                              52\n",
      "agent router go to date and time                     52\n",
      "search google current tab                            49\n",
      "search kagi new tab                                  48\n",
      "agent router go to receptionist                      48\n",
      "search current tab                                   48\n",
      "search new tab                                       47\n",
      "search perplexity new tab                            47\n",
      "agent router go to calendar                          46\n",
      "search perplexity current tab                        45\n",
      "search google scholar new tab                        44\n",
      "none                                                 10\n",
      "agent router go to math                               9\n",
      "search google scholar using clipboard current tab     8\n",
      "search phind using clipboard new tab                  7\n",
      "agent router go to todo list                          6\n",
      "search google using clipboard current tab             6\n",
      "search perplexity using clipboard new tab             6\n",
      "search kagi using clipboard new tab                   5\n",
      "search phind using clipboard current tab              5\n",
      "search google using clipboard new tab                 5\n",
      "search perplexity using clipboard current tab         4\n",
      "search using clipboard new tab                        4\n",
      "search kagi using clipboard current tab               4\n",
      "search google scholar using clipboard new tab         4\n",
      "search using clipboard current tab                    2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Instantiating ConfigurationManager() singleton...\n",
      "\n",
      "Using environment variables to instantiate configuration manager\n",
      "[0]th arg = [config_path=/src/conf/gib-app.ini]... done!\n",
      "[1]th arg = [splainer_path=/src/conf/gib-app-splainer.ini]... done!\n",
      "[2]th arg = [config_block_id=Genie+in+the+Box:+Development]... done!\n",
      "\n",
      "Name value dictionary pairs:\n",
      "\n",
      "[ config_block_id] = [Genie in the Box: Development]\n",
      "[     config_path] = [/src/conf/gib-app.ini]\n",
      "[   splainer_path] = [/src/conf/gib-app-splainer.ini]\n",
      "\n",
      "File exists! [/var/model/genie-in-the-box/src/conf/gib-app.ini]\n",
      "File exists! [/var/model/genie-in-the-box/src/conf/gib-app-splainer.ini]\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Initializing configuration_manager [/var/model/genie-in-the-box/src/conf/gib-app.ini]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Splainer path [/var/model/genie-in-the-box/src/conf/gib-app-splainer.ini]\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Sections, '*' = current block ID\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "  Genie in the Box: Baseline\n",
      "* Genie in the Box: Development\n",
      "  Genie in the Box: Production\n",
      "  default\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Calculating inheritance... * = parent block\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "* [Genie in the Box: Development] inherits from [Genie in the Box: Baseline]\n",
      "Scanning for immutable keys...\n",
      "Scanning for immutable keys... Done!\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Calculating defaults...\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Loading splainer file [/var/model/genie-in-the-box/src/conf/gib-app-splainer.ini]...\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Inserting DUPLICATE placeholders into the list. Requested length [500] > list length [74]\n",
      "Inserting DUPLICATE placeholders into the list. Requested length [500] > list length [148]\n",
      "Inserting DUPLICATE placeholders into the list. Requested length [500] > list length [296]\n",
      "Commands file for command [go to current tab] exists: True\n",
      "Commands file for command [go to new tab] exists: True\n",
      "Commands file for command [search current tab] exists: True\n",
      "Commands file for command [search new tab] exists: True\n",
      "Commands file for command [search google current tab] exists: True\n",
      "Commands file for command [search google new tab] exists: True\n",
      "Commands file for command [search google scholar current tab] exists: True\n",
      "Commands file for command [search google scholar new tab] exists: True\n",
      "Commands file for command [search kagi new tab] exists: True\n",
      "Commands file for command [search kagi current tab] exists: True\n",
      "Commands file for command [search perplexity current tab] exists: True\n",
      "Commands file for command [search perplexity new tab] exists: True\n",
      "Commands file for command [search phind current tab] exists: True\n",
      "Commands file for command [search phind new tab] exists: True\n",
      "\n",
      "Commands file for command [search using clipboard current tab] exists: True\n",
      "Commands file for command [search using clipboard new tab] exists: True\n",
      "Commands file for command [search google using clipboard current tab] exists: True\n",
      "Commands file for command [search google using clipboard new tab] exists: True\n",
      "Commands file for command [search google scholar using clipboard current tab] exists: True\n",
      "Commands file for command [search google scholar using clipboard new tab] exists: True\n",
      "Commands file for command [search kagi using clipboard current tab] exists: True\n",
      "Commands file for command [search kagi using clipboard new tab] exists: True\n",
      "Commands file for command [search perplexity using clipboard current tab] exists: True\n",
      "Commands file for command [search perplexity using clipboard new tab] exists: True\n",
      "Commands file for command [search phind using clipboard current tab] exists: True\n",
      "Commands file for command [search phind using clipboard new tab] exists: True\n",
      "Commands file for command [none] exists: True\n",
      "\n",
      "Commands file for command [agent router go to search function mapping] exists: True\n",
      "\n",
      "Commands file for command [agent router go to date and time] exists: True\n",
      "Commands file for command [agent router go to weather] exists: True\n",
      "Commands file for command [agent router go to calendar] exists: True\n",
      "Commands file for command [agent router go to receptionist] exists: True\n",
      "\n",
      "Commands file for command [agent router go to todo list] exists: True\n",
      "Commands file for command [agent router go to math] exists: True\n",
      "Commands file for command [none] exists: True\n",
      "\n",
      "Generating responses for 1,000 rows...\n",
      "Using HuggingFace model_name [Ministral-8B-Instruct-2410] in memory...\n",
      "\n",
      "Processing call [001] out of [1000] = [0.1%]... ETA: 0 seconds\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 3,024 ms\n",
      "Tokens per second [11.2] input tokens [604] + xml response tokens [34] = total tokens i/o [638]\n",
      "Response: [<response><command>search google new tab</command><args>Connection Refused Error</args></response>]\n",
      "\n",
      "Processing call [002] out of [1000] = [0.2%]... ETA mm:ss 25:09\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,134 ms\n",
      "Tokens per second [15.9] input tokens [375] + xml response tokens [34] = total tokens i/o [409]\n",
      "Response: [<response><command>agent router go to receptionist</command><args>Info Clerk</args></response>]\n",
      "\n",
      "Processing call [003] out of [1000] = [0.3%]... ETA mm:ss 28:34\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,640 ms\n",
      "Tokens per second [15.5] input tokens [607] + xml response tokens [41] = total tokens i/o [648]\n",
      "Response: [<response><command>search kagi current tab</command><args>Unbound Local Error: Local variable referenced before assignment</args></response>]\n",
      "\n",
      "Processing call [004] out of [1000] = [0.4%]... ETA mm:ss 32:22\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,668 ms\n",
      "Tokens per second [15.7] input tokens [612] + xml response tokens [42] = total tokens i/o [654]\n",
      "Response: [<response><command>search google current tab</command><args>How can system exit errors be handled gracefully in Python applications?</args></response>]\n",
      "\n",
      "Processing call [005] out of [1000] = [0.5%]... ETA mm:ss 34:43\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,947 ms\n",
      "Tokens per second [15.6] input tokens [618] + xml response tokens [46] = total tokens i/o [664]\n",
      "Response: [<response><command>search perplexity new tab</command><args>AI for health diagnostics: How does AI contribute to advancements in health diagnostics?</args></response>]\n",
      "\n",
      "Processing call [006] out of [1000] = [0.6%]... ETA mm:ss 37:02\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,423 ms\n",
      "Tokens per second [15.7] input tokens [607] + xml response tokens [38] = total tokens i/o [645]\n",
      "Response: [<response><command>search kagi current tab</command><args>Saving Pandas DataFrame to CSV</args></response>]\n",
      "\n",
      "Processing call [007] out of [1000] = [0.7%]... ETA mm:ss 37:27\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,103 ms\n",
      "Tokens per second [15.2] input tokens [612] + xml response tokens [32] = total tokens i/o [644]\n",
      "Response: [<response><command>search google scholar using clipboard current tab</command><args></args></response>]\n",
      "\n",
      "Processing call [008] out of [1000] = [0.8%]... ETA mm:ss 37:05\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,932 ms\n",
      "Tokens per second [15.7] input tokens [616] + xml response tokens [46] = total tokens i/o [662]\n",
      "Response: [<response><command>search phind current tab</command><args>What are bytes warnings in Python, and how are they significant in data handling?</args></response>]\n",
      "\n",
      "Processing call [009] out of [1000] = [0.9%]... ETA mm:ss 38:19\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,463 ms\n",
      "Tokens per second [15.4] input tokens [369] + xml response tokens [38] = total tokens i/o [407]\n",
      "Response: [<response><command>agent router go to date and time</command><args>Fremont, California</args></response>]\n",
      "\n",
      "Processing call [010] out of [1000] = [1.0%]... ETA mm:ss 38:30\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,348 ms\n",
      "Tokens per second [15.8] input tokens [605] + xml response tokens [37] = total tokens i/o [642]\n",
      "Response: [<response><command>go to current tab</command><args>login.excitingunicorn.net</args></response>]\n",
      "\n",
      "Processing call [011] out of [1000] = [1.1%]... ETA mm:ss 38:29\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,887 ms\n",
      "Tokens per second [15.9] input tokens [361] + xml response tokens [30] = total tokens i/o [391]\n",
      "Response: [<response><command>agent router go to calendar</command><args></args></response>]\n",
      "\n",
      "Processing call [012] out of [1000] = [1.2%]... ETA mm:ss 37:50\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,222 ms\n",
      "Tokens per second [15.8] input tokens [605] + xml response tokens [35] = total tokens i/o [640]\n",
      "Response: [<response><command>search kagi new tab</command><args>Connection Refused Error</args></response>]\n",
      "\n",
      "Processing call [013] out of [1000] = [1.3%]... ETA mm:ss 37:42\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,161 ms\n",
      "Tokens per second [15.7] input tokens [599] + xml response tokens [34] = total tokens i/o [633]\n",
      "Response: [<response><command>search perplexity current tab</command><args>Broken Pipe Error</args></response>]\n",
      "\n",
      "Processing call [014] out of [1000] = [1.4%]... ETA mm:ss 37:31\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,096 ms\n",
      "Tokens per second [15.7] input tokens [606] + xml response tokens [33] = total tokens i/o [639]\n",
      "Response: [<response><command>search kagi new tab</command><args>ImportWarning</args></response>]\n",
      "\n",
      "Processing call [015] out of [1000] = [1.5%]... ETA mm:ss 37:16\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,225 ms\n",
      "Tokens per second [15.7] input tokens [607] + xml response tokens [35] = total tokens i/o [642]\n",
      "Response: [<response><command>search google new tab</command><args>best movies of all time</args></response>]\n",
      "\n",
      "Processing call [016] out of [1000] = [1.6%]... ETA mm:ss 37:11\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,722 ms\n",
      "Tokens per second [15.8] input tokens [609] + xml response tokens [43] = total tokens i/o [652]\n",
      "Response: [<response><command>search google scholar current tab</command><args>How do you fix indentation errors that affect code structure in Python?</args></response>]\n",
      "\n",
      "Processing call [017] out of [1000] = [1.7%]... ETA mm:ss 37:35\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,355 ms\n",
      "Tokens per second [15.7] input tokens [606] + xml response tokens [37] = total tokens i/o [643]\n",
      "Response: [<response><command>go to new tab</command><args>blog.jubilantquartz.com</args></response>]\n",
      "\n",
      "Processing call [018] out of [1000] = [1.8%]... ETA mm:ss 37:36\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,257 ms\n",
      "Tokens per second [16.0] input tokens [371] + xml response tokens [36] = total tokens i/o [407]\n",
      "Response: [<response><command>agent router go to weather</command><args>Brussels, Belgium</args></response>]\n",
      "\n",
      "Processing call [019] out of [1000] = [1.9%]... ETA mm:ss 37:32\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,942 ms\n",
      "Tokens per second [16.0] input tokens [364] + xml response tokens [31] = total tokens i/o [395]\n",
      "Response: [<response><command>agent router go to todo list</command><args></args></response>]\n",
      "\n",
      "Processing call [020] out of [1000] = [2.0%]... ETA mm:ss 37:12\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,878 ms\n",
      "Tokens per second [16.0] input tokens [378] + xml response tokens [30] = total tokens i/o [408]\n",
      "Response: [<response><command>agent router go to calendar</command><args></args></response>]\n",
      "\n",
      "Processing call [021] out of [1000] = [2.1%]... ETA mm:ss 36:51\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,727 ms\n",
      "Tokens per second [15.8] input tokens [615] + xml response tokens [43] = total tokens i/o [658]\n",
      "Response: [<response><command>search phind new tab</command><args>File Not Found Error: File not found or path is incorrect</args></response>]\n",
      "\n",
      "Processing call [022] out of [1000] = [2.2%]... ETA mm:ss 37:10\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,285 ms\n",
      "Tokens per second [15.8] input tokens [606] + xml response tokens [36] = total tokens i/o [642]\n",
      "Response: [<response><command>search google new tab</command><args>how to make homemade bread</args></response>]\n",
      "\n",
      "Processing call [023] out of [1000] = [2.3%]... ETA mm:ss 37:08\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,979 ms\n",
      "Tokens per second [15.8] input tokens [615] + xml response tokens [47] = total tokens i/o [662]\n",
      "Response: [<response><command>search google new tab</command><args>Neural Network hyperparameter tuning: What approaches are recommended for neural network hyperparameter tuning?</args></response>]\n",
      "\n",
      "Processing call [024] out of [1000] = [2.4%]... ETA mm:ss 37:34\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,851 ms\n",
      "Tokens per second [15.8] input tokens [618] + xml response tokens [45] = total tokens i/o [663]\n",
      "Response: [<response><command>search phind new tab</command><args>What are common causes and solutions for errors related to incorrect syntax in Python?</args></response>]\n",
      "\n",
      "Processing call [025] out of [1000] = [2.5%]... ETA mm:ss 37:53\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,416 ms\n",
      "Tokens per second [15.7] input tokens [605] + xml response tokens [38] = total tokens i/o [643]\n",
      "Response: [<response><command>go to current tab</command><args>beta.remarkablezebra.info</args></response>]\n",
      "\n",
      "Processing call [026] out of [1000] = [2.6%]... ETA mm:ss 37:54\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,726 ms\n",
      "Tokens per second [15.8] input tokens [612] + xml response tokens [43] = total tokens i/o [655]\n",
      "Response: [<response><command>search perplexity current tab</command><args>How can you resolve errors related to ZIP file operations in Python?</args></response>]\n",
      "\n",
      "Processing call [027] out of [1000] = [2.7%]... ETA mm:ss 38:05\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,355 ms\n",
      "Tokens per second [15.7] input tokens [609] + xml response tokens [37] = total tokens i/o [646]\n",
      "Response: [<response><command>search phind new tab</command><args>Reading CSV files in Pandas</args></response>]\n",
      "\n",
      "Processing call [028] out of [1000] = [2.8%]... ETA mm:ss 38:03\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,947 ms\n",
      "Tokens per second [15.9] input tokens [367] + xml response tokens [31] = total tokens i/o [398]\n",
      "Response: [<response><command>agent router go to todo list</command><args></args></response>]\n",
      "\n",
      "Processing call [029] out of [1000] = [2.9%]... ETA mm:ss 37:47\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,284 ms\n",
      "Tokens per second [15.8] input tokens [608] + xml response tokens [36] = total tokens i/o [644]\n",
      "Response: [<response><command>search google new tab</command><args>JavaScript libraries for beginners</args></response>]\n",
      "\n",
      "Processing call [030] out of [1000] = [3.0%]... ETA mm:ss 37:43\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,371 ms\n",
      "Tokens per second [15.6] input tokens [604] + xml response tokens [37] = total tokens i/o [641]\n",
      "Response: [<response><command>go to new tab</command><args>login.fantasticvolcano.org</args></response>]\n",
      "\n",
      "Processing call [031] out of [1000] = [3.1%]... ETA mm:ss 37:42\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,344 ms\n",
      "Tokens per second [15.8] input tokens [605] + xml response tokens [37] = total tokens i/o [642]\n",
      "Response: [<response><command>search perplexity current tab</command><args>best vacation spots in the world</args></response>]\n",
      "\n",
      "Processing call [032] out of [1000] = [3.2%]... ETA mm:ss 37:40\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,192 ms\n",
      "Tokens per second [16.0] input tokens [363] + xml response tokens [35] = total tokens i/o [398]\n",
      "Response: [<response><command>agent router go to receptionist</command><args>Office Coordinator Agent</args></response>]\n",
      "\n",
      "Processing call [033] out of [1000] = [3.3%]... ETA mm:ss 37:34\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,881 ms\n",
      "Tokens per second [15.9] input tokens [361] + xml response tokens [30] = total tokens i/o [391]\n",
      "Response: [<response><command>agent router go to math</command><args></args></response>]\n",
      "\n",
      "Processing call [034] out of [1000] = [3.4%]... ETA mm:ss 37:19\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,116 ms\n",
      "Tokens per second [15.6] input tokens [599] + xml response tokens [33] = total tokens i/o [632]\n",
      "Response: [<response><command>search google current tab</command><args>best online games</args></response>]\n",
      "\n",
      "Processing call [035] out of [1000] = [3.5%]... ETA mm:ss 37:11\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 3,032 ms\n",
      "Tokens per second [15.8] input tokens [620] + xml response tokens [48] = total tokens i/o [668]\n",
      "Response: [<response><command>search phind new tab</command><args>Google Cloud AI services: What AI services does Google Cloud offer for developers and data scientists?</args></response>]\n",
      "\n",
      "Processing call [036] out of [1000] = [3.6%]... ETA mm:ss 37:28\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,780 ms\n",
      "Tokens per second [15.8] input tokens [613] + xml response tokens [44] = total tokens i/o [657]\n",
      "Response: [<response><command>search kagi current tab</command><args>Deep Learning optimization: What strategies are used for optimizing deep learning models?</args></response>]\n",
      "\n",
      "Processing call [037] out of [1000] = [3.7%]... ETA mm:ss 37:37\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,410 ms\n",
      "Tokens per second [15.8] input tokens [607] + xml response tokens [38] = total tokens i/o [645]\n",
      "Response: [<response><command>go to new tab</command><args>blog.hilariouspenguin.gov</args></response>]\n",
      "\n",
      "Processing call [038] out of [1000] = [3.8%]... ETA mm:ss 37:36\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,413 ms\n",
      "Tokens per second [15.7] input tokens [607] + xml response tokens [38] = total tokens i/o [645]\n",
      "Response: [<response><command>search google scholar new tab</command><args>Type Error: Incorrect type comparison</args></response>]\n",
      "\n",
      "Processing call [039] out of [1000] = [3.9%]... ETA mm:ss 37:36\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,354 ms\n",
      "Tokens per second [15.7] input tokens [606] + xml response tokens [37] = total tokens i/o [643]\n",
      "Response: [<response><command>go to new tab</command><args>login.hilariousrainbow.io</args></response>]\n",
      "\n",
      "Processing call [040] out of [1000] = [4.0%]... ETA mm:ss 37:34\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,347 ms\n",
      "Tokens per second [15.8] input tokens [603] + xml response tokens [37] = total tokens i/o [640]\n",
      "Response: [<response><command>search google scholar current tab</command><args>JavaScript libraries for beginners</args></response>]\n",
      "\n",
      "Processing call [041] out of [1000] = [4.1%]... ETA mm:ss 37:31\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,346 ms\n",
      "Tokens per second [15.8] input tokens [602] + xml response tokens [37] = total tokens i/o [639]\n",
      "Response: [<response><command>search google scholar current tab</command><args>Neural Network hyperparameter tuning</args></response>]\n",
      "\n",
      "Processing call [042] out of [1000] = [4.2%]... ETA mm:ss 37:29\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,159 ms\n",
      "Tokens per second [15.7] input tokens [601] + xml response tokens [34] = total tokens i/o [635]\n",
      "Response: [<response><command>search google scholar current tab</command><args>best online games</args></response>]\n",
      "\n",
      "Processing call [043] out of [1000] = [4.3%]... ETA mm:ss 37:22\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,726 ms\n",
      "Tokens per second [15.8] input tokens [609] + xml response tokens [43] = total tokens i/o [652]\n",
      "Response: [<response><command>search kagi current tab</command><args>File Not Found Error: File not found or path is incorrect</args></response>]\n",
      "\n",
      "Processing call [044] out of [1000] = [4.4%]... ETA mm:ss 37:28\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,315 ms\n",
      "Tokens per second [16.0] input tokens [372] + xml response tokens [37] = total tokens i/o [409]\n",
      "Response: [<response><command>agent router go to date and time</command><args>Denver, Colorado</args></response>]\n",
      "\n",
      "Processing call [045] out of [1000] = [4.5%]... ETA mm:ss 37:25\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,158 ms\n",
      "Tokens per second [15.8] input tokens [606] + xml response tokens [34] = total tokens i/o [640]\n",
      "Response: [<response><command>search kagi new tab</command><args>learning Japanese online</args></response>]\n",
      "\n",
      "Processing call [046] out of [1000] = [4.6%]... ETA mm:ss 37:19\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,035 ms\n",
      "Tokens per second [15.7] input tokens [600] + xml response tokens [32] = total tokens i/o [632]\n",
      "Response: [<response><command>search current tab</command><args>SQL database management</args></response>]\n",
      "\n",
      "Processing call [047] out of [1000] = [4.7%]... ETA mm:ss 37:10\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,187 ms\n",
      "Tokens per second [16.0] input tokens [362] + xml response tokens [35] = total tokens i/o [397]\n",
      "Response: [<response><command>agent router go to weather</command><args>Stockton, California</args></response>]\n",
      "\n",
      "Processing call [048] out of [1000] = [4.8%]... ETA mm:ss 37:05\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,319 ms\n",
      "Tokens per second [16.0] input tokens [368] + xml response tokens [37] = total tokens i/o [405]\n",
      "Response: [<response><command>agent router go to weather</command><args>Ulaanbaatar, Mongolia</args></response>]\n",
      "\n",
      "Processing call [049] out of [1000] = [4.9%]... ETA mm:ss 37:02\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,160 ms\n",
      "Tokens per second [15.7] input tokens [606] + xml response tokens [34] = total tokens i/o [640]\n",
      "Response: [<response><command>search phind new tab</command><args>Broken Pipe Error</args></response>]\n",
      "\n",
      "Processing call [050] out of [1000] = [5.0%]... ETA mm:ss 36:56\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,340 ms\n",
      "Tokens per second [15.8] input tokens [605] + xml response tokens [37] = total tokens i/o [642]\n",
      "Response: [<response><command>go to new tab</command><args>blog.jubilantquartz.com</args></response>]\n",
      "\n",
      "Processing call [051] out of [1000] = [5.1%]... ETA mm:ss 36:54\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,877 ms\n",
      "Tokens per second [16.0] input tokens [366] + xml response tokens [30] = total tokens i/o [396]\n",
      "Response: [<response><command>agent router go to calendar</command><args></args></response>]\n",
      "\n",
      "Processing call [052] out of [1000] = [5.2%]... ETA mm:ss 36:44\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,094 ms\n",
      "Tokens per second [15.8] input tokens [599] + xml response tokens [33] = total tokens i/o [632]\n",
      "Response: [<response><command>search perplexity current tab</command><args>Runtime Error</args></response>]\n",
      "\n",
      "Processing call [053] out of [1000] = [5.3%]... ETA mm:ss 36:37\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,282 ms\n",
      "Tokens per second [15.8] input tokens [609] + xml response tokens [36] = total tokens i/o [645]\n",
      "Response: [<response><command>search new tab</command><args>Saving Pandas DataFrame to CSV</args></response>]\n",
      "\n",
      "Processing call [054] out of [1000] = [5.4%]... ETA mm:ss 36:34\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,891 ms\n",
      "Tokens per second [15.9] input tokens [403] + xml response tokens [30] = total tokens i/o [433]\n",
      "Response: [<response><command>agent router go to calendar</command><args></args></response>]\n",
      "\n",
      "Processing call [055] out of [1000] = [5.5%]... ETA mm:ss 36:24\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,348 ms\n",
      "Tokens per second [15.8] input tokens [605] + xml response tokens [37] = total tokens i/o [642]\n",
      "Response: [<response><command>go to new tab</command><args>beta.beautifulvolcano.org</args></response>]\n",
      "\n",
      "Processing call [056] out of [1000] = [5.6%]... ETA mm:ss 36:23\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 3,097 ms\n",
      "Tokens per second [15.8] input tokens [620] + xml response tokens [49] = total tokens i/o [669]\n",
      "Response: [<response><command>search google current tab</command><args>TensorFlow latest updates: What are the latest features added to TensorFlow in its most recent update?</args></response>]\n",
      "\n",
      "Processing call [057] out of [1000] = [5.7%]... ETA mm:ss 36:33\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,345 ms\n",
      "Tokens per second [15.8] input tokens [601] + xml response tokens [37] = total tokens i/o [638]\n",
      "Response: [<response><command>go to current tab</command><args>test.spectacularxylophone.com</args></response>]\n",
      "\n",
      "Processing call [058] out of [1000] = [5.8%]... ETA mm:ss 36:31\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,892 ms\n",
      "Tokens per second [15.9] input tokens [421] + xml response tokens [30] = total tokens i/o [451]\n",
      "Response: [<response><command>agent router go to calendar</command><args></args></response>]\n",
      "\n",
      "Processing call [059] out of [1000] = [5.9%]... ETA mm:ss 36:22\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,158 ms\n",
      "Tokens per second [15.8] input tokens [600] + xml response tokens [34] = total tokens i/o [634]\n",
      "Response: [<response><command>search google scholar current tab</command><args>SQL database management</args></response>]\n",
      "\n",
      "Processing call [060] out of [1000] = [6.0%]... ETA mm:ss 36:17\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,350 ms\n",
      "Tokens per second [15.7] input tokens [604] + xml response tokens [37] = total tokens i/o [641]\n",
      "Response: [<response><command>search perplexity current tab</command><args>Reinforcement Learning breakthroughs</args></response>]\n",
      "\n",
      "Processing call [061] out of [1000] = [6.1%]... ETA mm:ss 36:16\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,348 ms\n",
      "Tokens per second [15.8] input tokens [606] + xml response tokens [37] = total tokens i/o [643]\n",
      "Response: [<response><command>go to new tab</command><args>blog.jubilantunicorn.io</args></response>]\n",
      "\n",
      "Processing call [062] out of [1000] = [6.2%]... ETA mm:ss 36:14\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,848 ms\n",
      "Tokens per second [15.8] input tokens [610] + xml response tokens [45] = total tokens i/o [655]\n",
      "Response: [<response><command>search perplexity current tab</command><args>Voice Recognition technologies: What are the latest advancements in voice recognition technology?</args></response>]\n",
      "\n",
      "Processing call [063] out of [1000] = [6.3%]... ETA mm:ss 36:19\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,195 ms\n",
      "Tokens per second [15.9] input tokens [387] + xml response tokens [35] = total tokens i/o [422]\n",
      "Response: [<response><command>agent router go to receptionist</command><args>Info Clerk Agent</args></response>]\n",
      "\n",
      "Processing call [064] out of [1000] = [6.4%]... ETA mm:ss 36:15\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,097 ms\n",
      "Tokens per second [15.7] input tokens [603] + xml response tokens [33] = total tokens i/o [636]\n",
      "Response: [<response><command>search perplexity new tab</command><args>RuntimeWarning</args></response>]\n",
      "\n",
      "Processing call [065] out of [1000] = [6.5%]... ETA mm:ss 36:09\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,654 ms\n",
      "Tokens per second [15.8] input tokens [610] + xml response tokens [42] = total tokens i/o [652]\n",
      "Response: [<response><command>search perplexity current tab</command><args>AI in gaming: How is AI changing the gaming industry?</args></response>]\n",
      "\n",
      "Processing call [066] out of [1000] = [6.6%]... ETA mm:ss 36:12\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,964 ms\n",
      "Tokens per second [15.9] input tokens [618] + xml response tokens [47] = total tokens i/o [665]\n",
      "Response: [<response><command>search google new tab</command><args>Google Cloud AI services: What AI services does Google Cloud offer for developers and data scientists?</args></response>]\n",
      "\n",
      "Processing call [067] out of [1000] = [6.7%]... ETA mm:ss 36:18\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,879 ms\n",
      "Tokens per second [16.0] input tokens [361] + xml response tokens [30] = total tokens i/o [391]\n",
      "Response: [<response><command>agent router go to math</command><args></args></response>]\n",
      "\n",
      "Processing call [068] out of [1000] = [6.8%]... ETA mm:ss 36:10\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,155 ms\n",
      "Tokens per second [15.8] input tokens [601] + xml response tokens [34] = total tokens i/o [635]\n",
      "Response: [<response><command>search kagi new tab</command><args>hip hop music</args></response>]\n",
      "\n",
      "Processing call [069] out of [1000] = [6.9%]... ETA mm:ss 36:05\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,470 ms\n",
      "Tokens per second [15.8] input tokens [608] + xml response tokens [39] = total tokens i/o [647]\n",
      "Response: [<response><command>go to new tab</command><args>dev.magnificentstrawberry.org</args></response>]\n",
      "\n",
      "Processing call [070] out of [1000] = [7.0%]... ETA mm:ss 36:05\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,223 ms\n",
      "Tokens per second [15.7] input tokens [600] + xml response tokens [35] = total tokens i/o [635]\n",
      "Response: [<response><command>go to current tab</command><args>amazingiceberg.org</args></response>]\n",
      "\n",
      "Processing call [071] out of [1000] = [7.1%]... ETA mm:ss 36:01\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,878 ms\n",
      "Tokens per second [16.0] input tokens [377] + xml response tokens [30] = total tokens i/o [407]\n",
      "Response: [<response><command>agent router go to calendar</command><args></args></response>]\n",
      "\n",
      "Processing call [072] out of [1000] = [7.2%]... ETA mm:ss 35:53\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,250 ms\n",
      "Tokens per second [16.0] input tokens [364] + xml response tokens [36] = total tokens i/o [400]\n",
      "Response: [<response><command>agent router go to date and time</command><args>Sydney, Australia</args></response>]\n",
      "\n",
      "Processing call [073] out of [1000] = [7.3%]... ETA mm:ss 35:50\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,718 ms\n",
      "Tokens per second [15.8] input tokens [612] + xml response tokens [43] = total tokens i/o [655]\n",
      "Response: [<response><command>search kagi current tab</command><args>How can system exit errors be handled gracefully in Python applications?</args></response>]\n",
      "\n",
      "Processing call [074] out of [1000] = [7.4%]... ETA mm:ss 35:52\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,905 ms\n",
      "Tokens per second [15.8] input tokens [616] + xml response tokens [46] = total tokens i/o [662]\n",
      "Response: [<response><command>search google scholar new tab</command><args>Quantum AI computing developments: What are the latest developments in quantum AI computing?</args></response>]\n",
      "\n",
      "Processing call [075] out of [1000] = [7.5%]... ETA mm:ss 35:57\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,405 ms\n",
      "Tokens per second [15.8] input tokens [606] + xml response tokens [38] = total tokens i/o [644]\n",
      "Response: [<response><command>go to current tab</command><args>beta.spectacularzebra.com</args></response>]\n",
      "\n",
      "Processing call [076] out of [1000] = [7.6%]... ETA mm:ss 35:56\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,154 ms\n",
      "Tokens per second [15.8] input tokens [602] + xml response tokens [34] = total tokens i/o [636]\n",
      "Response: [<response><command>search google current tab</command><args>FileNotFoundError</args></response>]\n",
      "\n",
      "Processing call [077] out of [1000] = [7.7%]... ETA mm:ss 35:51\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,295 ms\n",
      "Tokens per second [15.3] input tokens [606] + xml response tokens [35] = total tokens i/o [641]\n",
      "Response: [<response><command>search perplexity new tab</command><args>AI in financial forecasting</args></response>]\n",
      "\n",
      "Processing call [078] out of [1000] = [7.8%]... ETA mm:ss 35:49\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,192 ms\n",
      "Tokens per second [16.0] input tokens [366] + xml response tokens [35] = total tokens i/o [401]\n",
      "Response: [<response><command>agent router go to receptionist</command><args>Service Desk</args></response>]\n",
      "\n",
      "Processing call [079] out of [1000] = [7.9%]... ETA mm:ss 35:45\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,218 ms\n",
      "Tokens per second [15.8] input tokens [604] + xml response tokens [35] = total tokens i/o [639]\n",
      "Response: [<response><command>search kagi new tab</command><args>Connection Refused Error</args></response>]\n",
      "\n",
      "Processing call [080] out of [1000] = [8.0%]... ETA mm:ss 35:41\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,657 ms\n",
      "Tokens per second [15.8] input tokens [614] + xml response tokens [42] = total tokens i/o [656]\n",
      "Response: [<response><command>search kagi new tab</command><args>AI in gaming: How is AI changing the gaming industry?</args></response>]\n",
      "\n",
      "Processing call [081] out of [1000] = [8.1%]... ETA mm:ss 35:42\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,910 ms\n",
      "Tokens per second [15.7] input tokens [600] + xml response tokens [30] = total tokens i/o [630]\n",
      "Response: [<response><command>search using clipboard current tab</command><args></args></response>]\n",
      "\n",
      "Processing call [082] out of [1000] = [8.2%]... ETA mm:ss 35:35\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,218 ms\n",
      "Tokens per second [15.8] input tokens [607] + xml response tokens [35] = total tokens i/o [642]\n",
      "Response: [<response><command>search perplexity new tab</command><args>IsADirectoryError</args></response>]\n",
      "\n",
      "Processing call [083] out of [1000] = [8.3%]... ETA mm:ss 35:32\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,658 ms\n",
      "Tokens per second [15.8] input tokens [616] + xml response tokens [42] = total tokens i/o [658]\n",
      "Response: [<response><command>search perplexity new tab</command><args>AI in gaming: How is AI changing the gaming industry?</args></response>]\n",
      "\n",
      "Processing call [084] out of [1000] = [8.4%]... ETA mm:ss 35:33\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,034 ms\n",
      "Tokens per second [15.7] input tokens [614] + xml response tokens [32] = total tokens i/o [646]\n",
      "Response: [<response><command>search phind using clipboard new tab</command><args></args></response>]\n",
      "\n",
      "Processing call [085] out of [1000] = [8.5%]... ETA mm:ss 35:28\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,157 ms\n",
      "Tokens per second [15.8] input tokens [603] + xml response tokens [34] = total tokens i/o [637]\n",
      "Response: [<response><command>search new tab</command><args>AWS SageMaker for ML</args></response>]\n",
      "\n",
      "Processing call [086] out of [1000] = [8.6%]... ETA mm:ss 35:24\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,091 ms\n",
      "Tokens per second [15.8] input tokens [602] + xml response tokens [33] = total tokens i/o [635]\n",
      "Response: [<response><command>search google current tab</command><args>AI transparency initiatives</args></response>]\n",
      "\n",
      "Processing call [087] out of [1000] = [8.7%]... ETA mm:ss 35:19\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,153 ms\n",
      "Tokens per second [15.8] input tokens [606] + xml response tokens [34] = total tokens i/o [640]\n",
      "Response: [<response><command>search google new tab</command><args>FileNotFoundError</args></response>]\n",
      "\n",
      "Processing call [088] out of [1000] = [8.8%]... ETA mm:ss 35:15\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,940 ms\n",
      "Tokens per second [16.0] input tokens [363] + xml response tokens [31] = total tokens i/o [394]\n",
      "Response: [<response><command>agent router go to todo list</command><args></args></response>]\n",
      "\n",
      "Processing call [089] out of [1000] = [8.9%]... ETA mm:ss 35:09\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,341 ms\n",
      "Tokens per second [15.8] input tokens [608] + xml response tokens [37] = total tokens i/o [645]\n",
      "Response: [<response><command>search perplexity new tab</command><args>Reading CSV files in Pandas</args></response>]\n",
      "\n",
      "Processing call [090] out of [1000] = [9.0%]... ETA mm:ss 35:07\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 3,028 ms\n",
      "Tokens per second [15.9] input tokens [619] + xml response tokens [48] = total tokens i/o [667]\n",
      "Response: [<response><command>search google scholar new tab</command><args>Sentiment Analysis tools: What tools are widely used for sentiment analysis in social media data?</args></response>]\n",
      "\n",
      "Processing call [091] out of [1000] = [9.1%]... ETA mm:ss 35:11\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,280 ms\n",
      "Tokens per second [15.8] input tokens [606] + xml response tokens [36] = total tokens i/o [642]\n",
      "Response: [<response><command>search google scholar current tab</command><args>NoSQL databases for AI</args></response>]\n",
      "\n",
      "Processing call [092] out of [1000] = [9.2%]... ETA mm:ss 35:09\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,035 ms\n",
      "Tokens per second [15.7] input tokens [604] + xml response tokens [32] = total tokens i/o [636]\n",
      "Response: [<response><command>search phind using clipboard new tab</command><args></args></response>]\n",
      "\n",
      "Processing call [093] out of [1000] = [9.3%]... ETA mm:ss 35:04\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,316 ms\n",
      "Tokens per second [16.0] input tokens [363] + xml response tokens [37] = total tokens i/o [400]\n",
      "Response: [<response><command>agent router go to date and time</command><args>Anaheim, California</args></response>]\n",
      "\n",
      "Processing call [094] out of [1000] = [9.4%]... ETA mm:ss 35:01\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,340 ms\n",
      "Tokens per second [15.8] input tokens [606] + xml response tokens [37] = total tokens i/o [643]\n",
      "Response: [<response><command>go to current tab</command><args>login.incredibleiceberg.com</args></response>]\n",
      "\n",
      "Processing call [095] out of [1000] = [9.5%]... ETA mm:ss 34:59\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,282 ms\n",
      "Tokens per second [15.8] input tokens [606] + xml response tokens [36] = total tokens i/o [642]\n",
      "Response: [<response><command>search perplexity new tab</command><args>AWS SageMaker for ML</args></response>]\n",
      "\n",
      "Processing call [096] out of [1000] = [9.6%]... ETA mm:ss 34:56\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,590 ms\n",
      "Tokens per second [15.8] input tokens [613] + xml response tokens [41] = total tokens i/o [654]\n",
      "Response: [<response><command>search new tab</command><args>How can you resolve errors related to ZIP file operations in Python?</args></response>]\n",
      "\n",
      "Processing call [097] out of [1000] = [9.7%]... ETA mm:ss 34:57\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,529 ms\n",
      "Tokens per second [15.8] input tokens [609] + xml response tokens [40] = total tokens i/o [649]\n",
      "Response: [<response><command>search current tab</command><args>AI in gaming: How is AI changing the gaming industry?</args></response>]\n",
      "\n",
      "Processing call [098] out of [1000] = [9.8%]... ETA mm:ss 34:56\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,154 ms\n",
      "Tokens per second [15.8] input tokens [601] + xml response tokens [34] = total tokens i/o [635]\n",
      "Response: [<response><command>search phind current tab</command><args>best online games</args></response>]\n",
      "\n",
      "Processing call [099] out of [1000] = [9.9%]... ETA mm:ss 34:52\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,880 ms\n",
      "Tokens per second [16.0] input tokens [387] + xml response tokens [30] = total tokens i/o [417]\n",
      "Response: [<response><command>agent router go to calendar</command><args></args></response>]\n",
      "\n",
      "Processing call [100] out of [1000] = [10.0%]... ETA mm:ss 34:46\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,030 ms\n",
      "Tokens per second [15.8] input tokens [602] + xml response tokens [32] = total tokens i/o [634]\n",
      "Response: [<response><command>search google scholar using clipboard current tab</command><args></args></response>]\n",
      "\n",
      "Processing call [101] out of [1000] = [10.1%]... ETA mm:ss 34:41\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,282 ms\n",
      "Tokens per second [15.8] input tokens [604] + xml response tokens [36] = total tokens i/o [640]\n",
      "Response: [<response><command>search new tab</command><args>Type Error: Incorrect type comparison</args></response>]\n",
      "\n",
      "Processing call [102] out of [1000] = [10.2%]... ETA mm:ss 34:39\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,908 ms\n",
      "Tokens per second [15.8] input tokens [611] + xml response tokens [46] = total tokens i/o [657]\n",
      "Response: [<response><command>search google scholar current tab</command><args>How do you address runtime warnings in Python, especially those indicating risky runtime behavior?</args></response>]\n",
      "\n",
      "Processing call [103] out of [1000] = [10.3%]... ETA mm:ss 34:41\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,722 ms\n",
      "Tokens per second [15.8] input tokens [610] + xml response tokens [43] = total tokens i/o [653]\n",
      "Response: [<response><command>search kagi current tab</command><args>How can system exit errors be handled gracefully in Python applications?</args></response>]\n",
      "\n",
      "Processing call [104] out of [1000] = [10.4%]... ETA mm:ss 34:43\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,257 ms\n",
      "Tokens per second [16.0] input tokens [365] + xml response tokens [36] = total tokens i/o [401]\n",
      "Response: [<response><command>agent router go to weather</command><args>Riverside, California</args></response>]\n",
      "\n",
      "Processing call [105] out of [1000] = [10.5%]... ETA mm:ss 34:40\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,346 ms\n",
      "Tokens per second [15.8] input tokens [603] + xml response tokens [37] = total tokens i/o [640]\n",
      "Response: [<response><command>search google scholar current tab</command><args>Reading CSV files in Pandas</args></response>]\n",
      "\n",
      "Processing call [106] out of [1000] = [10.6%]... ETA mm:ss 34:37\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,378 ms\n",
      "Tokens per second [16.0] input tokens [374] + xml response tokens [38] = total tokens i/o [412]\n",
      "Response: [<response><command>agent router go to date and time</command><args>Fresno, California</args></response>]\n",
      "\n",
      "Processing call [107] out of [1000] = [10.7%]... ETA mm:ss 34:36\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,656 ms\n",
      "Tokens per second [15.8] input tokens [610] + xml response tokens [42] = total tokens i/o [652]\n",
      "Response: [<response><command>search perplexity current tab</command><args>AI in gaming: How is AI changing the gaming industry?</args></response>]\n",
      "\n",
      "Processing call [108] out of [1000] = [10.8%]... ETA mm:ss 34:36\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,225 ms\n",
      "Tokens per second [15.7] input tokens [607] + xml response tokens [35] = total tokens i/o [642]\n",
      "Response: [<response><command>search phind new tab</command><args>how to improve memory</args></response>]\n",
      "\n",
      "Processing call [109] out of [1000] = [10.9%]... ETA mm:ss 34:33\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 3,156 ms\n",
      "Tokens per second [15.8] input tokens [622] + xml response tokens [50] = total tokens i/o [672]\n",
      "Response: [<response><command>search google scholar new tab</command><args>Hadoop MapReduce tutorial: Where can you find a beginner-friendly tutorial for Hadoop MapReduce?</args></response>]\n",
      "\n",
      "Processing call [110] out of [1000] = [11.0%]... ETA mm:ss 34:37\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,431 ms\n",
      "Tokens per second [15.2] input tokens [607] + xml response tokens [37] = total tokens i/o [644]\n",
      "Response: [<response><command>search google scholar new tab</command><args>JavaScript libraries for beginners</args></response>]\n",
      "\n",
      "Processing call [111] out of [1000] = [11.1%]... ETA mm:ss 34:36\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,347 ms\n",
      "Tokens per second [15.3] input tokens [606] + xml response tokens [36] = total tokens i/o [642]\n",
      "Response: [<response><command>search kagi current tab</command><args>NoSQL databases for AI</args></response>]\n",
      "\n",
      "Processing call [112] out of [1000] = [11.2%]... ETA mm:ss 34:33\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,316 ms\n",
      "Tokens per second [16.0] input tokens [369] + xml response tokens [37] = total tokens i/o [406]\n",
      "Response: [<response><command>agent router go to date and time</command><args>Dakar, Senegal</args></response>]\n",
      "\n",
      "Processing call [113] out of [1000] = [11.3%]... ETA mm:ss 34:31\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,251 ms\n",
      "Tokens per second [16.0] input tokens [376] + xml response tokens [36] = total tokens i/o [412]\n",
      "Response: [<response><command>agent router go to date and time</command><args>Austin, Texas</args></response>]\n",
      "\n",
      "Processing call [114] out of [1000] = [11.4%]... ETA mm:ss 34:28\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 3,035 ms\n",
      "Tokens per second [15.8] input tokens [618] + xml response tokens [48] = total tokens i/o [666]\n",
      "Response: [<response><command>search phind current tab</command><args>Google Cloud AI services: What AI services does Google Cloud offer for developers and data scientists?</args></response>]\n",
      "\n",
      "Processing call [115] out of [1000] = [11.5%]... ETA mm:ss 34:31\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,161 ms\n",
      "Tokens per second [15.7] input tokens [605] + xml response tokens [34] = total tokens i/o [639]\n",
      "Response: [<response><command>search new tab</command><args>AWS SageMaker for ML</args></response>]\n",
      "\n",
      "Processing call [116] out of [1000] = [11.6%]... ETA mm:ss 34:27\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,966 ms\n",
      "Tokens per second [15.8] input tokens [619] + xml response tokens [47] = total tokens i/o [666]\n",
      "Response: [<response><command>search google new tab</command><args>Normalization vs. Standardization: When should you choose normalization over standardization in data preprocessing?</args></response>]\n",
      "\n",
      "Processing call [117] out of [1000] = [11.7%]... ETA mm:ss 34:30\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 3,091 ms\n",
      "Tokens per second [15.9] input tokens [621] + xml response tokens [49] = total tokens i/o [670]\n",
      "Response: [<response><command>search google new tab</command><args>Convolutional Neural Networks: What are the primary applications of convolutional neural networks in image processing?</args></response>]\n",
      "\n",
      "Processing call [118] out of [1000] = [11.8%]... ETA mm:ss 34:33\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,970 ms\n",
      "Tokens per second [15.8] input tokens [616] + xml response tokens [47] = total tokens i/o [663]\n",
      "Response: [<response><command>search google new tab</command><args>Google Cloud AI services: What AI services does Google Cloud offer for developers and data scientists?</args></response>]\n",
      "\n",
      "Processing call [119] out of [1000] = [11.9%]... ETA mm:ss 34:35\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 3,280 ms\n",
      "Tokens per second [15.9] input tokens [622] + xml response tokens [52] = total tokens i/o [674]\n",
      "Response: [<response><command>search new tab</command><args>Supervised vs. Unsupervised Learning: What are the key differences between supervised and unsupervised learning in AI?</args></response>]\n",
      "\n",
      "Processing call [120] out of [1000] = [12.0%]... ETA mm:ss 34:40\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,033 ms\n",
      "Tokens per second [15.7] input tokens [601] + xml response tokens [32] = total tokens i/o [633]\n",
      "Response: [<response><command>search perplexity using clipboard current tab</command><args></args></response>]\n",
      "\n",
      "Processing call [121] out of [1000] = [12.1%]... ETA mm:ss 34:35\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,158 ms\n",
      "Tokens per second [15.8] input tokens [602] + xml response tokens [34] = total tokens i/o [636]\n",
      "Response: [<response><command>search current tab</command><args>buying a new laptop</args></response>]\n",
      "\n",
      "Processing call [122] out of [1000] = [12.2%]... ETA mm:ss 34:31\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,886 ms\n",
      "Tokens per second [15.9] input tokens [411] + xml response tokens [30] = total tokens i/o [441]\n",
      "Response: [<response><command>agent router go to calendar</command><args></args></response>]\n",
      "\n",
      "Processing call [123] out of [1000] = [12.3%]... ETA mm:ss 34:25\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,327 ms\n",
      "Tokens per second [15.5] input tokens [377] + xml response tokens [36] = total tokens i/o [413]\n",
      "Response: [<response><command>agent router go to weather</command><args>North Las Vegas, Nevada</args></response>]\n",
      "\n",
      "Processing call [124] out of [1000] = [12.4%]... ETA mm:ss 34:23\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,886 ms\n",
      "Tokens per second [15.9] input tokens [376] + xml response tokens [30] = total tokens i/o [406]\n",
      "Response: [<response><command>agent router go to math</command><args></args></response>]\n",
      "\n",
      "Processing call [125] out of [1000] = [12.5%]... ETA mm:ss 34:17\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,089 ms\n",
      "Tokens per second [15.3] input tokens [601] + xml response tokens [32] = total tokens i/o [633]\n",
      "Response: [<response><command>search perplexity using clipboard new tab</command><args></args></response>]\n",
      "\n",
      "Processing call [126] out of [1000] = [12.6%]... ETA mm:ss 34:13\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,037 ms\n",
      "Tokens per second [15.7] input tokens [602] + xml response tokens [32] = total tokens i/o [634]\n",
      "Response: [<response><command>search perplexity using clipboard new tab</command><args></args></response>]\n",
      "\n",
      "Processing call [127] out of [1000] = [12.7%]... ETA mm:ss 34:08\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,346 ms\n",
      "Tokens per second [15.8] input tokens [607] + xml response tokens [37] = total tokens i/o [644]\n",
      "Response: [<response><command>search phind current tab</command><args>Working with datetime in Pandas</args></response>]\n",
      "\n",
      "Processing call [128] out of [1000] = [12.8%]... ETA mm:ss 34:06\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,904 ms\n",
      "Tokens per second [15.8] input tokens [614] + xml response tokens [46] = total tokens i/o [660]\n",
      "Response: [<response><command>search phind current tab</command><args>AI for health diagnostics: How does AI contribute to advancements in health diagnostics?</args></response>]\n",
      "\n",
      "Processing call [129] out of [1000] = [12.9%]... ETA mm:ss 34:08\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,846 ms\n",
      "Tokens per second [15.8] input tokens [612] + xml response tokens [45] = total tokens i/o [657]\n",
      "Response: [<response><command>search kagi current tab</command><args>What are the best practices to manage and prevent memory errors in Python applications?</args></response>]\n",
      "\n",
      "Processing call [130] out of [1000] = [13.0%]... ETA mm:ss 34:09\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 3,405 ms\n",
      "Tokens per second [15.9] input tokens [620] + xml response tokens [54] = total tokens i/o [674]\n",
      "Response: [<response><command>search kagi current tab</command><args>Supervised vs. Unsupervised Learning: What are the key differences between supervised and unsupervised learning in AI?</args></response>]\n",
      "\n",
      "Processing call [131] out of [1000] = [13.1%]... ETA mm:ss 34:13\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,034 ms\n",
      "Tokens per second [15.7] input tokens [602] + xml response tokens [32] = total tokens i/o [634]\n",
      "Response: [<response><command>search new tab</command><args>IndentationError</args></response>]\n",
      "\n",
      "Processing call [132] out of [1000] = [13.2%]... ETA mm:ss 34:09\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,316 ms\n",
      "Tokens per second [16.0] input tokens [363] + xml response tokens [37] = total tokens i/o [400]\n",
      "Response: [<response><command>agent router go to weather</command><args>Jersey City, New Jersey</args></response>]\n",
      "\n",
      "Processing call [133] out of [1000] = [13.3%]... ETA mm:ss 34:06\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,466 ms\n",
      "Tokens per second [15.8] input tokens [607] + xml response tokens [39] = total tokens i/o [646]\n",
      "Response: [<response><command>go to new tab</command><args>login.fantastickangaroo.io</args></response>]\n",
      "\n",
      "Processing call [134] out of [1000] = [13.4%]... ETA mm:ss 34:04\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,155 ms\n",
      "Tokens per second [15.8] input tokens [600] + xml response tokens [34] = total tokens i/o [634]\n",
      "Response: [<response><command>search phind current tab</command><args>best online games</args></response>]\n",
      "\n",
      "Processing call [135] out of [1000] = [13.5%]... ETA mm:ss 34:01\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,252 ms\n",
      "Tokens per second [16.0] input tokens [374] + xml response tokens [36] = total tokens i/o [410]\n",
      "Response: [<response><command>agent router go to weather</command><args>Tulsa, Oklahoma</args></response>]\n",
      "\n",
      "Processing call [136] out of [1000] = [13.6%]... ETA mm:ss 33:58\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,035 ms\n",
      "Tokens per second [15.7] input tokens [602] + xml response tokens [32] = total tokens i/o [634]\n",
      "Response: [<response><command>search google current tab</command><args>ImportWarning</args></response>]\n",
      "\n",
      "Processing call [137] out of [1000] = [13.7%]... ETA mm:ss 33:53\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,878 ms\n",
      "Tokens per second [16.0] input tokens [364] + xml response tokens [30] = total tokens i/o [394]\n",
      "Response: [<response><command>agent router go to calendar</command><args></args></response>]\n",
      "\n",
      "Processing call [138] out of [1000] = [13.8%]... ETA mm:ss 33:48\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,143 ms\n",
      "Tokens per second [15.4] input tokens [600] + xml response tokens [33] = total tokens i/o [633]\n",
      "Response: [<response><command>search kagi current tab</command><args>RuntimeWarning</args></response>]\n",
      "\n",
      "Processing call [139] out of [1000] = [13.9%]... ETA mm:ss 33:44\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,263 ms\n",
      "Tokens per second [15.5] input tokens [601] + xml response tokens [35] = total tokens i/o [636]\n",
      "Response: [<response><command>search kagi current tab</command><args>AI in financial forecasting</args></response>]\n",
      "\n",
      "Processing call [140] out of [1000] = [14.0%]... ETA mm:ss 33:41\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,895 ms\n",
      "Tokens per second [15.5] input tokens [614] + xml response tokens [45] = total tokens i/o [659]\n",
      "Response: [<response><command>search kagi new tab</command><args>How do you handle broken pipe errors in Python, especially in network communications?</args></response>]\n",
      "\n",
      "Processing call [141] out of [1000] = [14.1%]... ETA mm:ss 33:42\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,511 ms\n",
      "Tokens per second [15.5] input tokens [606] + xml response tokens [39] = total tokens i/o [645]\n",
      "Response: [<response><command>search kagi current tab</command><args>Deprecation Warning: Feature is deprecated</args></response>]\n",
      "\n",
      "Processing call [142] out of [1000] = [14.2%]... ETA mm:ss 33:41\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 3,067 ms\n",
      "Tokens per second [15.7] input tokens [617] + xml response tokens [48] = total tokens i/o [665]\n",
      "Response: [<response><command>search perplexity current tab</command><args>Neural Network hyperparameter tuning: What approaches are recommended for neural network hyperparameter tuning?</args></response>]\n",
      "\n",
      "Processing call [143] out of [1000] = [14.3%]... ETA mm:ss 33:43\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,617 ms\n",
      "Tokens per second [15.7] input tokens [612] + xml response tokens [41] = total tokens i/o [653]\n",
      "Response: [<response><command>search new tab</command><args>How do you fix indentation errors that affect code structure in Python?</args></response>]\n",
      "\n",
      "Processing call [144] out of [1000] = [14.4%]... ETA mm:ss 33:42\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,346 ms\n",
      "Tokens per second [15.8] input tokens [603] + xml response tokens [37] = total tokens i/o [640]\n",
      "Response: [<response><command>search phind current tab</command><args>best vacation spots in the world</args></response>]\n",
      "\n",
      "Processing call [145] out of [1000] = [14.5%]... ETA mm:ss 33:39\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,035 ms\n",
      "Tokens per second [15.7] input tokens [596] + xml response tokens [32] = total tokens i/o [628]\n",
      "Response: [<response><command>search google current tab</command><args>EnvironmentError</args></response>]\n",
      "\n",
      "Processing call [146] out of [1000] = [14.6%]... ETA mm:ss 33:35\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,094 ms\n",
      "Tokens per second [15.8] input tokens [605] + xml response tokens [33] = total tokens i/o [638]\n",
      "Response: [<response><command>search phind new tab</command><args>Connection Error</args></response>]\n",
      "\n",
      "Processing call [147] out of [1000] = [14.7%]... ETA mm:ss 33:31\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,345 ms\n",
      "Tokens per second [15.8] input tokens [606] + xml response tokens [37] = total tokens i/o [643]\n",
      "Response: [<response><command>go to new tab</command><args>prod.jubilantlemur.io</args></response>]\n",
      "\n",
      "Processing call [148] out of [1000] = [14.8%]... ETA mm:ss 33:29\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,472 ms\n",
      "Tokens per second [15.8] input tokens [612] + xml response tokens [39] = total tokens i/o [651]\n",
      "Response: [<response><command>search google new tab</command><args>Unsorted Index Error: Index is unsorted</args></response>]\n",
      "\n",
      "Processing call [149] out of [1000] = [14.9%]... ETA mm:ss 33:27\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,347 ms\n",
      "Tokens per second [15.8] input tokens [611] + xml response tokens [37] = total tokens i/o [648]\n",
      "Response: [<response><command>search phind new tab</command><args>best vacation spots in the world</args></response>]\n",
      "\n",
      "Processing call [150] out of [1000] = [15.0%]... ETA mm:ss 33:25\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,283 ms\n",
      "Tokens per second [15.8] input tokens [608] + xml response tokens [36] = total tokens i/o [644]\n",
      "Response: [<response><command>search google new tab</command><args>Data imputation with Pandas</args></response>]\n",
      "\n",
      "Processing call [151] out of [1000] = [15.1%]... ETA mm:ss 33:22\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,097 ms\n",
      "Tokens per second [15.7] input tokens [604] + xml response tokens [33] = total tokens i/o [637]\n",
      "Response: [<response><command>search phind current tab</command><args>Runtime Error</args></response>]\n",
      "\n",
      "Processing call [152] out of [1000] = [15.2%]... ETA mm:ss 33:18\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,844 ms\n",
      "Tokens per second [15.8] input tokens [611] + xml response tokens [45] = total tokens i/o [656]\n",
      "Response: [<response><command>search google scholar current tab</command><args>Voice Recognition technologies: What are the latest advancements in voice recognition technology?</args></response>]\n",
      "\n",
      "Processing call [153] out of [1000] = [15.3%]... ETA mm:ss 33:18\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,251 ms\n",
      "Tokens per second [16.0] input tokens [371] + xml response tokens [36] = total tokens i/o [407]\n",
      "Response: [<response><command>agent router go to date and time</command><args>Rome, Italy</args></response>]\n",
      "\n",
      "Processing call [154] out of [1000] = [15.4%]... ETA mm:ss 33:16\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,155 ms\n",
      "Tokens per second [15.8] input tokens [600] + xml response tokens [34] = total tokens i/o [634]\n",
      "Response: [<response><command>search phind current tab</command><args>hip hop music</args></response>]\n",
      "\n",
      "Processing call [155] out of [1000] = [15.5%]... ETA mm:ss 33:12\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,346 ms\n",
      "Tokens per second [15.8] input tokens [607] + xml response tokens [37] = total tokens i/o [644]\n",
      "Response: [<response><command>go to new tab</command><args>www.hilariouswalrus.net</args></response>]\n",
      "\n",
      "Processing call [156] out of [1000] = [15.6%]... ETA mm:ss 33:10\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,406 ms\n",
      "Tokens per second [15.8] input tokens [604] + xml response tokens [38] = total tokens i/o [642]\n",
      "Response: [<response><command>go to current tab</command><args>dev.wonderfulhamburger.gov</args></response>]\n",
      "\n",
      "Processing call [157] out of [1000] = [15.7%]... ETA mm:ss 33:08\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,722 ms\n",
      "Tokens per second [15.8] input tokens [610] + xml response tokens [43] = total tokens i/o [653]\n",
      "Response: [<response><command>search kagi current tab</command><args>How can system exit errors be handled gracefully in Python applications?</args></response>]\n",
      "\n",
      "Processing call [158] out of [1000] = [15.8%]... ETA mm:ss 33:07\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,348 ms\n",
      "Tokens per second [15.8] input tokens [604] + xml response tokens [37] = total tokens i/o [641]\n",
      "Response: [<response><command>search kagi current tab</command><args>User Warning: User-defined warning</args></response>]\n",
      "\n",
      "Processing call [159] out of [1000] = [15.9%]... ETA mm:ss 33:05\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,972 ms\n",
      "Tokens per second [15.7] input tokens [597] + xml response tokens [31] = total tokens i/o [628]\n",
      "Response: [<response><command>search current tab</command><args>Connection Error</args></response>]\n",
      "\n",
      "Processing call [160] out of [1000] = [16.0%]... ETA mm:ss 33:00\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,970 ms\n",
      "Tokens per second [15.7] input tokens [612] + xml response tokens [31] = total tokens i/o [643]\n",
      "Response: [<response><command>search google using clipboard current tab</command><args></args></response>]\n",
      "\n",
      "Processing call [161] out of [1000] = [16.1%]... ETA mm:ss 32:56\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,344 ms\n",
      "Tokens per second [15.8] input tokens [606] + xml response tokens [37] = total tokens i/o [643]\n",
      "Response: [<response><command>go to new tab</command><args>login.hilariousxylophone.info</args></response>]\n",
      "\n",
      "Processing call [162] out of [1000] = [16.2%]... ETA mm:ss 32:54\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,285 ms\n",
      "Tokens per second [15.8] input tokens [604] + xml response tokens [36] = total tokens i/o [640]\n",
      "Response: [<response><command>search new tab</command><args>Using Pandas for ETL processes</args></response>]\n",
      "\n",
      "Processing call [163] out of [1000] = [16.3%]... ETA mm:ss 32:51\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,037 ms\n",
      "Tokens per second [15.7] input tokens [599] + xml response tokens [32] = total tokens i/o [631]\n",
      "Response: [<response><command>search current tab</command><args>RecursionError</args></response>]\n",
      "\n",
      "Processing call [164] out of [1000] = [16.4%]... ETA mm:ss 32:47\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,349 ms\n",
      "Tokens per second [15.8] input tokens [609] + xml response tokens [37] = total tokens i/o [646]\n",
      "Response: [<response><command>search phind new tab</command><args>Future Warning: Future change warning</args></response>]\n",
      "\n",
      "Processing call [165] out of [1000] = [16.5%]... ETA mm:ss 32:44\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 3,154 ms\n",
      "Tokens per second [15.9] input tokens [621] + xml response tokens [50] = total tokens i/o [671]\n",
      "Response: [<response><command>search google scholar new tab</command><args>Convolutional Neural Networks: What are the primary applications of convolutional neural networks in image processing?</args></response>]\n",
      "\n",
      "Processing call [166] out of [1000] = [16.6%]... ETA mm:ss 32:46\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,345 ms\n",
      "Tokens per second [15.8] input tokens [606] + xml response tokens [37] = total tokens i/o [643]\n",
      "Response: [<response><command>search google scholar new tab</command><args>Data imputation with Pandas</args></response>]\n",
      "\n",
      "Processing call [167] out of [1000] = [16.7%]... ETA mm:ss 32:44\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,157 ms\n",
      "Tokens per second [15.8] input tokens [604] + xml response tokens [34] = total tokens i/o [638]\n",
      "Response: [<response><command>search phind current tab</command><args>hip hop music</args></response>]\n",
      "\n",
      "Processing call [168] out of [1000] = [16.8%]... ETA mm:ss 32:40\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,471 ms\n",
      "Tokens per second [15.8] input tokens [611] + xml response tokens [39] = total tokens i/o [650]\n",
      "Response: [<response><command>search phind new tab</command><args>Setting and resetting index in Pandas</args></response>]\n",
      "\n",
      "Processing call [169] out of [1000] = [16.9%]... ETA mm:ss 32:39\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,379 ms\n",
      "Tokens per second [16.0] input tokens [377] + xml response tokens [38] = total tokens i/o [415]\n",
      "Response: [<response><command>agent router go to date and time</command><args>Laredo, Texas</args></response>]\n",
      "\n",
      "Processing call [170] out of [1000] = [17.0%]... ETA mm:ss 32:36\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,345 ms\n",
      "Tokens per second [15.8] input tokens [605] + xml response tokens [37] = total tokens i/o [642]\n",
      "Response: [<response><command>search google scholar current tab</command><args>Reading CSV files in Pandas</args></response>]\n",
      "\n",
      "Processing call [171] out of [1000] = [17.1%]... ETA mm:ss 32:34\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,219 ms\n",
      "Tokens per second [15.8] input tokens [602] + xml response tokens [35] = total tokens i/o [637]\n",
      "Response: [<response><command>go to new tab</command><args>hilariousiceberg.io</args></response>]\n",
      "\n",
      "Processing call [172] out of [1000] = [17.2%]... ETA mm:ss 32:31\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,345 ms\n",
      "Tokens per second [15.8] input tokens [605] + xml response tokens [37] = total tokens i/o [642]\n",
      "Response: [<response><command>go to current tab</command><args>mail.spectacularwalrus.info</args></response>]\n",
      "\n",
      "Processing call [173] out of [1000] = [17.3%]... ETA mm:ss 32:28\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,127 ms\n",
      "Tokens per second [16.0] input tokens [387] + xml response tokens [34] = total tokens i/o [421]\n",
      "Response: [<response><command>agent router go to receptionist</command><args>Office Assistant</args></response>]\n",
      "\n",
      "Processing call [174] out of [1000] = [17.4%]... ETA mm:ss 32:25\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,885 ms\n",
      "Tokens per second [15.9] input tokens [403] + xml response tokens [30] = total tokens i/o [433]\n",
      "Response: [<response><command>agent router go to calendar</command><args></args></response>]\n",
      "\n",
      "Processing call [175] out of [1000] = [17.5%]... ETA mm:ss 32:20\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,094 ms\n",
      "Tokens per second [15.8] input tokens [605] + xml response tokens [33] = total tokens i/o [638]\n",
      "Response: [<response><command>search perplexity new tab</command><args>BufferError</args></response>]\n",
      "\n",
      "Processing call [176] out of [1000] = [17.6%]... ETA mm:ss 32:17\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,253 ms\n",
      "Tokens per second [16.0] input tokens [371] + xml response tokens [36] = total tokens i/o [407]\n",
      "Response: [<response><command>agent router go to weather</command><args>Bogota, Colombia</args></response>]\n",
      "\n",
      "Processing call [177] out of [1000] = [17.7%]... ETA mm:ss 32:14\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,343 ms\n",
      "Tokens per second [15.8] input tokens [602] + xml response tokens [37] = total tokens i/o [639]\n",
      "Response: [<response><command>go to current tab</command><args>blog.fantasticnovember.io</args></response>]\n",
      "\n",
      "Processing call [178] out of [1000] = [17.8%]... ETA mm:ss 32:12\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,284 ms\n",
      "Tokens per second [15.8] input tokens [608] + xml response tokens [36] = total tokens i/o [644]\n",
      "Response: [<response><command>search perplexity new tab</command><args>AWS SageMaker for ML</args></response>]\n",
      "\n",
      "Processing call [179] out of [1000] = [17.9%]... ETA mm:ss 32:09\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,349 ms\n",
      "Tokens per second [15.8] input tokens [605] + xml response tokens [37] = total tokens i/o [642]\n",
      "Response: [<response><command>search phind current tab</command><args>Data imputation with Pandas</args></response>]\n",
      "\n",
      "Processing call [180] out of [1000] = [18.0%]... ETA mm:ss 32:07\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,350 ms\n",
      "Tokens per second [15.7] input tokens [605] + xml response tokens [37] = total tokens i/o [642]\n",
      "Response: [<response><command>go to new tab</command><args>blog.jubilantquartz.info</args></response>]\n",
      "\n",
      "Processing call [181] out of [1000] = [18.1%]... ETA mm:ss 32:04\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,878 ms\n",
      "Tokens per second [16.0] input tokens [371] + xml response tokens [30] = total tokens i/o [401]\n",
      "Response: [<response><command>agent router go to calendar</command><args></args></response>]\n",
      "\n",
      "Processing call [182] out of [1000] = [18.2%]... ETA mm:ss 32:00\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,471 ms\n",
      "Tokens per second [15.8] input tokens [605] + xml response tokens [39] = total tokens i/o [644]\n",
      "Response: [<response><command>search google current tab</command><args>Unsorted Index Error: Index is unsorted</args></response>]\n",
      "\n",
      "Processing call [183] out of [1000] = [18.3%]... ETA mm:ss 31:58\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,220 ms\n",
      "Tokens per second [15.8] input tokens [609] + xml response tokens [35] = total tokens i/o [644]\n",
      "Response: [<response><command>search google new tab</command><args>how to make pizza dough</args></response>]\n",
      "\n",
      "Processing call [184] out of [1000] = [18.4%]... ETA mm:ss 31:55\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,534 ms\n",
      "Tokens per second [15.8] input tokens [605] + xml response tokens [40] = total tokens i/o [645]\n",
      "Response: [<response><command>search perplexity current tab</command><args>Unsorted Index Error: Index is unsorted</args></response>]\n",
      "\n",
      "Processing call [185] out of [1000] = [18.5%]... ETA mm:ss 31:54\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,162 ms\n",
      "Tokens per second [15.7] input tokens [607] + xml response tokens [34] = total tokens i/o [641]\n",
      "Response: [<response><command>search new tab</command><args>AWS SageMaker for ML</args></response>]\n",
      "\n",
      "Processing call [186] out of [1000] = [18.6%]... ETA mm:ss 31:51\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,343 ms\n",
      "Tokens per second [15.8] input tokens [605] + xml response tokens [37] = total tokens i/o [642]\n",
      "Response: [<response><command>go to new tab</command><args>prod.amazingoctopus.info</args></response>]\n",
      "\n",
      "Processing call [187] out of [1000] = [18.7%]... ETA mm:ss 31:48\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,098 ms\n",
      "Tokens per second [15.7] input tokens [600] + xml response tokens [33] = total tokens i/o [633]\n",
      "Response: [<response><command>search perplexity current tab</command><args>Memory Error</args></response>]\n",
      "\n",
      "Processing call [188] out of [1000] = [18.8%]... ETA mm:ss 31:45\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,407 ms\n",
      "Tokens per second [15.8] input tokens [604] + xml response tokens [38] = total tokens i/o [642]\n",
      "Response: [<response><command>search perplexity current tab</command><args>Type Error: Incorrect type comparison</args></response>]\n",
      "\n",
      "Processing call [189] out of [1000] = [18.9%]... ETA mm:ss 31:43\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,282 ms\n",
      "Tokens per second [15.8] input tokens [608] + xml response tokens [36] = total tokens i/o [644]\n",
      "Response: [<response><command>search google scholar new tab</command><args>best movies of all time</args></response>]\n",
      "\n",
      "Processing call [190] out of [1000] = [19.0%]... ETA mm:ss 31:40\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,906 ms\n",
      "Tokens per second [15.7] input tokens [604] + xml response tokens [30] = total tokens i/o [634]\n",
      "Response: [<response><command>search using clipboard new tab</command><args></args></response>]\n",
      "\n",
      "Processing call [191] out of [1000] = [19.1%]... ETA mm:ss 31:36\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,877 ms\n",
      "Tokens per second [16.0] input tokens [370] + xml response tokens [30] = total tokens i/o [400]\n",
      "Response: [<response><command>agent router go to calendar</command><args></args></response>]\n",
      "\n",
      "Processing call [192] out of [1000] = [19.2%]... ETA mm:ss 31:32\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,190 ms\n",
      "Tokens per second [16.0] input tokens [381] + xml response tokens [35] = total tokens i/o [416]\n",
      "Response: [<response><command>agent router go to weather</command><args>Montreal, Canada</args></response>]\n",
      "\n",
      "Processing call [193] out of [1000] = [19.3%]... ETA mm:ss 31:29\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,342 ms\n",
      "Tokens per second [15.8] input tokens [604] + xml response tokens [37] = total tokens i/o [641]\n",
      "Response: [<response><command>go to new tab</command><args>blog.jubilantunicorn.io</args></response>]\n",
      "\n",
      "Processing call [194] out of [1000] = [19.4%]... ETA mm:ss 31:26\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,785 ms\n",
      "Tokens per second [15.8] input tokens [612] + xml response tokens [44] = total tokens i/o [656]\n",
      "Response: [<response><command>search new tab</command><args>Machine Learning in biotech: How is machine learning applied in biotechnology research?</args></response>]\n",
      "\n",
      "Processing call [195] out of [1000] = [19.5%]... ETA mm:ss 31:26\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,970 ms\n",
      "Tokens per second [15.7] input tokens [600] + xml response tokens [31] = total tokens i/o [631]\n",
      "Response: [<response><command>search new tab</command><args>Reference Error</args></response>]\n",
      "\n",
      "Processing call [196] out of [1000] = [19.6%]... ETA mm:ss 31:22\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,096 ms\n",
      "Tokens per second [15.7] input tokens [601] + xml response tokens [33] = total tokens i/o [634]\n",
      "Response: [<response><command>search current tab</command><args>IBM Watson AI tools</args></response>]\n",
      "\n",
      "Processing call [197] out of [1000] = [19.7%]... ETA mm:ss 31:18\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,405 ms\n",
      "Tokens per second [15.8] input tokens [611] + xml response tokens [38] = total tokens i/o [649]\n",
      "Response: [<response><command>search phind new tab</command><args>Stop Iteration: Iteration stopped</args></response>]\n",
      "\n",
      "Processing call [198] out of [1000] = [19.8%]... ETA mm:ss 31:16\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,407 ms\n",
      "Tokens per second [15.8] input tokens [610] + xml response tokens [38] = total tokens i/o [648]\n",
      "Response: [<response><command>search phind new tab</command><args>System Error: Internal Python system issue</args></response>]\n",
      "\n",
      "Processing call [199] out of [1000] = [19.9%]... ETA mm:ss 31:14\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,470 ms\n",
      "Tokens per second [15.8] input tokens [607] + xml response tokens [39] = total tokens i/o [646]\n",
      "Response: [<response><command>search phind current tab</command><args>Deprecation Warning: Feature is deprecated</args></response>]\n",
      "\n",
      "Processing call [200] out of [1000] = [20.0%]... ETA mm:ss 31:13\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,406 ms\n",
      "Tokens per second [15.8] input tokens [609] + xml response tokens [38] = total tokens i/o [647]\n",
      "Response: [<response><command>search kagi new tab</command><args>System Error: Internal Python system issue</args></response>]\n",
      "\n",
      "Processing call [201] out of [1000] = [20.1%]... ETA mm:ss 31:10\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,130 ms\n",
      "Tokens per second [16.0] input tokens [363] + xml response tokens [34] = total tokens i/o [397]\n",
      "Response: [<response><command>agent router go to receptionist</command><args>Visitor Coordinator</args></response>]\n",
      "\n",
      "Processing call [202] out of [1000] = [20.2%]... ETA mm:ss 31:07\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,603 ms\n",
      "Tokens per second [15.8] input tokens [608] + xml response tokens [41] = total tokens i/o [649]\n",
      "Response: [<response><command>search perplexity current tab</command><args>Unbound Local Error: Local variable referenced before assignment</args></response>]\n",
      "\n",
      "Processing call [203] out of [1000] = [20.3%]... ETA mm:ss 31:06\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,849 ms\n",
      "Tokens per second [15.8] input tokens [616] + xml response tokens [45] = total tokens i/o [661]\n",
      "Response: [<response><command>search google current tab</command><args>Machine Learning in biotech: How is machine learning applied in biotechnology research?</args></response>]\n",
      "\n",
      "Processing call [204] out of [1000] = [20.4%]... ETA mm:ss 31:06\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,725 ms\n",
      "Tokens per second [15.8] input tokens [607] + xml response tokens [43] = total tokens i/o [650]\n",
      "Response: [<response><command>search kagi current tab</command><args>File Not Found Error: File not found or path is incorrect</args></response>]\n",
      "\n",
      "Processing call [205] out of [1000] = [20.5%]... ETA mm:ss 31:05\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,846 ms\n",
      "Tokens per second [15.8] input tokens [609] + xml response tokens [45] = total tokens i/o [654]\n",
      "Response: [<response><command>search perplexity current tab</command><args>What are the best practices to manage and prevent memory errors in Python applications?</args></response>]\n",
      "\n",
      "Processing call [206] out of [1000] = [20.6%]... ETA mm:ss 31:04\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,717 ms\n",
      "Tokens per second [15.8] input tokens [611] + xml response tokens [43] = total tokens i/o [654]\n",
      "Response: [<response><command>search current tab</command><args>OpenAI latest projects: What are the latest projects undertaken by OpenAI?</args></response>]\n",
      "\n",
      "Processing call [207] out of [1000] = [20.7%]... ETA mm:ss 31:03\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,413 ms\n",
      "Tokens per second [15.7] input tokens [607] + xml response tokens [38] = total tokens i/o [645]\n",
      "Response: [<response><command>go to new tab</command><args>alpha.hilariousyogurt.net</args></response>]\n",
      "\n",
      "Processing call [208] out of [1000] = [20.8%]... ETA mm:ss 31:01\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,037 ms\n",
      "Tokens per second [15.7] input tokens [605] + xml response tokens [32] = total tokens i/o [637]\n",
      "Response: [<response><command>search phind using clipboard current tab</command><args></args></response>]\n",
      "\n",
      "Processing call [209] out of [1000] = [20.9%]... ETA mm:ss 30:58\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,720 ms\n",
      "Tokens per second [15.8] input tokens [613] + xml response tokens [43] = total tokens i/o [656]\n",
      "Response: [<response><command>search current tab</command><args>What are the best practices to manage and prevent memory errors in Python applications?</args></response>]\n",
      "\n",
      "Processing call [210] out of [1000] = [21.0%]... ETA mm:ss 30:57\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,465 ms\n",
      "Tokens per second [15.8] input tokens [606] + xml response tokens [39] = total tokens i/o [645]\n",
      "Response: [<response><command>go to current tab</command><args>mail.magnificentstrawberry.net</args></response>]\n",
      "\n",
      "Processing call [211] out of [1000] = [21.1%]... ETA mm:ss 30:55\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,189 ms\n",
      "Tokens per second [16.0] input tokens [376] + xml response tokens [35] = total tokens i/o [411]\n",
      "Response: [<response><command>agent router go to weather</command><args>Los Angeles, USA</args></response>]\n",
      "\n",
      "Processing call [212] out of [1000] = [21.2%]... ETA mm:ss 30:52\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,187 ms\n",
      "Tokens per second [16.0] input tokens [371] + xml response tokens [35] = total tokens i/o [406]\n",
      "Response: [<response><command>agent router go to weather</command><args>Dublin, Ireland</args></response>]\n",
      "\n",
      "Processing call [213] out of [1000] = [21.3%]... ETA mm:ss 30:49\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,285 ms\n",
      "Tokens per second [15.8] input tokens [363] + xml response tokens [36] = total tokens i/o [399]\n",
      "Response: [<response><command>agent router go to receptionist</command><args>Desk Clerk Agent</args></response>]\n",
      "\n",
      "Processing call [214] out of [1000] = [21.4%]... ETA mm:ss 30:46\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,909 ms\n",
      "Tokens per second [15.8] input tokens [619] + xml response tokens [46] = total tokens i/o [665]\n",
      "Response: [<response><command>search perplexity new tab</command><args>How do you address runtime warnings in Python, especially those indicating risky runtime behavior?</args></response>]\n",
      "\n",
      "Processing call [215] out of [1000] = [21.5%]... ETA mm:ss 30:46\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,348 ms\n",
      "Tokens per second [15.8] input tokens [607] + xml response tokens [37] = total tokens i/o [644]\n",
      "Response: [<response><command>search google scholar new tab</command><args>Handling JSON data in Pandas</args></response>]\n",
      "\n",
      "Processing call [216] out of [1000] = [21.6%]... ETA mm:ss 30:44\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,349 ms\n",
      "Tokens per second [15.8] input tokens [613] + xml response tokens [37] = total tokens i/o [650]\n",
      "Response: [<response><command>search perplexity new tab</command><args>how to make homemade bread</args></response>]\n",
      "\n",
      "Processing call [217] out of [1000] = [21.7%]... ETA mm:ss 30:41\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,406 ms\n",
      "Tokens per second [15.8] input tokens [607] + xml response tokens [38] = total tokens i/o [645]\n",
      "Response: [<response><command>search kagi current tab</command><args>Pandas and geospatial data</args></response>]\n",
      "\n",
      "Processing call [218] out of [1000] = [21.8%]... ETA mm:ss 30:39\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 3,158 ms\n",
      "Tokens per second [15.8] input tokens [624] + xml response tokens [50] = total tokens i/o [674]\n",
      "Response: [<response><command>search perplexity current tab</command><args>Hadoop MapReduce tutorial: Where can you find a beginner-friendly tutorial for Hadoop MapReduce?</args></response>]\n",
      "\n",
      "Processing call [219] out of [1000] = [21.9%]... ETA mm:ss 30:40\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,405 ms\n",
      "Tokens per second [15.8] input tokens [608] + xml response tokens [38] = total tokens i/o [646]\n",
      "Response: [<response><command>go to new tab</command><args>blog.fantastictornado.info</args></response>]\n",
      "\n",
      "Processing call [220] out of [1000] = [22.0%]... ETA mm:ss 30:38\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,784 ms\n",
      "Tokens per second [15.8] input tokens [614] + xml response tokens [44] = total tokens i/o [658]\n",
      "Response: [<response><command>search kagi current tab</command><args>Responsible AI practices: How can organizations implement responsible AI practices?</args></response>]\n",
      "\n",
      "Processing call [221] out of [1000] = [22.1%]... ETA mm:ss 30:37\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,344 ms\n",
      "Tokens per second [15.8] input tokens [605] + xml response tokens [37] = total tokens i/o [642]\n",
      "Response: [<response><command>search kagi current tab</command><args>Reinforcement Learning breakthroughs</args></response>]\n",
      "\n",
      "Processing call [222] out of [1000] = [22.2%]... ETA mm:ss 30:34\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,655 ms\n",
      "Tokens per second [15.8] input tokens [612] + xml response tokens [42] = total tokens i/o [654]\n",
      "Response: [<response><command>search google scholar new tab</command><args>How can decoding errors with Unicode characters be resolved in Python?</args></response>]\n",
      "\n",
      "Processing call [223] out of [1000] = [22.3%]... ETA mm:ss 30:33\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,659 ms\n",
      "Tokens per second [15.7] input tokens [597] + xml response tokens [26] = total tokens i/o [623]\n",
      "Response: [<response><command>none</command><args></args></response>]\n",
      "\n",
      "Processing call [224] out of [1000] = [22.4%]... ETA mm:ss 30:28\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,343 ms\n",
      "Tokens per second [15.8] input tokens [604] + xml response tokens [37] = total tokens i/o [641]\n",
      "Response: [<response><command>go to new tab</command><args>blog.jubilantquartz.info</args></response>]\n",
      "\n",
      "Processing call [225] out of [1000] = [22.5%]... ETA mm:ss 30:26\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 3,407 ms\n",
      "Tokens per second [15.8] input tokens [626] + xml response tokens [54] = total tokens i/o [680]\n",
      "Response: [<response><command>search phind new tab</command><args>Supervised vs. Unsupervised Learning: What are the key differences between supervised and unsupervised learning in AI?</args></response>]\n",
      "\n",
      "Processing call [226] out of [1000] = [22.6%]... ETA mm:ss 30:27\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,409 ms\n",
      "Tokens per second [15.8] input tokens [603] + xml response tokens [38] = total tokens i/o [641]\n",
      "Response: [<response><command>search google scholar current tab</command><args>Saving Pandas DataFrame to CSV</args></response>]\n",
      "\n",
      "Processing call [227] out of [1000] = [22.7%]... ETA mm:ss 30:25\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,282 ms\n",
      "Tokens per second [15.8] input tokens [606] + xml response tokens [36] = total tokens i/o [642]\n",
      "Response: [<response><command>search phind current tab</command><args>AWS SageMaker for ML</args></response>]\n",
      "\n",
      "Processing call [228] out of [1000] = [22.8%]... ETA mm:ss 30:22\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,313 ms\n",
      "Tokens per second [16.0] input tokens [378] + xml response tokens [37] = total tokens i/o [415]\n",
      "Response: [<response><command>agent router go to date and time</command><args>Taipei, Taiwan</args></response>]\n",
      "\n",
      "Processing call [229] out of [1000] = [22.9%]... ETA mm:ss 30:20\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,281 ms\n",
      "Tokens per second [15.8] input tokens [601] + xml response tokens [36] = total tokens i/o [637]\n",
      "Response: [<response><command>go to current tab</command><args>amazingstrawberry.org</args></response>]\n",
      "\n",
      "Processing call [230] out of [1000] = [23.0%]... ETA mm:ss 30:17\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,191 ms\n",
      "Tokens per second [16.0] input tokens [365] + xml response tokens [35] = total tokens i/o [400]\n",
      "Response: [<response><command>agent router go to receptionist</command><args>Service Rep Agent</args></response>]\n",
      "\n",
      "Processing call [231] out of [1000] = [23.1%]... ETA mm:ss 30:14\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,155 ms\n",
      "Tokens per second [15.8] input tokens [602] + xml response tokens [34] = total tokens i/o [636]\n",
      "Response: [<response><command>search perplexity current tab</command><args>AI transparency initiatives</args></response>]\n",
      "\n",
      "Processing call [232] out of [1000] = [23.2%]... ETA mm:ss 30:11\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,285 ms\n",
      "Tokens per second [15.8] input tokens [604] + xml response tokens [36] = total tokens i/o [640]\n",
      "Response: [<response><command>search new tab</command><args>Pandas and geospatial data</args></response>]\n",
      "\n",
      "Processing call [233] out of [1000] = [23.3%]... ETA mm:ss 30:08\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,341 ms\n",
      "Tokens per second [15.8] input tokens [603] + xml response tokens [37] = total tokens i/o [640]\n",
      "Response: [<response><command>go to current tab</command><args>dev.hilariousbanana.io</args></response>]\n",
      "\n",
      "Processing call [234] out of [1000] = [23.4%]... ETA mm:ss 30:06\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,935 ms\n",
      "Tokens per second [16.0] input tokens [368] + xml response tokens [31] = total tokens i/o [399]\n",
      "Response: [<response><command>agent router go to todo list</command><args></args></response>]\n",
      "\n",
      "Processing call [235] out of [1000] = [23.5%]... ETA mm:ss 30:02\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,408 ms\n",
      "Tokens per second [15.8] input tokens [604] + xml response tokens [38] = total tokens i/o [642]\n",
      "Response: [<response><command>search google current tab</command><args>Recursion Error: Maximum recursion depth exceeded</args></response>]\n",
      "\n",
      "Processing call [236] out of [1000] = [23.6%]... ETA mm:ss 30:00\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,318 ms\n",
      "Tokens per second [16.0] input tokens [373] + xml response tokens [37] = total tokens i/o [410]\n",
      "Response: [<response><command>agent router go to date and time</command><args>Amsterdam, Netherlands</args></response>]\n",
      "\n",
      "Processing call [237] out of [1000] = [23.7%]... ETA mm:ss 29:58\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,157 ms\n",
      "Tokens per second [15.8] input tokens [602] + xml response tokens [34] = total tokens i/o [636]\n",
      "Response: [<response><command>search current tab</command><args>AWS SageMaker for ML</args></response>]\n",
      "\n",
      "Processing call [238] out of [1000] = [23.8%]... ETA mm:ss 29:55\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,599 ms\n",
      "Tokens per second [15.8] input tokens [611] + xml response tokens [41] = total tokens i/o [652]\n",
      "Response: [<response><command>search current tab</command><args>How can you resolve errors related to ZIP file operations in Python?</args></response>]\n",
      "\n",
      "Processing call [239] out of [1000] = [23.9%]... ETA mm:ss 29:53\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,344 ms\n",
      "Tokens per second [15.8] input tokens [607] + xml response tokens [37] = total tokens i/o [644]\n",
      "Response: [<response><command>search google scholar new tab</command><args>Reinforcement Learning breakthroughs</args></response>]\n",
      "\n",
      "Processing call [240] out of [1000] = [24.0%]... ETA mm:ss 29:51\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,254 ms\n",
      "Tokens per second [16.0] input tokens [361] + xml response tokens [36] = total tokens i/o [397]\n",
      "Response: [<response><command>agent router go to receptionist</command><args>Concierge Agent</args></response>]\n",
      "\n",
      "Processing call [241] out of [1000] = [24.1%]... ETA mm:ss 29:48\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,297 ms\n",
      "Tokens per second [15.7] input tokens [605] + xml response tokens [36] = total tokens i/o [641]\n",
      "Response: [<response><command>search google new tab</command><args>Reading Excel files with Pandas</args></response>]\n",
      "\n",
      "Processing call [242] out of [1000] = [24.2%]... ETA mm:ss 29:45\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,184 ms\n",
      "Tokens per second [16.0] input tokens [382] + xml response tokens [35] = total tokens i/o [417]\n",
      "Response: [<response><command>agent router go to receptionist</command><args>Service Desk</args></response>]\n",
      "\n",
      "Processing call [243] out of [1000] = [24.3%]... ETA mm:ss 29:43\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,472 ms\n",
      "Tokens per second [15.8] input tokens [607] + xml response tokens [39] = total tokens i/o [646]\n",
      "Response: [<response><command>go to current tab</command><args>www.spectacularelephant.org</args></response>]\n",
      "\n",
      "Processing call [244] out of [1000] = [24.4%]... ETA mm:ss 29:41\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,342 ms\n",
      "Tokens per second [15.8] input tokens [602] + xml response tokens [37] = total tokens i/o [639]\n",
      "Response: [<response><command>search kagi current tab</command><args>Neural Network hyperparameter tuning</args></response>]\n",
      "\n",
      "Processing call [245] out of [1000] = [24.5%]... ETA mm:ss 29:38\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,223 ms\n",
      "Tokens per second [15.7] input tokens [607] + xml response tokens [35] = total tokens i/o [642]\n",
      "Response: [<response><command>search new tab</command><args>how to make homemade bread</args></response>]\n",
      "\n",
      "Processing call [246] out of [1000] = [24.6%]... ETA mm:ss 29:35\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,318 ms\n",
      "Tokens per second [16.0] input tokens [373] + xml response tokens [37] = total tokens i/o [410]\n",
      "Response: [<response><command>agent router go to date and time</command><args>San Antonio, Texas</args></response>]\n",
      "\n",
      "Processing call [247] out of [1000] = [24.7%]... ETA mm:ss 29:33\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,034 ms\n",
      "Tokens per second [15.7] input tokens [600] + xml response tokens [32] = total tokens i/o [632]\n",
      "Response: [<response><command>search google current tab</command><args>Runtime Error</args></response>]\n",
      "\n",
      "Processing call [248] out of [1000] = [24.8%]... ETA mm:ss 29:30\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,171 ms\n",
      "Tokens per second [15.7] input tokens [601] + xml response tokens [34] = total tokens i/o [635]\n",
      "Response: [<response><command>search phind new tab</command><args>AssertionError</args></response>]\n",
      "\n",
      "Processing call [249] out of [1000] = [24.9%]... ETA mm:ss 29:27\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,157 ms\n",
      "Tokens per second [15.8] input tokens [600] + xml response tokens [34] = total tokens i/o [634]\n",
      "Response: [<response><command>search google scholar current tab</command><args>AssertionError</args></response>]\n",
      "\n",
      "Processing call [250] out of [1000] = [25.0%]... ETA mm:ss 29:24\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,098 ms\n",
      "Tokens per second [15.7] input tokens [598] + xml response tokens [33] = total tokens i/o [631]\n",
      "Response: [<response><command>search kagi current tab</command><args>RuntimeWarning</args></response>]\n",
      "\n",
      "Processing call [251] out of [1000] = [25.1%]... ETA mm:ss 29:21\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,220 ms\n",
      "Tokens per second [15.8] input tokens [606] + xml response tokens [35] = total tokens i/o [641]\n",
      "Response: [<response><command>search phind current tab</command><args>FileNotFoundError</args></response>]\n",
      "\n",
      "Processing call [252] out of [1000] = [25.2%]... ETA mm:ss 29:18\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,346 ms\n",
      "Tokens per second [15.8] input tokens [604] + xml response tokens [37] = total tokens i/o [641]\n",
      "Response: [<response><command>go to current tab</command><args>stage.jubilantwalrus.info</args></response>]\n",
      "\n",
      "Processing call [253] out of [1000] = [25.3%]... ETA mm:ss 29:16\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,355 ms\n",
      "Tokens per second [15.7] input tokens [605] + xml response tokens [37] = total tokens i/o [642]\n",
      "Response: [<response><command>search perplexity new tab</command><args>Neural Network hyperparameter tuning</args></response>]\n",
      "\n",
      "Processing call [254] out of [1000] = [25.4%]... ETA mm:ss 29:13\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,901 ms\n",
      "Tokens per second [15.9] input tokens [618] + xml response tokens [46] = total tokens i/o [664]\n",
      "Response: [<response><command>search current tab</command><args>Google Cloud AI services: What AI services does Google Cloud offer for developers and data scientists?</args></response>]\n",
      "\n",
      "Processing call [255] out of [1000] = [25.5%]... ETA mm:ss 29:13\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,874 ms\n",
      "Tokens per second [16.0] input tokens [370] + xml response tokens [30] = total tokens i/o [400]\n",
      "Response: [<response><command>agent router go to calendar</command><args></args></response>]\n",
      "\n",
      "Processing call [256] out of [1000] = [25.6%]... ETA mm:ss 29:09\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,780 ms\n",
      "Tokens per second [15.8] input tokens [614] + xml response tokens [44] = total tokens i/o [658]\n",
      "Response: [<response><command>search google current tab</command><args>What are common causes and solutions for errors related to incorrect syntax in Python?</args></response>]\n",
      "\n",
      "Processing call [257] out of [1000] = [25.7%]... ETA mm:ss 29:08\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,191 ms\n",
      "Tokens per second [16.0] input tokens [375] + xml response tokens [35] = total tokens i/o [410]\n",
      "Response: [<response><command>agent router go to receptionist</command><args>Customer Service Representative</args></response>]\n",
      "\n",
      "Processing call [258] out of [1000] = [25.8%]... ETA mm:ss 29:05\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,347 ms\n",
      "Tokens per second [15.8] input tokens [603] + xml response tokens [37] = total tokens i/o [640]\n",
      "Response: [<response><command>search google scholar current tab</command><args>Cross-tabulation in Pandas</args></response>]\n",
      "\n",
      "Processing call [259] out of [1000] = [25.9%]... ETA mm:ss 29:02\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,407 ms\n",
      "Tokens per second [15.8] input tokens [606] + xml response tokens [38] = total tokens i/o [644]\n",
      "Response: [<response><command>search google current tab</command><args>Setting and resetting index in Pandas</args></response>]\n",
      "\n",
      "Processing call [260] out of [1000] = [26.0%]... ETA mm:ss 29:00\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,907 ms\n",
      "Tokens per second [15.8] input tokens [611] + xml response tokens [46] = total tokens i/o [657]\n",
      "Response: [<response><command>search perplexity current tab</command><args>Quantum AI computing developments: What are the latest developments in quantum AI computing?</args></response>]\n",
      "\n",
      "Processing call [261] out of [1000] = [26.1%]... ETA mm:ss 29:00\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,349 ms\n",
      "Tokens per second [15.8] input tokens [606] + xml response tokens [37] = total tokens i/o [643]\n",
      "Response: [<response><command>go to new tab</command><args>prod.hilariousvolcano.info</args></response>]\n",
      "\n",
      "Processing call [262] out of [1000] = [26.2%]... ETA mm:ss 28:57\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,882 ms\n",
      "Tokens per second [15.9] input tokens [365] + xml response tokens [30] = total tokens i/o [395]\n",
      "Response: [<response><command>agent router go to calendar</command><args></args></response>]\n",
      "\n",
      "Processing call [263] out of [1000] = [26.3%]... ETA mm:ss 28:53\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,217 ms\n",
      "Tokens per second [15.8] input tokens [603] + xml response tokens [35] = total tokens i/o [638]\n",
      "Response: [<response><command>search google scholar current tab</command><args>Connection Refused Error</args></response>]\n",
      "\n",
      "Processing call [264] out of [1000] = [26.4%]... ETA mm:ss 28:51\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,348 ms\n",
      "Tokens per second [15.8] input tokens [605] + xml response tokens [37] = total tokens i/o [642]\n",
      "Response: [<response><command>go to current tab</command><args>dev.jubilantquartz.com</args></response>]\n",
      "\n",
      "Processing call [265] out of [1000] = [26.5%]... ETA mm:ss 28:48\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,156 ms\n",
      "Tokens per second [15.8] input tokens [604] + xml response tokens [34] = total tokens i/o [638]\n",
      "Response: [<response><command>search phind current tab</command><args>RecursionError</args></response>]\n",
      "\n",
      "Processing call [266] out of [1000] = [26.6%]... ETA mm:ss 28:45\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,095 ms\n",
      "Tokens per second [15.8] input tokens [603] + xml response tokens [33] = total tokens i/o [636]\n",
      "Response: [<response><command>search phind current tab</command><args>Overflow Error</args></response>]\n",
      "\n",
      "Processing call [267] out of [1000] = [26.7%]... ETA mm:ss 28:42\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,884 ms\n",
      "Tokens per second [15.9] input tokens [389] + xml response tokens [30] = total tokens i/o [419]\n",
      "Response: [<response><command>agent router go to calendar</command><args></args></response>]\n",
      "\n",
      "Processing call [268] out of [1000] = [26.8%]... ETA mm:ss 28:39\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,470 ms\n",
      "Tokens per second [15.8] input tokens [607] + xml response tokens [39] = total tokens i/o [646]\n",
      "Response: [<response><command>search current tab</command><args>Unbound Local Error: Local variable referenced before assignment</args></response>]\n",
      "\n",
      "Processing call [269] out of [1000] = [26.9%]... ETA mm:ss 28:37\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,315 ms\n",
      "Tokens per second [16.0] input tokens [375] + xml response tokens [37] = total tokens i/o [412]\n",
      "Response: [<response><command>agent router go to weather</command><args>Jersey City, New Jersey</args></response>]\n",
      "\n",
      "Processing call [270] out of [1000] = [27.0%]... ETA mm:ss 28:34\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,281 ms\n",
      "Tokens per second [15.8] input tokens [603] + xml response tokens [36] = total tokens i/o [639]\n",
      "Response: [<response><command>go to current tab</command><args>amazingstrawberry.org</args></response>]\n",
      "\n",
      "Processing call [271] out of [1000] = [27.1%]... ETA mm:ss 28:32\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,222 ms\n",
      "Tokens per second [15.8] input tokens [608] + xml response tokens [35] = total tokens i/o [643]\n",
      "Response: [<response><command>search kagi new tab</command><args>Is A Directory Error</args></response>]\n",
      "\n",
      "Processing call [272] out of [1000] = [27.2%]... ETA mm:ss 28:29\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,345 ms\n",
      "Tokens per second [15.8] input tokens [605] + xml response tokens [37] = total tokens i/o [642]\n",
      "Response: [<response><command>go to current tab</command><args>prod.incrediblevolcano.info</args></response>]\n",
      "\n",
      "Processing call [273] out of [1000] = [27.3%]... ETA mm:ss 28:27\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,094 ms\n",
      "Tokens per second [15.8] input tokens [606] + xml response tokens [33] = total tokens i/o [639]\n",
      "Response: [<response><command>search perplexity new tab</command><args>EnvironmentError</args></response>]\n",
      "\n",
      "Processing call [274] out of [1000] = [27.4%]... ETA mm:ss 28:24\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,412 ms\n",
      "Tokens per second [15.7] input tokens [608] + xml response tokens [38] = total tokens i/o [646]\n",
      "Response: [<response><command>search phind current tab</command><args>System Error: Internal Python system issue</args></response>]\n",
      "\n",
      "Processing call [275] out of [1000] = [27.5%]... ETA mm:ss 28:22\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,972 ms\n",
      "Tokens per second [15.7] input tokens [604] + xml response tokens [31] = total tokens i/o [635]\n",
      "Response: [<response><command>search new tab</command><args>RuntimeWarning</args></response>]\n",
      "\n",
      "Processing call [276] out of [1000] = [27.6%]... ETA mm:ss 28:18\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,380 ms\n",
      "Tokens per second [16.0] input tokens [367] + xml response tokens [38] = total tokens i/o [405]\n",
      "Response: [<response><command>agent router go to date and time</command><args>Dhaka, Bangladesh</args></response>]\n",
      "\n",
      "Processing call [277] out of [1000] = [27.7%]... ETA mm:ss 28:16\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,407 ms\n",
      "Tokens per second [15.8] input tokens [609] + xml response tokens [38] = total tokens i/o [647]\n",
      "Response: [<response><command>search google scholar new tab</command><args>Pandas and geospatial data</args></response>]\n",
      "\n",
      "Processing call [278] out of [1000] = [27.8%]... ETA mm:ss 28:14\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,285 ms\n",
      "Tokens per second [15.8] input tokens [609] + xml response tokens [36] = total tokens i/o [645]\n",
      "Response: [<response><command>search phind new tab</command><args>how to tie a tie</args></response>]\n",
      "\n",
      "Processing call [279] out of [1000] = [27.9%]... ETA mm:ss 28:11\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,130 ms\n",
      "Tokens per second [16.0] input tokens [378] + xml response tokens [34] = total tokens i/o [412]\n",
      "Response: [<response><command>agent router go to receptionist</command><args>Customer Service</args></response>]\n",
      "\n",
      "Processing call [280] out of [1000] = [28.0%]... ETA mm:ss 28:08\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,315 ms\n",
      "Tokens per second [16.0] input tokens [381] + xml response tokens [37] = total tokens i/o [418]\n",
      "Response: [<response><command>agent router go to date and time</command><args>Dublin, Ireland</args></response>]\n",
      "\n",
      "Processing call [281] out of [1000] = [28.1%]... ETA mm:ss 28:06\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,467 ms\n",
      "Tokens per second [15.8] input tokens [609] + xml response tokens [39] = total tokens i/o [648]\n",
      "Response: [<response><command>go to new tab</command><args>dev.magnificentstrawberry.org</args></response>]\n",
      "\n",
      "Processing call [282] out of [1000] = [28.2%]... ETA mm:ss 28:04\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,408 ms\n",
      "Tokens per second [15.8] input tokens [608] + xml response tokens [38] = total tokens i/o [646]\n",
      "Response: [<response><command>search google scholar new tab</command><args>System Error: Internal Python system issue</args></response>]\n",
      "\n",
      "Processing call [283] out of [1000] = [28.3%]... ETA mm:ss 28:02\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,153 ms\n",
      "Tokens per second [15.8] input tokens [599] + xml response tokens [34] = total tokens i/o [633]\n",
      "Response: [<response><command>search google scholar current tab</command><args>AI transparency initiatives</args></response>]\n",
      "\n",
      "Processing call [284] out of [1000] = [28.4%]... ETA mm:ss 27:59\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,095 ms\n",
      "Tokens per second [15.8] input tokens [603] + xml response tokens [33] = total tokens i/o [636]\n",
      "Response: [<response><command>search phind current tab</command><args>BufferError</args></response>]\n",
      "\n",
      "Processing call [285] out of [1000] = [28.5%]... ETA mm:ss 27:56\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,155 ms\n",
      "Tokens per second [15.8] input tokens [607] + xml response tokens [34] = total tokens i/o [641]\n",
      "Response: [<response><command>search phind new tab</command><args>AI transparency initiatives</args></response>]\n",
      "\n",
      "Processing call [286] out of [1000] = [28.6%]... ETA mm:ss 27:53\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,346 ms\n",
      "Tokens per second [15.8] input tokens [609] + xml response tokens [37] = total tokens i/o [646]\n",
      "Response: [<response><command>search phind new tab</command><args>User Warning: User-defined warning</args></response>]\n",
      "\n",
      "Processing call [287] out of [1000] = [28.7%]... ETA mm:ss 27:51\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,033 ms\n",
      "Tokens per second [15.7] input tokens [600] + xml response tokens [32] = total tokens i/o [632]\n",
      "Response: [<response><command>search perplexity using clipboard new tab</command><args></args></response>]\n",
      "\n",
      "Processing call [288] out of [1000] = [28.8%]... ETA mm:ss 27:48\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,846 ms\n",
      "Tokens per second [15.8] input tokens [614] + xml response tokens [45] = total tokens i/o [659]\n",
      "Response: [<response><command>search phind current tab</command><args>What are the best practices for handling reset connections in network communications in Python?</args></response>]\n",
      "\n",
      "Processing call [289] out of [1000] = [28.9%]... ETA mm:ss 27:47\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,316 ms\n",
      "Tokens per second [16.0] input tokens [372] + xml response tokens [37] = total tokens i/o [409]\n",
      "Response: [<response><command>agent router go to date and time</command><args>Mumbai, India</args></response>]\n",
      "\n",
      "Processing call [290] out of [1000] = [29.0%]... ETA mm:ss 27:44\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,633 ms\n",
      "Tokens per second [15.9] input tokens [363] + xml response tokens [26] = total tokens i/o [389]\n",
      "Response: [<response><command>none</command><args></args></response>]\n",
      "\n",
      "Processing call [291] out of [1000] = [29.1%]... ETA mm:ss 27:40\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,034 ms\n",
      "Tokens per second [15.7] input tokens [602] + xml response tokens [32] = total tokens i/o [634]\n",
      "Response: [<response><command>search google current tab</command><args>ImportWarning</args></response>]\n",
      "\n",
      "Processing call [292] out of [1000] = [29.2%]... ETA mm:ss 27:37\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,941 ms\n",
      "Tokens per second [16.0] input tokens [370] + xml response tokens [31] = total tokens i/o [401]\n",
      "Response: [<response><command>agent router go to todo list</command><args></args></response>]\n",
      "\n",
      "Processing call [293] out of [1000] = [29.3%]... ETA mm:ss 27:34\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 3,035 ms\n",
      "Tokens per second [15.8] input tokens [613] + xml response tokens [48] = total tokens i/o [661]\n",
      "Response: [<response><command>search kagi current tab</command><args>Sentiment Analysis tools: What tools are widely used for sentiment analysis in social media data?</args></response>]\n",
      "\n",
      "Processing call [294] out of [1000] = [29.4%]... ETA mm:ss 27:33\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,408 ms\n",
      "Tokens per second [15.8] input tokens [607] + xml response tokens [38] = total tokens i/o [645]\n",
      "Response: [<response><command>go to current tab</command><args>prod.incrediblejellyfish.net</args></response>]\n",
      "\n",
      "Processing call [295] out of [1000] = [29.5%]... ETA mm:ss 27:31\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,411 ms\n",
      "Tokens per second [15.8] input tokens [606] + xml response tokens [38] = total tokens i/o [644]\n",
      "Response: [<response><command>go to current tab</command><args>login.magnificenthamburger.gov</args></response>]\n",
      "\n",
      "Processing call [296] out of [1000] = [29.6%]... ETA mm:ss 27:29\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,100 ms\n",
      "Tokens per second [15.7] input tokens [601] + xml response tokens [33] = total tokens i/o [634]\n",
      "Response: [<response><command>search kagi current tab</command><args>Overflow Error</args></response>]\n",
      "\n",
      "Processing call [297] out of [1000] = [29.7%]... ETA mm:ss 27:26\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,221 ms\n",
      "Tokens per second [15.8] input tokens [604] + xml response tokens [35] = total tokens i/o [639]\n",
      "Response: [<response><command>search current tab</command><args>how to make homemade bread</args></response>]\n",
      "\n",
      "Processing call [298] out of [1000] = [29.8%]... ETA mm:ss 27:23\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,133 ms\n",
      "Tokens per second [15.9] input tokens [386] + xml response tokens [34] = total tokens i/o [420]\n",
      "Response: [<response><command>agent router go to receptionist</command><args>Information Clerk</args></response>]\n",
      "\n",
      "Processing call [299] out of [1000] = [29.9%]... ETA mm:ss 27:20\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,349 ms\n",
      "Tokens per second [15.7] input tokens [605] + xml response tokens [37] = total tokens i/o [642]\n",
      "Response: [<response><command>go to current tab</command><args>dev.spectacularwalrus.info</args></response>]\n",
      "\n",
      "Processing call [300] out of [1000] = [30.0%]... ETA mm:ss 27:18\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,408 ms\n",
      "Tokens per second [15.8] input tokens [606] + xml response tokens [38] = total tokens i/o [644]\n",
      "Response: [<response><command>search kagi new tab</command><args>Using Pandas for ETL processes</args></response>]\n",
      "\n",
      "Processing call [301] out of [1000] = [30.1%]... ETA mm:ss 27:16\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,663 ms\n",
      "Tokens per second [15.8] input tokens [614] + xml response tokens [42] = total tokens i/o [656]\n",
      "Response: [<response><command>search kagi new tab</command><args>How can decoding errors with Unicode characters be resolved in Python?</args></response>]\n",
      "\n",
      "Processing call [302] out of [1000] = [30.2%]... ETA mm:ss 27:14\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,968 ms\n",
      "Tokens per second [15.8] input tokens [599] + xml response tokens [31] = total tokens i/o [630]\n",
      "Response: [<response><command>search current tab</command><args>Memory Error</args></response>]\n",
      "\n",
      "Processing call [303] out of [1000] = [30.3%]... ETA mm:ss 27:11\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,189 ms\n",
      "Tokens per second [16.0] input tokens [364] + xml response tokens [35] = total tokens i/o [399]\n",
      "Response: [<response><command>agent router go to receptionist</command><args>Receptionist</args></response>]\n",
      "\n",
      "Processing call [304] out of [1000] = [30.4%]... ETA mm:ss 27:08\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,095 ms\n",
      "Tokens per second [15.8] input tokens [600] + xml response tokens [33] = total tokens i/o [633]\n",
      "Response: [<response><command>search phind new tab</command><args>ImportWarning</args></response>]\n",
      "\n",
      "Processing call [305] out of [1000] = [30.5%]... ETA mm:ss 27:05\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 3,336 ms\n",
      "Tokens per second [15.9] input tokens [621] + xml response tokens [53] = total tokens i/o [674]\n",
      "Response: [<response><command>search google current tab</command><args>Supervised vs. Unsupervised Learning: What are the key differences between supervised and unsupervised learning in AI?</args></response>]\n",
      "\n",
      "Processing call [306] out of [1000] = [30.6%]... ETA mm:ss 27:05\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 3,031 ms\n",
      "Tokens per second [15.8] input tokens [615] + xml response tokens [48] = total tokens i/o [663]\n",
      "Response: [<response><command>search kagi current tab</command><args>ROC Curve analysis: How is ROC curve analysis utilized to evaluate the performance of binary classifiers?</args></response>]\n",
      "\n",
      "Processing call [307] out of [1000] = [30.7%]... ETA mm:ss 27:05\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,187 ms\n",
      "Tokens per second [16.0] input tokens [367] + xml response tokens [35] = total tokens i/o [402]\n",
      "Response: [<response><command>agent router go to weather</command><args>Oslo, Norway</args></response>]\n",
      "\n",
      "Processing call [308] out of [1000] = [30.8%]... ETA mm:ss 27:02\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,096 ms\n",
      "Tokens per second [15.7] input tokens [605] + xml response tokens [33] = total tokens i/o [638]\n",
      "Response: [<response><command>search google new tab</command><args>best online games</args></response>]\n",
      "\n",
      "Processing call [309] out of [1000] = [30.9%]... ETA mm:ss 26:59\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,717 ms\n",
      "Tokens per second [15.8] input tokens [611] + xml response tokens [43] = total tokens i/o [654]\n",
      "Response: [<response><command>search phind current tab</command><args>File Not Found Error: File not found or path is incorrect</args></response>]\n",
      "\n",
      "Processing call [310] out of [1000] = [31.0%]... ETA mm:ss 26:57\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,313 ms\n",
      "Tokens per second [16.0] input tokens [365] + xml response tokens [37] = total tokens i/o [402]\n",
      "Response: [<response><command>agent router go to date and time</command><args>Omaha, Nebraska</args></response>]\n",
      "\n",
      "Processing call [311] out of [1000] = [31.1%]... ETA mm:ss 26:55\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 3,032 ms\n",
      "Tokens per second [15.8] input tokens [615] + xml response tokens [48] = total tokens i/o [663]\n",
      "Response: [<response><command>search phind current tab</command><args>Normalization vs. Standardization: When should you choose normalization over standardization in data preprocessing?</args></response>]\n",
      "\n",
      "Processing call [312] out of [1000] = [31.2%]... ETA mm:ss 26:54\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,194 ms\n",
      "Tokens per second [16.0] input tokens [366] + xml response tokens [35] = total tokens i/o [401]\n",
      "Response: [<response><command>agent router go to weather</command><args>Portland, Oregon</args></response>]\n",
      "\n",
      "Processing call [313] out of [1000] = [31.3%]... ETA mm:ss 26:52\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,033 ms\n",
      "Tokens per second [15.7] input tokens [603] + xml response tokens [32] = total tokens i/o [635]\n",
      "Response: [<response><command>search kagi using clipboard new tab</command><args></args></response>]\n",
      "\n",
      "Processing call [314] out of [1000] = [31.4%]... ETA mm:ss 26:49\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 3,155 ms\n",
      "Tokens per second [15.8] input tokens [617] + xml response tokens [50] = total tokens i/o [667]\n",
      "Response: [<response><command>search perplexity current tab</command><args>Hadoop MapReduce tutorial: Where can you find a beginner-friendly tutorial for Hadoop MapReduce?</args></response>]\n",
      "\n",
      "Processing call [315] out of [1000] = [31.5%]... ETA mm:ss 26:48\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,779 ms\n",
      "Tokens per second [15.8] input tokens [611] + xml response tokens [44] = total tokens i/o [655]\n",
      "Response: [<response><command>search google current tab</command><args>OpenAI latest projects: What are the latest projects undertaken by OpenAI?</args></response>]\n",
      "\n",
      "Processing call [316] out of [1000] = [31.6%]... ETA mm:ss 26:47\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,875 ms\n",
      "Tokens per second [16.0] input tokens [379] + xml response tokens [30] = total tokens i/o [409]\n",
      "Response: [<response><command>agent router go to calendar</command><args></args></response>]\n",
      "\n",
      "Processing call [317] out of [1000] = [31.7%]... ETA mm:ss 26:43\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,533 ms\n",
      "Tokens per second [15.8] input tokens [608] + xml response tokens [40] = total tokens i/o [648]\n",
      "Response: [<response><command>search new tab</command><args>AI in gaming: How is AI changing the gaming industry?</args></response>]\n",
      "\n",
      "Processing call [318] out of [1000] = [31.8%]... ETA mm:ss 26:41\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,188 ms\n",
      "Tokens per second [16.0] input tokens [380] + xml response tokens [35] = total tokens i/o [415]\n",
      "Response: [<response><command>agent router go to weather</command><args>San Antonio, Texas</args></response>]\n",
      "\n",
      "Processing call [319] out of [1000] = [31.9%]... ETA mm:ss 26:39\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,843 ms\n",
      "Tokens per second [15.8] input tokens [612] + xml response tokens [45] = total tokens i/o [657]\n",
      "Response: [<response><command>search phind current tab</command><args>How do you handle broken pipe errors in Python, especially in network communications?</args></response>]\n",
      "\n",
      "Processing call [320] out of [1000] = [32.0%]... ETA mm:ss 26:37\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,093 ms\n",
      "Tokens per second [15.8] input tokens [604] + xml response tokens [33] = total tokens i/o [637]\n",
      "Response: [<response><command>search perplexity new tab</command><args>ImportWarning</args></response>]\n",
      "\n",
      "Processing call [321] out of [1000] = [32.1%]... ETA mm:ss 26:34\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,345 ms\n",
      "Tokens per second [15.8] input tokens [605] + xml response tokens [37] = total tokens i/o [642]\n",
      "Response: [<response><command>go to new tab</command><args>alpha.beautifullemur.io</args></response>]\n",
      "\n",
      "Processing call [322] out of [1000] = [32.2%]... ETA mm:ss 26:32\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,376 ms\n",
      "Tokens per second [16.0] input tokens [377] + xml response tokens [38] = total tokens i/o [415]\n",
      "Response: [<response><command>agent router go to date and time</command><args>Wichita, Kansas</args></response>]\n",
      "\n",
      "Processing call [323] out of [1000] = [32.3%]... ETA mm:ss 26:30\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,218 ms\n",
      "Tokens per second [15.8] input tokens [605] + xml response tokens [35] = total tokens i/o [640]\n",
      "Response: [<response><command>search google scholar new tab</command><args>how to improve memory</args></response>]\n",
      "\n",
      "Processing call [324] out of [1000] = [32.4%]... ETA mm:ss 26:27\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,350 ms\n",
      "Tokens per second [15.7] input tokens [605] + xml response tokens [37] = total tokens i/o [642]\n",
      "Response: [<response><command>go to new tab</command><args>prod.jubilantlemur.io</args></response>]\n",
      "\n",
      "Processing call [325] out of [1000] = [32.5%]... ETA mm:ss 26:25\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,096 ms\n",
      "Tokens per second [15.7] input tokens [604] + xml response tokens [33] = total tokens i/o [637]\n",
      "Response: [<response><command>search phind current tab</command><args>Overflow Error</args></response>]\n",
      "\n",
      "Processing call [326] out of [1000] = [32.6%]... ETA mm:ss 26:22\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 3,031 ms\n",
      "Tokens per second [15.8] input tokens [620] + xml response tokens [48] = total tokens i/o [668]\n",
      "Response: [<response><command>search google scholar new tab</command><args>What are the causes of floating point errors in Python, and how can they be minimized?</args></response>]\n",
      "\n",
      "Processing call [327] out of [1000] = [32.7%]... ETA mm:ss 26:21\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,908 ms\n",
      "Tokens per second [15.8] input tokens [615] + xml response tokens [46] = total tokens i/o [661]\n",
      "Response: [<response><command>search perplexity current tab</command><args>Quantum AI computing developments: What are the latest developments in quantum AI computing?</args></response>]\n",
      "\n",
      "Processing call [328] out of [1000] = [32.8%]... ETA mm:ss 26:20\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,473 ms\n",
      "Tokens per second [15.8] input tokens [608] + xml response tokens [39] = total tokens i/o [647]\n",
      "Response: [<response><command>search phind current tab</command><args>Setting and resetting index in Pandas</args></response>]\n",
      "\n",
      "Processing call [329] out of [1000] = [32.9%]... ETA mm:ss 26:18\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,219 ms\n",
      "Tokens per second [15.8] input tokens [602] + xml response tokens [35] = total tokens i/o [637]\n",
      "Response: [<response><command>search current tab</command><args>best vacation spots in the world</args></response>]\n",
      "\n",
      "Processing call [330] out of [1000] = [33.0%]... ETA mm:ss 26:15\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,316 ms\n",
      "Tokens per second [16.0] input tokens [368] + xml response tokens [37] = total tokens i/o [405]\n",
      "Response: [<response><command>agent router go to weather</command><args>Greensboro, North Carolina</args></response>]\n",
      "\n",
      "Processing call [331] out of [1000] = [33.1%]... ETA mm:ss 26:13\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 3,036 ms\n",
      "Tokens per second [15.8] input tokens [618] + xml response tokens [48] = total tokens i/o [666]\n",
      "Response: [<response><command>search current tab</command><args>Hadoop MapReduce tutorial: Where can you find a beginner-friendly tutorial for Hadoop MapReduce?</args></response>]\n",
      "\n",
      "Processing call [332] out of [1000] = [33.2%]... ETA mm:ss 26:12\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,033 ms\n",
      "Tokens per second [15.7] input tokens [598] + xml response tokens [32] = total tokens i/o [630]\n",
      "Response: [<response><command>search current tab</command><args>Broken Pipe Error</args></response>]\n",
      "\n",
      "Processing call [333] out of [1000] = [33.3%]... ETA mm:ss 26:09\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,975 ms\n",
      "Tokens per second [15.8] input tokens [618] + xml response tokens [47] = total tokens i/o [665]\n",
      "Response: [<response><command>search google scholar new tab</command><args>AWS SageMaker for ML: How does AWS SageMaker facilitate machine learning model development?</args></response>]\n",
      "\n",
      "Processing call [334] out of [1000] = [33.4%]... ETA mm:ss 26:07\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,225 ms\n",
      "Tokens per second [15.7] input tokens [607] + xml response tokens [35] = total tokens i/o [642]\n",
      "Response: [<response><command>search perplexity new tab</command><args>AI in financial forecasting</args></response>]\n",
      "\n",
      "Processing call [335] out of [1000] = [33.5%]... ETA mm:ss 26:05\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,350 ms\n",
      "Tokens per second [15.7] input tokens [604] + xml response tokens [37] = total tokens i/o [641]\n",
      "Response: [<response><command>search kagi current tab</command><args>why do dogs wag their tail</args></response>]\n",
      "\n",
      "Processing call [336] out of [1000] = [33.6%]... ETA mm:ss 26:03\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,406 ms\n",
      "Tokens per second [15.8] input tokens [605] + xml response tokens [38] = total tokens i/o [643]\n",
      "Response: [<response><command>go to current tab</command><args>test.hilarioushamburger.info</args></response>]\n",
      "\n",
      "Processing call [337] out of [1000] = [33.7%]... ETA mm:ss 26:00\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,216 ms\n",
      "Tokens per second [15.8] input tokens [606] + xml response tokens [35] = total tokens i/o [641]\n",
      "Response: [<response><command>search google new tab</command><args>AI in content moderation</args></response>]\n",
      "\n",
      "Processing call [338] out of [1000] = [33.8%]... ETA mm:ss 25:58\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,345 ms\n",
      "Tokens per second [15.8] input tokens [608] + xml response tokens [37] = total tokens i/o [645]\n",
      "Response: [<response><command>go to current tab</command><args>test.spectacularxylophone.com</args></response>]\n",
      "\n",
      "Processing call [339] out of [1000] = [33.9%]... ETA mm:ss 25:55\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,035 ms\n",
      "Tokens per second [15.7] input tokens [602] + xml response tokens [32] = total tokens i/o [634]\n",
      "Response: [<response><command>search perplexity using clipboard current tab</command><args></args></response>]\n",
      "\n",
      "Processing call [340] out of [1000] = [34.0%]... ETA mm:ss 25:52\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,218 ms\n",
      "Tokens per second [15.8] input tokens [604] + xml response tokens [35] = total tokens i/o [639]\n",
      "Response: [<response><command>search google new tab</command><args>buying a new laptop</args></response>]\n",
      "\n",
      "Processing call [341] out of [1000] = [34.1%]... ETA mm:ss 25:50\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,783 ms\n",
      "Tokens per second [15.8] input tokens [617] + xml response tokens [44] = total tokens i/o [661]\n",
      "Response: [<response><command>search google new tab</command><args>How do you handle broken pipe errors in Python, especially in network communications?</args></response>]\n",
      "\n",
      "Processing call [342] out of [1000] = [34.2%]... ETA mm:ss 25:48\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,219 ms\n",
      "Tokens per second [15.8] input tokens [601] + xml response tokens [35] = total tokens i/o [636]\n",
      "Response: [<response><command>search google scholar current tab</command><args>IBM Watson AI tools</args></response>]\n",
      "\n",
      "Processing call [343] out of [1000] = [34.3%]... ETA mm:ss 25:46\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,031 ms\n",
      "Tokens per second [15.8] input tokens [600] + xml response tokens [32] = total tokens i/o [632]\n",
      "Response: [<response><command>search perplexity using clipboard new tab</command><args></args></response>]\n",
      "\n",
      "Processing call [344] out of [1000] = [34.4%]... ETA mm:ss 25:43\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,315 ms\n",
      "Tokens per second [16.0] input tokens [371] + xml response tokens [37] = total tokens i/o [408]\n",
      "Response: [<response><command>agent router go to date and time</command><args>Phoenix, Arizona</args></response>]\n",
      "\n",
      "Processing call [345] out of [1000] = [34.5%]... ETA mm:ss 25:40\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,969 ms\n",
      "Tokens per second [15.8] input tokens [617] + xml response tokens [47] = total tokens i/o [664]\n",
      "Response: [<response><command>search kagi current tab</command><args>Feature Engineering best practices: How does feature engineering improve the performance of machine learning models?</args></response>]\n",
      "\n",
      "Processing call [346] out of [1000] = [34.6%]... ETA mm:ss 25:39\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,346 ms\n",
      "Tokens per second [15.8] input tokens [607] + xml response tokens [37] = total tokens i/o [644]\n",
      "Response: [<response><command>search kagi new tab</command><args>what are the benefits of exercise?</args></response>]\n",
      "\n",
      "Processing call [347] out of [1000] = [34.7%]... ETA mm:ss 25:37\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,034 ms\n",
      "Tokens per second [15.7] input tokens [602] + xml response tokens [32] = total tokens i/o [634]\n",
      "Response: [<response><command>search kagi using clipboard current tab</command><args></args></response>]\n",
      "\n",
      "Processing call [348] out of [1000] = [34.8%]... ETA mm:ss 25:34\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,374 ms\n",
      "Tokens per second [16.0] input tokens [371] + xml response tokens [38] = total tokens i/o [409]\n",
      "Response: [<response><command>agent router go to date and time</command><args>Seoul, South Korea</args></response>]\n",
      "\n",
      "Processing call [349] out of [1000] = [34.9%]... ETA mm:ss 25:31\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 3,096 ms\n",
      "Tokens per second [15.8] input tokens [620] + xml response tokens [49] = total tokens i/o [669]\n",
      "Response: [<response><command>search google current tab</command><args>Hadoop MapReduce tutorial: Where can you find a beginner-friendly tutorial for Hadoop MapReduce?</args></response>]\n",
      "\n",
      "Processing call [350] out of [1000] = [35.0%]... ETA mm:ss 25:30\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,654 ms\n",
      "Tokens per second [15.8] input tokens [613] + xml response tokens [42] = total tokens i/o [655]\n",
      "Response: [<response><command>search google scholar new tab</command><args>AI in gaming: How is AI changing the gaming industry?</args></response>]\n",
      "\n",
      "Processing call [351] out of [1000] = [35.1%]... ETA mm:ss 25:29\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,156 ms\n",
      "Tokens per second [15.8] input tokens [601] + xml response tokens [34] = total tokens i/o [635]\n",
      "Response: [<response><command>search current tab</command><args>AWS SageMaker for ML</args></response>]\n",
      "\n",
      "Processing call [352] out of [1000] = [35.2%]... ETA mm:ss 25:26\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,966 ms\n",
      "Tokens per second [15.8] input tokens [612] + xml response tokens [47] = total tokens i/o [659]\n",
      "Response: [<response><command>search google scholar current tab</command><args>Feature Engineering best practices: How does feature engineering improve the performance of machine learning models?</args></response>]\n",
      "\n",
      "Processing call [353] out of [1000] = [35.3%]... ETA mm:ss 25:25\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,874 ms\n",
      "Tokens per second [16.0] input tokens [370] + xml response tokens [30] = total tokens i/o [400]\n",
      "Response: [<response><command>agent router go to calendar</command><args></args></response>]\n",
      "\n",
      "Processing call [354] out of [1000] = [35.4%]... ETA mm:ss 25:21\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,251 ms\n",
      "Tokens per second [16.0] input tokens [378] + xml response tokens [36] = total tokens i/o [414]\n",
      "Response: [<response><command>agent router go to weather</command><args>Tashkent, Uzbekistan</args></response>]\n",
      "\n",
      "Processing call [355] out of [1000] = [35.5%]... ETA mm:ss 25:19\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,720 ms\n",
      "Tokens per second [15.8] input tokens [611] + xml response tokens [43] = total tokens i/o [654]\n",
      "Response: [<response><command>search current tab</command><args>Voice Recognition technologies: What are the latest advancements in voice recognition technology?</args></response>]\n",
      "\n",
      "Processing call [356] out of [1000] = [35.6%]... ETA mm:ss 25:17\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,349 ms\n",
      "Tokens per second [15.8] input tokens [603] + xml response tokens [37] = total tokens i/o [640]\n",
      "Response: [<response><command>go to current tab</command><args>dev.jubilantquartz.com</args></response>]\n",
      "\n",
      "Processing call [357] out of [1000] = [35.7%]... ETA mm:ss 25:15\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,879 ms\n",
      "Tokens per second [16.0] input tokens [376] + xml response tokens [30] = total tokens i/o [406]\n",
      "Response: [<response><command>agent router go to math</command><args></args></response>]\n",
      "\n",
      "Processing call [358] out of [1000] = [35.8%]... ETA mm:ss 25:12\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,472 ms\n",
      "Tokens per second [15.8] input tokens [606] + xml response tokens [39] = total tokens i/o [645]\n",
      "Response: [<response><command>go to new tab</command><args>mail.amazingkangaroo.gov</args></response>]\n",
      "\n",
      "Processing call [359] out of [1000] = [35.9%]... ETA mm:ss 25:09\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,034 ms\n",
      "Tokens per second [15.7] input tokens [614] + xml response tokens [32] = total tokens i/o [646]\n",
      "Response: [<response><command>search kagi using clipboard new tab</command><args></args></response>]\n",
      "\n",
      "Processing call [360] out of [1000] = [36.0%]... ETA mm:ss 25:07\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 3,094 ms\n",
      "Tokens per second [15.8] input tokens [616] + xml response tokens [49] = total tokens i/o [665]\n",
      "Response: [<response><command>search perplexity current tab</command><args>What are the causes of translation errors with Unicode characters in Python and how can they be fixed?</args></response>]\n",
      "\n",
      "Processing call [361] out of [1000] = [36.1%]... ETA mm:ss 25:05\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 3,033 ms\n",
      "Tokens per second [15.8] input tokens [616] + xml response tokens [48] = total tokens i/o [664]\n",
      "Response: [<response><command>search perplexity new tab</command><args>How do you address deprecation warnings in Python to ensure code compatibility with future versions?</args></response>]\n",
      "\n",
      "Processing call [362] out of [1000] = [36.2%]... ETA mm:ss 25:04\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,286 ms\n",
      "Tokens per second [15.7] input tokens [606] + xml response tokens [36] = total tokens i/o [642]\n",
      "Response: [<response><command>search google scholar new tab</command><args>how to make pizza dough</args></response>]\n",
      "\n",
      "Processing call [363] out of [1000] = [36.3%]... ETA mm:ss 25:02\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,032 ms\n",
      "Tokens per second [15.7] input tokens [611] + xml response tokens [32] = total tokens i/o [643]\n",
      "Response: [<response><command>search google scholar using clipboard current tab</command><args></args></response>]\n",
      "\n",
      "Processing call [364] out of [1000] = [36.4%]... ETA mm:ss 24:59\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 3,029 ms\n",
      "Tokens per second [15.8] input tokens [618] + xml response tokens [48] = total tokens i/o [666]\n",
      "Response: [<response><command>search phind new tab</command><args>How do you address deprecation warnings in Python to ensure code compatibility with future versions?</args></response>]\n",
      "\n",
      "Processing call [365] out of [1000] = [36.5%]... ETA mm:ss 24:58\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,967 ms\n",
      "Tokens per second [15.8] input tokens [615] + xml response tokens [47] = total tokens i/o [662]\n",
      "Response: [<response><command>search google current tab</command><args>What are the causes of floating point errors in Python, and how can they be minimized?</args></response>]\n",
      "\n",
      "Processing call [366] out of [1000] = [36.6%]... ETA mm:ss 24:56\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,158 ms\n",
      "Tokens per second [15.8] input tokens [605] + xml response tokens [34] = total tokens i/o [639]\n",
      "Response: [<response><command>search new tab</command><args>AWS SageMaker for ML</args></response>]\n",
      "\n",
      "Processing call [367] out of [1000] = [36.7%]... ETA mm:ss 24:54\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,344 ms\n",
      "Tokens per second [15.8] input tokens [602] + xml response tokens [37] = total tokens i/o [639]\n",
      "Response: [<response><command>search google scholar current tab</command><args>User Warning: User-defined warning</args></response>]\n",
      "\n",
      "Processing call [368] out of [1000] = [36.8%]... ETA mm:ss 24:51\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,097 ms\n",
      "Tokens per second [15.7] input tokens [602] + xml response tokens [33] = total tokens i/o [635]\n",
      "Response: [<response><command>search current tab</command><args>URLError</args></response>]\n",
      "\n",
      "Processing call [369] out of [1000] = [36.9%]... ETA mm:ss 24:48\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,033 ms\n",
      "Tokens per second [15.7] input tokens [597] + xml response tokens [32] = total tokens i/o [629]\n",
      "Response: [<response><command>search google scholar current tab</command><args>Warning</args></response>]\n",
      "\n",
      "Processing call [370] out of [1000] = [37.0%]... ETA mm:ss 24:46\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,187 ms\n",
      "Tokens per second [16.0] input tokens [376] + xml response tokens [35] = total tokens i/o [411]\n",
      "Response: [<response><command>agent router go to receptionist</command><args>Administrative Assistant</args></response>]\n",
      "\n",
      "Processing call [371] out of [1000] = [37.1%]... ETA mm:ss 24:43\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,250 ms\n",
      "Tokens per second [16.0] input tokens [364] + xml response tokens [36] = total tokens i/o [400]\n",
      "Response: [<response><command>agent router go to receptionist</command><args>Front Office Coordinator Agent</args></response>]\n",
      "\n",
      "Processing call [372] out of [1000] = [37.2%]... ETA mm:ss 24:40\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,406 ms\n",
      "Tokens per second [15.8] input tokens [608] + xml response tokens [38] = total tokens i/o [646]\n",
      "Response: [<response><command>search kagi current tab</command><args>Saving Pandas DataFrame to CSV</args></response>]\n",
      "\n",
      "Processing call [373] out of [1000] = [37.3%]... ETA mm:ss 24:38\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,968 ms\n",
      "Tokens per second [15.8] input tokens [618] + xml response tokens [47] = total tokens i/o [665]\n",
      "Response: [<response><command>search phind current tab</command><args>Feature Engineering best practices: How does feature engineering improve the performance of machine learning models?</args></response>]\n",
      "\n",
      "Processing call [374] out of [1000] = [37.4%]... ETA mm:ss 24:37\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,844 ms\n",
      "Tokens per second [15.8] input tokens [613] + xml response tokens [45] = total tokens i/o [658]\n",
      "Response: [<response><command>search current tab</command><args>AWS SageMaker for ML: How does AWS SageMaker facilitate machine learning model development?</args></response>]\n",
      "\n",
      "Processing call [375] out of [1000] = [37.5%]... ETA mm:ss 24:35\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,092 ms\n",
      "Tokens per second [15.8] input tokens [604] + xml response tokens [33] = total tokens i/o [637]\n",
      "Response: [<response><command>search current tab</command><args>FileNotFoundError</args></response>]\n",
      "\n",
      "Processing call [376] out of [1000] = [37.6%]... ETA mm:ss 24:32\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,310 ms\n",
      "Tokens per second [16.0] input tokens [373] + xml response tokens [37] = total tokens i/o [410]\n",
      "Response: [<response><command>agent router go to date and time</command><args>Dublin, Ireland</args></response>]\n",
      "\n",
      "Processing call [377] out of [1000] = [37.7%]... ETA mm:ss 24:30\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,158 ms\n",
      "Tokens per second [15.8] input tokens [604] + xml response tokens [34] = total tokens i/o [638]\n",
      "Response: [<response><command>search kagi new tab</command><args>hip hop music</args></response>]\n",
      "\n",
      "Processing call [378] out of [1000] = [37.8%]... ETA mm:ss 24:27\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,038 ms\n",
      "Tokens per second [15.7] input tokens [601] + xml response tokens [32] = total tokens i/o [633]\n",
      "Response: [<response><command>search google scholar using clipboard new tab</command><args></args></response>]\n",
      "\n",
      "Processing call [379] out of [1000] = [37.9%]... ETA mm:ss 24:24\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,346 ms\n",
      "Tokens per second [15.8] input tokens [605] + xml response tokens [37] = total tokens i/o [642]\n",
      "Response: [<response><command>go to current tab</command><args>prod.jubilantxylophone.org</args></response>]\n",
      "\n",
      "Processing call [380] out of [1000] = [38.0%]... ETA mm:ss 24:22\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,903 ms\n",
      "Tokens per second [15.8] input tokens [616] + xml response tokens [46] = total tokens i/o [662]\n",
      "Response: [<response><command>search new tab</command><args>Azure Machine Learning platform: What features does the Azure Machine Learning platform provide for model deployment?</args></response>]\n",
      "\n",
      "Processing call [381] out of [1000] = [38.1%]... ETA mm:ss 24:21\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,906 ms\n",
      "Tokens per second [15.7] input tokens [606] + xml response tokens [30] = total tokens i/o [636]\n",
      "Response: [<response><command>search using clipboard new tab</command><args></args></response>]\n",
      "\n",
      "Processing call [382] out of [1000] = [38.2%]... ETA mm:ss 24:17\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,219 ms\n",
      "Tokens per second [15.8] input tokens [609] + xml response tokens [35] = total tokens i/o [644]\n",
      "Response: [<response><command>search perplexity current tab</command><args>how to improve memory</args></response>]\n",
      "\n",
      "Processing call [383] out of [1000] = [38.3%]... ETA mm:ss 24:15\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,842 ms\n",
      "Tokens per second [15.8] input tokens [617] + xml response tokens [45] = total tokens i/o [662]\n",
      "Response: [<response><command>search google scholar current tab</command><args>OpenAI latest projects: What are the latest projects undertaken by OpenAI?</args></response>]\n",
      "\n",
      "Processing call [384] out of [1000] = [38.4%]... ETA mm:ss 24:13\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,349 ms\n",
      "Tokens per second [15.8] input tokens [607] + xml response tokens [37] = total tokens i/o [644]\n",
      "Response: [<response><command>search phind current tab</command><args>OS Error: Operating system error</args></response>]\n",
      "\n",
      "Processing call [385] out of [1000] = [38.5%]... ETA mm:ss 24:11\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,283 ms\n",
      "Tokens per second [15.8] input tokens [609] + xml response tokens [36] = total tokens i/o [645]\n",
      "Response: [<response><command>search phind new tab</command><args>AI in content moderation</args></response>]\n",
      "\n",
      "Processing call [386] out of [1000] = [38.6%]... ETA mm:ss 24:08\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,408 ms\n",
      "Tokens per second [15.8] input tokens [604] + xml response tokens [38] = total tokens i/o [642]\n",
      "Response: [<response><command>search google current tab</command><args>Deprecation Warning: Feature is deprecated</args></response>]\n",
      "\n",
      "Processing call [387] out of [1000] = [38.7%]... ETA mm:ss 24:06\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,157 ms\n",
      "Tokens per second [15.8] input tokens [599] + xml response tokens [34] = total tokens i/o [633]\n",
      "Response: [<response><command>search perplexity current tab</command><args>learning Japanese online</args></response>]\n",
      "\n",
      "Processing call [388] out of [1000] = [38.8%]... ETA mm:ss 24:03\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,189 ms\n",
      "Tokens per second [16.0] input tokens [383] + xml response tokens [35] = total tokens i/o [418]\n",
      "Response: [<response><command>agent router go to receptionist</command><args>Office Assistant Agent</args></response>]\n",
      "\n",
      "Processing call [389] out of [1000] = [38.9%]... ETA mm:ss 24:01\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,658 ms\n",
      "Tokens per second [15.7] input tokens [603] + xml response tokens [26] = total tokens i/o [629]\n",
      "Response: [<response><command>none</command><args></args></response>]\n",
      "\n",
      "Processing call [390] out of [1000] = [39.0%]... ETA mm:ss 23:57\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,969 ms\n",
      "Tokens per second [15.7] input tokens [611] + xml response tokens [31] = total tokens i/o [642]\n",
      "Response: [<response><command>search google using clipboard new tab</command><args></args></response>]\n",
      "\n",
      "Processing call [391] out of [1000] = [39.1%]... ETA mm:ss 23:54\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,185 ms\n",
      "Tokens per second [16.0] input tokens [376] + xml response tokens [35] = total tokens i/o [411]\n",
      "Response: [<response><command>agent router go to date and time</command><args>Macau</args></response>]\n",
      "\n",
      "Processing call [392] out of [1000] = [39.2%]... ETA mm:ss 23:52\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,279 ms\n",
      "Tokens per second [15.8] input tokens [610] + xml response tokens [36] = total tokens i/o [646]\n",
      "Response: [<response><command>search perplexity new tab</command><args>Pending Deprecation Warning</args></response>]\n",
      "\n",
      "Processing call [393] out of [1000] = [39.3%]... ETA mm:ss 23:49\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 3,027 ms\n",
      "Tokens per second [15.9] input tokens [621] + xml response tokens [48] = total tokens i/o [669]\n",
      "Response: [<response><command>search phind new tab</command><args>ROC Curve analysis: How is ROC curve analysis utilized to evaluate the performance of binary classifiers?</args></response>]\n",
      "\n",
      "Processing call [394] out of [1000] = [39.4%]... ETA mm:ss 23:48\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,028 ms\n",
      "Tokens per second [15.8] input tokens [603] + xml response tokens [32] = total tokens i/o [635]\n",
      "Response: [<response><command>search google new tab</command><args>Reference Error</args></response>]\n",
      "\n",
      "Processing call [395] out of [1000] = [39.5%]... ETA mm:ss 23:45\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,277 ms\n",
      "Tokens per second [15.8] input tokens [607] + xml response tokens [36] = total tokens i/o [643]\n",
      "Response: [<response><command>search phind current tab</command><args>Pandas DataFrame creation</args></response>]\n",
      "\n",
      "Processing call [396] out of [1000] = [39.6%]... ETA mm:ss 23:43\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,906 ms\n",
      "Tokens per second [15.8] input tokens [617] + xml response tokens [46] = total tokens i/o [663]\n",
      "Response: [<response><command>search google scholar new tab</command><args>What are bytes warnings in Python, and how are they significant in data handling?</args></response>]\n",
      "\n",
      "Processing call [397] out of [1000] = [39.7%]... ETA mm:ss 23:41\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,968 ms\n",
      "Tokens per second [15.8] input tokens [601] + xml response tokens [31] = total tokens i/o [632]\n",
      "Response: [<response><command>search google using clipboard current tab</command><args></args></response>]\n",
      "\n",
      "Processing call [398] out of [1000] = [39.8%]... ETA mm:ss 23:38\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,156 ms\n",
      "Tokens per second [15.8] input tokens [601] + xml response tokens [34] = total tokens i/o [635]\n",
      "Response: [<response><command>search google scholar new tab</command><args>IndentationError</args></response>]\n",
      "\n",
      "Processing call [399] out of [1000] = [39.9%]... ETA mm:ss 23:36\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,034 ms\n",
      "Tokens per second [15.7] input tokens [601] + xml response tokens [32] = total tokens i/o [633]\n",
      "Response: [<response><command>search google current tab</command><args>Reference Error</args></response>]\n",
      "\n",
      "Processing call [400] out of [1000] = [40.0%]... ETA mm:ss 23:33\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,478 ms\n",
      "Tokens per second [15.7] input tokens [611] + xml response tokens [39] = total tokens i/o [650]\n",
      "Response: [<response><command>search perplexity new tab</command><args>Recursion Error: Maximum recursion depth exceeded</args></response>]\n",
      "\n",
      "Processing call [401] out of [1000] = [40.1%]... ETA mm:ss 23:31\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,220 ms\n",
      "Tokens per second [15.8] input tokens [607] + xml response tokens [35] = total tokens i/o [642]\n",
      "Response: [<response><command>search perplexity new tab</command><args>URLError</args></response>]\n",
      "\n",
      "Processing call [402] out of [1000] = [40.2%]... ETA mm:ss 23:28\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,153 ms\n",
      "Tokens per second [15.8] input tokens [602] + xml response tokens [34] = total tokens i/o [636]\n",
      "Response: [<response><command>search new tab</command><args>NoSQL databases for AI</args></response>]\n",
      "\n",
      "Processing call [403] out of [1000] = [40.3%]... ETA mm:ss 23:25\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,156 ms\n",
      "Tokens per second [15.8] input tokens [603] + xml response tokens [34] = total tokens i/o [637]\n",
      "Response: [<response><command>search kagi current tab</command><args>AI transparency initiatives</args></response>]\n",
      "\n",
      "Processing call [404] out of [1000] = [40.4%]... ETA mm:ss 23:23\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,653 ms\n",
      "Tokens per second [15.8] input tokens [614] + xml response tokens [42] = total tokens i/o [656]\n",
      "Response: [<response><command>search google new tab</command><args>How do you fix indentation errors that affect code structure in Python?</args></response>]\n",
      "\n",
      "Processing call [405] out of [1000] = [40.5%]... ETA mm:ss 23:21\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,183 ms\n",
      "Tokens per second [16.0] input tokens [371] + xml response tokens [35] = total tokens i/o [406]\n",
      "Response: [<response><command>agent router go to weather</command><args>Long Beach, California</args></response>]\n",
      "\n",
      "Processing call [406] out of [1000] = [40.6%]... ETA mm:ss 23:18\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,162 ms\n",
      "Tokens per second [15.7] input tokens [607] + xml response tokens [34] = total tokens i/o [641]\n",
      "Response: [<response><command>search google new tab</command><args>IBM Watson AI tools</args></response>]\n",
      "\n",
      "Processing call [407] out of [1000] = [40.7%]... ETA mm:ss 23:16\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,313 ms\n",
      "Tokens per second [16.0] input tokens [372] + xml response tokens [37] = total tokens i/o [409]\n",
      "Response: [<response><command>agent router go to date and time</command><args>Louisville, Kentucky</args></response>]\n",
      "\n",
      "Processing call [408] out of [1000] = [40.8%]... ETA mm:ss 23:13\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,440 ms\n",
      "Tokens per second [16.0] input tokens [370] + xml response tokens [39] = total tokens i/o [409]\n",
      "Response: [<response><command>agent router go to date and time</command><args>Cape Town, South Africa</args></response>]\n",
      "\n",
      "Processing call [409] out of [1000] = [40.9%]... ETA mm:ss 23:11\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,407 ms\n",
      "Tokens per second [15.8] input tokens [609] + xml response tokens [38] = total tokens i/o [647]\n",
      "Response: [<response><command>search google new tab</command><args>Setting and resetting index in Pandas</args></response>]\n",
      "\n",
      "Processing call [410] out of [1000] = [41.0%]... ETA mm:ss 23:09\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,879 ms\n",
      "Tokens per second [16.0] input tokens [382] + xml response tokens [30] = total tokens i/o [412]\n",
      "Response: [<response><command>agent router go to calendar</command><args></args></response>]\n",
      "\n",
      "Processing call [411] out of [1000] = [41.1%]... ETA mm:ss 23:06\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,221 ms\n",
      "Tokens per second [15.8] input tokens [601] + xml response tokens [35] = total tokens i/o [636]\n",
      "Response: [<response><command>search google scholar current tab</command><args>Connection Refused Error</args></response>]\n",
      "\n",
      "Processing call [412] out of [1000] = [41.2%]... ETA mm:ss 23:03\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 3,033 ms\n",
      "Tokens per second [15.8] input tokens [615] + xml response tokens [48] = total tokens i/o [663]\n",
      "Response: [<response><command>search phind current tab</command><args>Google Cloud AI services: What AI services does Google Cloud offer for developers and data scientists?</args></response>]\n",
      "\n",
      "Processing call [413] out of [1000] = [41.3%]... ETA mm:ss 23:02\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,309 ms\n",
      "Tokens per second [16.0] input tokens [367] + xml response tokens [37] = total tokens i/o [404]\n",
      "Response: [<response><command>agent router go to date and time</command><args>Jakarta, Indonesia</args></response>]\n",
      "\n",
      "Processing call [414] out of [1000] = [41.4%]... ETA mm:ss 22:59\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 3,153 ms\n",
      "Tokens per second [15.9] input tokens [624] + xml response tokens [50] = total tokens i/o [674]\n",
      "Response: [<response><command>search phind new tab</command><args>Hadoop MapReduce tutorial: Where can you find a beginner-friendly tutorial for Hadoop MapReduce?</args></response>]\n",
      "\n",
      "Processing call [415] out of [1000] = [41.5%]... ETA mm:ss 22:58\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,902 ms\n",
      "Tokens per second [15.9] input tokens [619] + xml response tokens [46] = total tokens i/o [665]\n",
      "Response: [<response><command>search phind new tab</command><args>How do you address runtime warnings in Python, especially those indicating risky runtime behavior?</args></response>]\n",
      "\n",
      "Processing call [416] out of [1000] = [41.6%]... ETA mm:ss 22:56\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,314 ms\n",
      "Tokens per second [16.0] input tokens [369] + xml response tokens [37] = total tokens i/o [406]\n",
      "Response: [<response><command>agent router go to date and time</command><args>Budapest, Hungary</args></response>]\n",
      "\n",
      "Processing call [417] out of [1000] = [41.7%]... ETA mm:ss 22:54\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,189 ms\n",
      "Tokens per second [16.0] input tokens [361] + xml response tokens [35] = total tokens i/o [396]\n",
      "Response: [<response><command>agent router go to weather</command><args>Stockton, California</args></response>]\n",
      "\n",
      "Processing call [418] out of [1000] = [41.8%]... ETA mm:ss 22:51\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,219 ms\n",
      "Tokens per second [15.8] input tokens [606] + xml response tokens [35] = total tokens i/o [641]\n",
      "Response: [<response><command>search new tab</command><args>Cross-tabulation in Pandas</args></response>]\n",
      "\n",
      "Processing call [419] out of [1000] = [41.9%]... ETA mm:ss 22:49\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,095 ms\n",
      "Tokens per second [15.8] input tokens [601] + xml response tokens [33] = total tokens i/o [634]\n",
      "Response: [<response><command>search new tab</command><args>IsADirectoryError</args></response>]\n",
      "\n",
      "Processing call [420] out of [1000] = [42.0%]... ETA mm:ss 22:46\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,094 ms\n",
      "Tokens per second [15.8] input tokens [605] + xml response tokens [33] = total tokens i/o [638]\n",
      "Response: [<response><command>search perplexity new tab</command><args>EnvironmentError</args></response>]\n",
      "\n",
      "Processing call [421] out of [1000] = [42.1%]... ETA mm:ss 22:43\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,345 ms\n",
      "Tokens per second [15.8] input tokens [603] + xml response tokens [37] = total tokens i/o [640]\n",
      "Response: [<response><command>search phind current tab</command><args>how to make homemade bread</args></response>]\n",
      "\n",
      "Processing call [422] out of [1000] = [42.2%]... ETA mm:ss 22:41\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,094 ms\n",
      "Tokens per second [15.8] input tokens [603] + xml response tokens [33] = total tokens i/o [636]\n",
      "Response: [<response><command>search kagi new tab</command><args>Reference Error</args></response>]\n",
      "\n",
      "Processing call [423] out of [1000] = [42.3%]... ETA mm:ss 22:38\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,409 ms\n",
      "Tokens per second [15.8] input tokens [610] + xml response tokens [38] = total tokens i/o [648]\n",
      "Response: [<response><command>search google new tab</command><args>Recursion Error: Maximum recursion depth exceeded</args></response>]\n",
      "\n",
      "Processing call [424] out of [1000] = [42.4%]... ETA mm:ss 22:36\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,339 ms\n",
      "Tokens per second [15.8] input tokens [605] + xml response tokens [37] = total tokens i/o [642]\n",
      "Response: [<response><command>search phind current tab</command><args>Handling JSON data in Pandas</args></response>]\n",
      "\n",
      "Processing call [425] out of [1000] = [42.5%]... ETA mm:ss 22:34\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,276 ms\n",
      "Tokens per second [15.8] input tokens [605] + xml response tokens [36] = total tokens i/o [641]\n",
      "Response: [<response><command>go to new tab</command><args>dev.remarkableapple.net</args></response>]\n",
      "\n",
      "Processing call [426] out of [1000] = [42.6%]... ETA mm:ss 22:31\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,279 ms\n",
      "Tokens per second [15.8] input tokens [607] + xml response tokens [36] = total tokens i/o [643]\n",
      "Response: [<response><command>search kagi new tab</command><args>AWS SageMaker for ML</args></response>]\n",
      "\n",
      "Processing call [427] out of [1000] = [42.7%]... ETA mm:ss 22:29\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,966 ms\n",
      "Tokens per second [15.8] input tokens [601] + xml response tokens [31] = total tokens i/o [632]\n",
      "Response: [<response><command>search google using clipboard new tab</command><args></args></response>]\n",
      "\n",
      "Processing call [428] out of [1000] = [42.8%]... ETA mm:ss 22:26\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,157 ms\n",
      "Tokens per second [15.8] input tokens [606] + xml response tokens [34] = total tokens i/o [640]\n",
      "Response: [<response><command>search google new tab</command><args>AI in financial forecasting</args></response>]\n",
      "\n",
      "Processing call [429] out of [1000] = [42.9%]... ETA mm:ss 22:23\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,188 ms\n",
      "Tokens per second [16.0] input tokens [371] + xml response tokens [35] = total tokens i/o [406]\n",
      "Response: [<response><command>agent router go to receptionist</command><args>Visitor Coordinator Agent</args></response>]\n",
      "\n",
      "Processing call [430] out of [1000] = [43.0%]... ETA mm:ss 22:21\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 3,155 ms\n",
      "Tokens per second [15.8] input tokens [620] + xml response tokens [50] = total tokens i/o [670]\n",
      "Response: [<response><command>search perplexity current tab</command><args>Hadoop MapReduce tutorial: Where can you find a beginner-friendly tutorial for Hadoop MapReduce?</args></response>]\n",
      "\n",
      "Processing call [431] out of [1000] = [43.1%]... ETA mm:ss 22:19\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,345 ms\n",
      "Tokens per second [15.8] input tokens [606] + xml response tokens [37] = total tokens i/o [643]\n",
      "Response: [<response><command>go to current tab</command><args>alpha.hilariouslemur.gov</args></response>]\n",
      "\n",
      "Processing call [432] out of [1000] = [43.2%]... ETA mm:ss 22:17\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,185 ms\n",
      "Tokens per second [16.0] input tokens [376] + xml response tokens [35] = total tokens i/o [411]\n",
      "Response: [<response><command>agent router go to weather</command><args>Garland, Texas</args></response>]\n",
      "\n",
      "Processing call [433] out of [1000] = [43.3%]... ETA mm:ss 22:14\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,972 ms\n",
      "Tokens per second [15.7] input tokens [603] + xml response tokens [31] = total tokens i/o [634]\n",
      "Response: [<response><command>search google using clipboard current tab</command><args></args></response>]\n",
      "\n",
      "Processing call [434] out of [1000] = [43.4%]... ETA mm:ss 22:12\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,407 ms\n",
      "Tokens per second [15.8] input tokens [605] + xml response tokens [38] = total tokens i/o [643]\n",
      "Response: [<response><command>go to new tab</command><args>beta.remarkablestrawberry.info</args></response>]\n",
      "\n",
      "Processing call [435] out of [1000] = [43.5%]... ETA mm:ss 22:09\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,160 ms\n",
      "Tokens per second [15.7] input tokens [607] + xml response tokens [34] = total tokens i/o [641]\n",
      "Response: [<response><command>search google new tab</command><args>FileNotFoundError</args></response>]\n",
      "\n",
      "Processing call [436] out of [1000] = [43.6%]... ETA mm:ss 22:07\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,593 ms\n",
      "Tokens per second [15.8] input tokens [611] + xml response tokens [41] = total tokens i/o [652]\n",
      "Response: [<response><command>search current tab</command><args>How can system exit errors be handled gracefully in Python applications?</args></response>]\n",
      "\n",
      "Processing call [437] out of [1000] = [43.7%]... ETA mm:ss 22:05\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,341 ms\n",
      "Tokens per second [15.8] input tokens [607] + xml response tokens [37] = total tokens i/o [644]\n",
      "Response: [<response><command>search google scholar new tab</command><args>how to make homemade bread</args></response>]\n",
      "\n",
      "Processing call [438] out of [1000] = [43.8%]... ETA mm:ss 22:02\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,654 ms\n",
      "Tokens per second [15.8] input tokens [615] + xml response tokens [42] = total tokens i/o [657]\n",
      "Response: [<response><command>search google scholar new tab</command><args>How can decoding errors with Unicode characters be resolved in Python?</args></response>]\n",
      "\n",
      "Processing call [439] out of [1000] = [43.9%]... ETA mm:ss 22:00\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,316 ms\n",
      "Tokens per second [16.0] input tokens [370] + xml response tokens [37] = total tokens i/o [407]\n",
      "Response: [<response><command>agent router go to weather</command><args>Ulaanbaatar, Mongolia</args></response>]\n",
      "\n",
      "Processing call [440] out of [1000] = [44.0%]... ETA mm:ss 21:58\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,348 ms\n",
      "Tokens per second [15.8] input tokens [608] + xml response tokens [37] = total tokens i/o [645]\n",
      "Response: [<response><command>search phind new tab</command><args>Reinforcement Learning breakthroughs</args></response>]\n",
      "\n",
      "Processing call [441] out of [1000] = [44.1%]... ETA mm:ss 21:56\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,877 ms\n",
      "Tokens per second [16.0] input tokens [375] + xml response tokens [30] = total tokens i/o [405]\n",
      "Response: [<response><command>agent router go to calendar</command><args></args></response>]\n",
      "\n",
      "Processing call [442] out of [1000] = [44.2%]... ETA mm:ss 21:53\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,128 ms\n",
      "Tokens per second [16.0] input tokens [361] + xml response tokens [34] = total tokens i/o [395]\n",
      "Response: [<response><command>agent router go to receptionist</command><args>Office Coordinator</args></response>]\n",
      "\n",
      "Processing call [443] out of [1000] = [44.3%]... ETA mm:ss 21:50\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,911 ms\n",
      "Tokens per second [15.8] input tokens [611] + xml response tokens [46] = total tokens i/o [657]\n",
      "Response: [<response><command>search google scholar current tab</command><args>How do you address runtime warnings in Python, especially those indicating risky runtime behavior?</args></response>]\n",
      "\n",
      "Processing call [444] out of [1000] = [44.4%]... ETA mm:ss 21:48\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,660 ms\n",
      "Tokens per second [15.7] input tokens [598] + xml response tokens [26] = total tokens i/o [624]\n",
      "Response: [<response><command>none</command><args></args></response>]\n",
      "\n",
      "Processing call [445] out of [1000] = [44.5%]... ETA mm:ss 21:45\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,410 ms\n",
      "Tokens per second [15.8] input tokens [605] + xml response tokens [38] = total tokens i/o [643]\n",
      "Response: [<response><command>go to new tab</command><args>blog.magnificentcherry.io</args></response>]\n",
      "\n",
      "Processing call [446] out of [1000] = [44.6%]... ETA mm:ss 21:43\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,187 ms\n",
      "Tokens per second [16.0] input tokens [369] + xml response tokens [35] = total tokens i/o [404]\n",
      "Response: [<response><command>agent router go to weather</command><args>Oslo, Norway</args></response>]\n",
      "\n",
      "Processing call [447] out of [1000] = [44.7%]... ETA mm:ss 21:40\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,312 ms\n",
      "Tokens per second [16.0] input tokens [379] + xml response tokens [37] = total tokens i/o [416]\n",
      "Response: [<response><command>agent router go to date and time</command><args>New Orleans, USA</args></response>]\n",
      "\n",
      "Processing call [448] out of [1000] = [44.8%]... ETA mm:ss 21:38\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,314 ms\n",
      "Tokens per second [16.0] input tokens [367] + xml response tokens [37] = total tokens i/o [404]\n",
      "Response: [<response><command>agent router go to date and time</command><args>Lexington, Kentucky</args></response>]\n",
      "\n",
      "Processing call [449] out of [1000] = [44.9%]... ETA mm:ss 21:36\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 3,155 ms\n",
      "Tokens per second [15.8] input tokens [621] + xml response tokens [50] = total tokens i/o [671]\n",
      "Response: [<response><command>search google scholar new tab</command><args>Hadoop MapReduce tutorial: Where can you find a beginner-friendly tutorial for Hadoop MapReduce?</args></response>]\n",
      "\n",
      "Processing call [450] out of [1000] = [45.0%]... ETA mm:ss 21:34\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,036 ms\n",
      "Tokens per second [15.7] input tokens [605] + xml response tokens [32] = total tokens i/o [637]\n",
      "Response: [<response><command>search phind using clipboard current tab</command><args></args></response>]\n",
      "\n",
      "Processing call [451] out of [1000] = [45.1%]... ETA mm:ss 21:31\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,472 ms\n",
      "Tokens per second [15.8] input tokens [610] + xml response tokens [39] = total tokens i/o [649]\n",
      "Response: [<response><command>search kagi new tab</command><args>Deprecation Warning: Feature is deprecated</args></response>]\n",
      "\n",
      "Processing call [452] out of [1000] = [45.2%]... ETA mm:ss 21:29\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,187 ms\n",
      "Tokens per second [16.0] input tokens [373] + xml response tokens [35] = total tokens i/o [408]\n",
      "Response: [<response><command>agent router go to receptionist</command><args>Front Desk</args></response>]\n",
      "\n",
      "Processing call [453] out of [1000] = [45.3%]... ETA mm:ss 21:27\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,031 ms\n",
      "Tokens per second [15.8] input tokens [602] + xml response tokens [32] = total tokens i/o [634]\n",
      "Response: [<response><command>search google current tab</command><args>Runtime Error</args></response>]\n",
      "\n",
      "Processing call [454] out of [1000] = [45.4%]... ETA mm:ss 21:24\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 3,034 ms\n",
      "Tokens per second [15.8] input tokens [619] + xml response tokens [48] = total tokens i/o [667]\n",
      "Response: [<response><command>search phind current tab</command><args>How do you address deprecation warnings in Python to ensure code compatibility with future versions?</args></response>]\n",
      "\n",
      "Processing call [455] out of [1000] = [45.5%]... ETA mm:ss 21:22\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,040 ms\n",
      "Tokens per second [15.7] input tokens [602] + xml response tokens [32] = total tokens i/o [634]\n",
      "Response: [<response><command>search current tab</command><args>SQL database management</args></response>]\n",
      "\n",
      "Processing call [456] out of [1000] = [45.6%]... ETA mm:ss 21:20\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,161 ms\n",
      "Tokens per second [15.7] input tokens [606] + xml response tokens [34] = total tokens i/o [640]\n",
      "Response: [<response><command>search phind new tab</command><args>RecursionError</args></response>]\n",
      "\n",
      "Processing call [457] out of [1000] = [45.7%]... ETA mm:ss 21:17\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,131 ms\n",
      "Tokens per second [16.0] input tokens [392] + xml response tokens [34] = total tokens i/o [426]\n",
      "Response: [<response><command>agent router go to receptionist</command><args>Service Agent</args></response>]\n",
      "\n",
      "Processing call [458] out of [1000] = [45.8%]... ETA mm:ss 21:14\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,186 ms\n",
      "Tokens per second [16.0] input tokens [372] + xml response tokens [35] = total tokens i/o [407]\n",
      "Response: [<response><command>agent router go to weather</command><args>New York, USA</args></response>]\n",
      "\n",
      "Processing call [459] out of [1000] = [45.9%]... ETA mm:ss 21:12\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,160 ms\n",
      "Tokens per second [15.7] input tokens [604] + xml response tokens [34] = total tokens i/o [638]\n",
      "Response: [<response><command>search google scholar new tab</command><args>hip hop music</args></response>]\n",
      "\n",
      "Processing call [460] out of [1000] = [46.0%]... ETA mm:ss 21:09\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,344 ms\n",
      "Tokens per second [15.8] input tokens [606] + xml response tokens [37] = total tokens i/o [643]\n",
      "Response: [<response><command>go to new tab</command><args>stage.beautifuliceberg.io</args></response>]\n",
      "\n",
      "Processing call [461] out of [1000] = [46.1%]... ETA mm:ss 21:07\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,437 ms\n",
      "Tokens per second [15.6] input tokens [604] + xml response tokens [38] = total tokens i/o [642]\n",
      "Response: [<response><command>go to new tab</command><args>test.beautifulpenguin.gov</args></response>]\n",
      "\n",
      "Processing call [462] out of [1000] = [46.2%]... ETA mm:ss 21:05\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,284 ms\n",
      "Tokens per second [15.8] input tokens [603] + xml response tokens [36] = total tokens i/o [639]\n",
      "Response: [<response><command>search perplexity current tab</command><args>AI for customer service automation</args></response>]\n",
      "\n",
      "Processing call [463] out of [1000] = [46.3%]... ETA mm:ss 21:02\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,095 ms\n",
      "Tokens per second [15.8] input tokens [602] + xml response tokens [33] = total tokens i/o [635]\n",
      "Response: [<response><command>search new tab</command><args>FileNotFoundError</args></response>]\n",
      "\n",
      "Processing call [464] out of [1000] = [46.4%]... ETA mm:ss 21:00\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,847 ms\n",
      "Tokens per second [15.8] input tokens [614] + xml response tokens [45] = total tokens i/o [659]\n",
      "Response: [<response><command>search kagi new tab</command><args>OpenAI latest projects: What are the latest projects undertaken by OpenAI?</args></response>]\n",
      "\n",
      "Processing call [465] out of [1000] = [46.5%]... ETA mm:ss 20:58\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,034 ms\n",
      "Tokens per second [15.7] input tokens [600] + xml response tokens [32] = total tokens i/o [632]\n",
      "Response: [<response><command>search kagi using clipboard current tab</command><args></args></response>]\n",
      "\n",
      "Processing call [466] out of [1000] = [46.6%]... ETA mm:ss 20:55\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,100 ms\n",
      "Tokens per second [15.7] input tokens [601] + xml response tokens [33] = total tokens i/o [634]\n",
      "Response: [<response><command>search phind current tab</command><args>RuntimeWarning</args></response>]\n",
      "\n",
      "Processing call [467] out of [1000] = [46.7%]... ETA mm:ss 20:52\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,910 ms\n",
      "Tokens per second [15.8] input tokens [614] + xml response tokens [46] = total tokens i/o [660]\n",
      "Response: [<response><command>search current tab</command><args>Normalization vs. Standardization: When should you choose normalization over standardization in data preprocessing?</args></response>]\n",
      "\n",
      "Processing call [468] out of [1000] = [46.8%]... ETA mm:ss 20:51\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,313 ms\n",
      "Tokens per second [16.0] input tokens [372] + xml response tokens [37] = total tokens i/o [409]\n",
      "Response: [<response><command>agent router go to date and time</command><args>Dakar, Senegal</args></response>]\n",
      "\n",
      "Processing call [469] out of [1000] = [46.9%]... ETA mm:ss 20:48\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,657 ms\n",
      "Tokens per second [15.7] input tokens [596] + xml response tokens [26] = total tokens i/o [622]\n",
      "Response: [<response><command>none</command><args></args></response>]\n",
      "\n",
      "Processing call [470] out of [1000] = [47.0%]... ETA mm:ss 20:45\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,282 ms\n",
      "Tokens per second [15.8] input tokens [605] + xml response tokens [36] = total tokens i/o [641]\n",
      "Response: [<response><command>search google new tab</command><args>Reading CSV files in Pandas</args></response>]\n",
      "\n",
      "Processing call [471] out of [1000] = [47.1%]... ETA mm:ss 20:43\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,845 ms\n",
      "Tokens per second [15.8] input tokens [617] + xml response tokens [45] = total tokens i/o [662]\n",
      "Response: [<response><command>search kagi new tab</command><args>OpenAI latest projects: What are the latest projects undertaken by OpenAI?</args></response>]\n",
      "\n",
      "Processing call [472] out of [1000] = [47.2%]... ETA mm:ss 20:41\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,284 ms\n",
      "Tokens per second [15.8] input tokens [603] + xml response tokens [36] = total tokens i/o [639]\n",
      "Response: [<response><command>search google scholar current tab</command><args>AWS SageMaker for ML</args></response>]\n",
      "\n",
      "Processing call [473] out of [1000] = [47.3%]... ETA mm:ss 20:39\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,344 ms\n",
      "Tokens per second [15.8] input tokens [607] + xml response tokens [37] = total tokens i/o [644]\n",
      "Response: [<response><command>search kagi new tab</command><args>Working with datetime in Pandas</args></response>]\n",
      "\n",
      "Processing call [474] out of [1000] = [47.4%]... ETA mm:ss 20:36\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,905 ms\n",
      "Tokens per second [15.7] input tokens [598] + xml response tokens [30] = total tokens i/o [628]\n",
      "Response: [<response><command>search current tab</command><args>Warning</args></response>]\n",
      "\n",
      "Processing call [475] out of [1000] = [47.5%]... ETA mm:ss 20:33\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,342 ms\n",
      "Tokens per second [15.8] input tokens [603] + xml response tokens [37] = total tokens i/o [640]\n",
      "Response: [<response><command>go to new tab</command><args>incredibledolphin.info</args></response>]\n",
      "\n",
      "Processing call [476] out of [1000] = [47.6%]... ETA mm:ss 20:31\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,344 ms\n",
      "Tokens per second [15.8] input tokens [607] + xml response tokens [37] = total tokens i/o [644]\n",
      "Response: [<response><command>search perplexity current tab</command><args>how to make homemade bread</args></response>]\n",
      "\n",
      "Processing call [477] out of [1000] = [47.7%]... ETA mm:ss 20:29\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,284 ms\n",
      "Tokens per second [15.8] input tokens [603] + xml response tokens [36] = total tokens i/o [639]\n",
      "Response: [<response><command>search phind current tab</command><args>Pandas DataFrame creation</args></response>]\n",
      "\n",
      "Processing call [478] out of [1000] = [47.8%]... ETA mm:ss 20:26\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,190 ms\n",
      "Tokens per second [16.0] input tokens [384] + xml response tokens [35] = total tokens i/o [419]\n",
      "Response: [<response><command>agent router go to receptionist</command><args>Secretary Agent</args></response>]\n",
      "\n",
      "Processing call [479] out of [1000] = [47.9%]... ETA mm:ss 20:24\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,216 ms\n",
      "Tokens per second [15.8] input tokens [608] + xml response tokens [35] = total tokens i/o [643]\n",
      "Response: [<response><command>search google scholar new tab</command><args>IBM Watson AI tools</args></response>]\n",
      "\n",
      "Processing call [480] out of [1000] = [48.0%]... ETA mm:ss 20:21\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,718 ms\n",
      "Tokens per second [15.8] input tokens [615] + xml response tokens [43] = total tokens i/o [658]\n",
      "Response: [<response><command>search google scholar new tab</command><args>How do you fix indentation errors that affect code structure in Python?</args></response>]\n",
      "\n",
      "Processing call [481] out of [1000] = [48.1%]... ETA mm:ss 20:19\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,404 ms\n",
      "Tokens per second [15.8] input tokens [604] + xml response tokens [38] = total tokens i/o [642]\n",
      "Response: [<response><command>search google scholar current tab</command><args>Type Error: Incorrect type comparison</args></response>]\n",
      "\n",
      "Processing call [482] out of [1000] = [48.2%]... ETA mm:ss 20:17\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,187 ms\n",
      "Tokens per second [16.0] input tokens [378] + xml response tokens [35] = total tokens i/o [413]\n",
      "Response: [<response><command>agent router go to weather</command><args>Boise, Idaho</args></response>]\n",
      "\n",
      "Processing call [483] out of [1000] = [48.3%]... ETA mm:ss 20:14\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,158 ms\n",
      "Tokens per second [15.8] input tokens [601] + xml response tokens [34] = total tokens i/o [635]\n",
      "Response: [<response><command>search google new tab</command><args>how to improve memory</args></response>]\n",
      "\n",
      "Processing call [484] out of [1000] = [48.4%]... ETA mm:ss 20:12\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,406 ms\n",
      "Tokens per second [15.8] input tokens [607] + xml response tokens [38] = total tokens i/o [645]\n",
      "Response: [<response><command>search google current tab</command><args>Setting and resetting index in Pandas</args></response>]\n",
      "\n",
      "Processing call [485] out of [1000] = [48.5%]... ETA mm:ss 20:10\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,309 ms\n",
      "Tokens per second [16.0] input tokens [376] + xml response tokens [37] = total tokens i/o [413]\n",
      "Response: [<response><command>agent router go to date and time</command><args>Anaheim, California</args></response>]\n",
      "\n",
      "Processing call [486] out of [1000] = [48.6%]... ETA mm:ss 20:07\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,905 ms\n",
      "Tokens per second [15.8] input tokens [616] + xml response tokens [46] = total tokens i/o [662]\n",
      "Response: [<response><command>search perplexity current tab</command><args>How do you address runtime warnings in Python, especially those indicating risky runtime behavior?</args></response>]\n",
      "\n",
      "Processing call [487] out of [1000] = [48.7%]... ETA mm:ss 20:05\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,220 ms\n",
      "Tokens per second [15.8] input tokens [603] + xml response tokens [35] = total tokens i/o [638]\n",
      "Response: [<response><command>search current tab</command><args>Cross-tabulation in Pandas</args></response>]\n",
      "\n",
      "Processing call [488] out of [1000] = [48.8%]... ETA mm:ss 20:03\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,881 ms\n",
      "Tokens per second [15.9] input tokens [379] + xml response tokens [30] = total tokens i/o [409]\n",
      "Response: [<response><command>agent router go to calendar</command><args></args></response>]\n",
      "\n",
      "Processing call [489] out of [1000] = [48.9%]... ETA mm:ss 20:00\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,878 ms\n",
      "Tokens per second [16.0] input tokens [376] + xml response tokens [30] = total tokens i/o [406]\n",
      "Response: [<response><command>agent router go to math</command><args></args></response>]\n",
      "\n",
      "Processing call [490] out of [1000] = [49.0%]... ETA mm:ss 19:57\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,342 ms\n",
      "Tokens per second [15.8] input tokens [604] + xml response tokens [37] = total tokens i/o [641]\n",
      "Response: [<response><command>search google current tab</command><args>Type Error: Incorrect type comparison</args></response>]\n",
      "\n",
      "Processing call [491] out of [1000] = [49.1%]... ETA mm:ss 19:55\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,905 ms\n",
      "Tokens per second [15.8] input tokens [615] + xml response tokens [46] = total tokens i/o [661]\n",
      "Response: [<response><command>search kagi current tab</command><args>AI for health diagnostics: How does AI contribute to advancements in health diagnostics?</args></response>]\n",
      "\n",
      "Processing call [492] out of [1000] = [49.2%]... ETA mm:ss 19:53\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,343 ms\n",
      "Tokens per second [15.8] input tokens [606] + xml response tokens [37] = total tokens i/o [643]\n",
      "Response: [<response><command>search perplexity current tab</command><args>Reinforcement Learning breakthroughs</args></response>]\n",
      "\n",
      "Processing call [493] out of [1000] = [49.3%]... ETA mm:ss 19:51\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,220 ms\n",
      "Tokens per second [15.8] input tokens [600] + xml response tokens [35] = total tokens i/o [635]\n",
      "Response: [<response><command>search phind current tab</command><args>AI in financial forecasting</args></response>]\n",
      "\n",
      "Processing call [494] out of [1000] = [49.4%]... ETA mm:ss 19:48\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,286 ms\n",
      "Tokens per second [15.7] input tokens [602] + xml response tokens [36] = total tokens i/o [638]\n",
      "Response: [<response><command>search google scholar current tab</command><args>AI in content moderation</args></response>]\n",
      "\n",
      "Processing call [495] out of [1000] = [49.5%]... ETA mm:ss 19:46\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,843 ms\n",
      "Tokens per second [15.8] input tokens [613] + xml response tokens [45] = total tokens i/o [658]\n",
      "Response: [<response><command>search phind current tab</command><args>What are the best practices to manage and prevent memory errors in Python applications?</args></response>]\n",
      "\n",
      "Processing call [496] out of [1000] = [49.6%]... ETA mm:ss 19:44\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,844 ms\n",
      "Tokens per second [15.8] input tokens [615] + xml response tokens [45] = total tokens i/o [660]\n",
      "Response: [<response><command>search google new tab</command><args>Machine Learning in biotech: How is machine learning applied in biotechnology research?</args></response>]\n",
      "\n",
      "Processing call [497] out of [1000] = [49.7%]... ETA mm:ss 19:42\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 3,097 ms\n",
      "Tokens per second [15.8] input tokens [621] + xml response tokens [49] = total tokens i/o [670]\n",
      "Response: [<response><command>search google new tab</command><args>Convolutional Neural Networks: What are the primary applications of convolutional neural networks in image processing?</args></response>]\n",
      "\n",
      "Processing call [498] out of [1000] = [49.8%]... ETA mm:ss 19:41\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,031 ms\n",
      "Tokens per second [15.8] input tokens [601] + xml response tokens [32] = total tokens i/o [633]\n",
      "Response: [<response><command>search phind using clipboard new tab</command><args></args></response>]\n",
      "\n",
      "Processing call [499] out of [1000] = [49.9%]... ETA mm:ss 19:38\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,350 ms\n",
      "Tokens per second [15.7] input tokens [604] + xml response tokens [37] = total tokens i/o [641]\n",
      "Response: [<response><command>go to current tab</command><args>www.hilariousiceberg.org</args></response>]\n",
      "\n",
      "Processing call [500] out of [1000] = [50.0%]... ETA mm:ss 19:36\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,160 ms\n",
      "Tokens per second [15.7] input tokens [599] + xml response tokens [34] = total tokens i/o [633]\n",
      "Response: [<response><command>search google scholar current tab</command><args>best online games</args></response>]\n",
      "\n",
      "Processing call [501] out of [1000] = [50.1%]... ETA mm:ss 19:33\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,281 ms\n",
      "Tokens per second [15.8] input tokens [604] + xml response tokens [36] = total tokens i/o [640]\n",
      "Response: [<response><command>search current tab</command><args>Type Error: Incorrect type comparison</args></response>]\n",
      "\n",
      "Processing call [502] out of [1000] = [50.2%]... ETA mm:ss 19:31\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 3,031 ms\n",
      "Tokens per second [15.8] input tokens [621] + xml response tokens [48] = total tokens i/o [669]\n",
      "Response: [<response><command>search phind new tab</command><args>Sentiment Analysis tools: What tools are widely used for sentiment analysis in social media data?</args></response>]\n",
      "\n",
      "Processing call [503] out of [1000] = [50.3%]... ETA mm:ss 19:29\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,220 ms\n",
      "Tokens per second [15.8] input tokens [608] + xml response tokens [35] = total tokens i/o [643]\n",
      "Response: [<response><command>search perplexity new tab</command><args>how to improve memory</args></response>]\n",
      "\n",
      "Processing call [504] out of [1000] = [50.4%]... ETA mm:ss 19:26\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,242 ms\n",
      "Tokens per second [15.6] input tokens [365] + xml response tokens [35] = total tokens i/o [400]\n",
      "Response: [<response><command>agent router go to receptionist</command><args>Desk Clerk</args></response>]\n",
      "\n",
      "Processing call [505] out of [1000] = [50.5%]... ETA mm:ss 19:24\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,157 ms\n",
      "Tokens per second [15.8] input tokens [610] + xml response tokens [34] = total tokens i/o [644]\n",
      "Response: [<response><command>search perplexity new tab</command><args>SQL database management</args></response>]\n",
      "\n",
      "Processing call [506] out of [1000] = [50.6%]... ETA mm:ss 19:21\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,342 ms\n",
      "Tokens per second [15.8] input tokens [606] + xml response tokens [37] = total tokens i/o [643]\n",
      "Response: [<response><command>go to new tab</command><args>beta.spectacularbanana.org</args></response>]\n",
      "\n",
      "Processing call [507] out of [1000] = [50.7%]... ETA mm:ss 19:19\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,032 ms\n",
      "Tokens per second [15.7] input tokens [603] + xml response tokens [32] = total tokens i/o [635]\n",
      "Response: [<response><command>search perplexity new tab</command><args>Warning</args></response>]\n",
      "\n",
      "Processing call [508] out of [1000] = [50.8%]... ETA mm:ss 19:16\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,346 ms\n",
      "Tokens per second [15.8] input tokens [605] + xml response tokens [37] = total tokens i/o [642]\n",
      "Response: [<response><command>go to new tab</command><args>test.fantasticrainbow.gov</args></response>]\n",
      "\n",
      "Processing call [509] out of [1000] = [50.9%]... ETA mm:ss 19:14\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,892 ms\n",
      "Tokens per second [15.9] input tokens [406] + xml response tokens [30] = total tokens i/o [436]\n",
      "Response: [<response><command>agent router go to calendar</command><args></args></response>]\n",
      "\n",
      "Processing call [510] out of [1000] = [51.0%]... ETA mm:ss 19:11\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,289 ms\n",
      "Tokens per second [15.7] input tokens [606] + xml response tokens [36] = total tokens i/o [642]\n",
      "Response: [<response><command>search google scholar new tab</command><args>AWS SageMaker for ML</args></response>]\n",
      "\n",
      "Processing call [511] out of [1000] = [51.1%]... ETA mm:ss 19:09\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,253 ms\n",
      "Tokens per second [16.0] input tokens [371] + xml response tokens [36] = total tokens i/o [407]\n",
      "Response: [<response><command>agent router go to receptionist</command><args>Switchboard Operator Agent</args></response>]\n",
      "\n",
      "Processing call [512] out of [1000] = [51.2%]... ETA mm:ss 19:06\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,840 ms\n",
      "Tokens per second [15.8] input tokens [613] + xml response tokens [45] = total tokens i/o [658]\n",
      "Response: [<response><command>search google current tab</command><args>Machine Learning in biotech: How is machine learning applied in biotechnology research?</args></response>]\n",
      "\n",
      "Processing call [513] out of [1000] = [51.3%]... ETA mm:ss 19:05\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,093 ms\n",
      "Tokens per second [15.8] input tokens [601] + xml response tokens [33] = total tokens i/o [634]\n",
      "Response: [<response><command>search kagi new tab</command><args>Overflow Error</args></response>]\n",
      "\n",
      "Processing call [514] out of [1000] = [51.4%]... ETA mm:ss 19:02\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,189 ms\n",
      "Tokens per second [16.0] input tokens [380] + xml response tokens [35] = total tokens i/o [415]\n",
      "Response: [<response><command>agent router go to receptionist</command><args>Switchboard Agent</args></response>]\n",
      "\n",
      "Processing call [515] out of [1000] = [51.5%]... ETA mm:ss 18:59\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,339 ms\n",
      "Tokens per second [15.8] input tokens [604] + xml response tokens [37] = total tokens i/o [641]\n",
      "Response: [<response><command>go to current tab</command><args>beta.spectacularvolcano.gov</args></response>]\n",
      "\n",
      "Processing call [516] out of [1000] = [51.6%]... ETA mm:ss 18:57\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,977 ms\n",
      "Tokens per second [15.8] input tokens [614] + xml response tokens [47] = total tokens i/o [661]\n",
      "Response: [<response><command>search google new tab</command><args>Neural Network hyperparameter tuning: What approaches are recommended for neural network hyperparameter tuning?</args></response>]\n",
      "\n",
      "Processing call [517] out of [1000] = [51.7%]... ETA mm:ss 18:55\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,033 ms\n",
      "Tokens per second [15.7] input tokens [603] + xml response tokens [32] = total tokens i/o [635]\n",
      "Response: [<response><command>search current tab</command><args>best online games</args></response>]\n",
      "\n",
      "Processing call [518] out of [1000] = [51.8%]... ETA mm:ss 18:53\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,158 ms\n",
      "Tokens per second [15.8] input tokens [602] + xml response tokens [34] = total tokens i/o [636]\n",
      "Response: [<response><command>search perplexity current tab</command><args>best online games</args></response>]\n",
      "\n",
      "Processing call [519] out of [1000] = [51.9%]... ETA mm:ss 18:50\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,281 ms\n",
      "Tokens per second [15.8] input tokens [606] + xml response tokens [36] = total tokens i/o [642]\n",
      "Response: [<response><command>search kagi new tab</command><args>Pandas DataFrame creation</args></response>]\n",
      "\n",
      "Processing call [520] out of [1000] = [52.0%]... ETA mm:ss 18:48\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,718 ms\n",
      "Tokens per second [15.8] input tokens [610] + xml response tokens [43] = total tokens i/o [653]\n",
      "Response: [<response><command>search perplexity current tab</command><args>How do you fix indentation errors that affect code structure in Python?</args></response>]\n",
      "\n",
      "Processing call [521] out of [1000] = [52.1%]... ETA mm:ss 18:46\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,345 ms\n",
      "Tokens per second [15.8] input tokens [606] + xml response tokens [37] = total tokens i/o [643]\n",
      "Response: [<response><command>search perplexity current tab</command><args>User Warning: User-defined warning</args></response>]\n",
      "\n",
      "Processing call [522] out of [1000] = [52.2%]... ETA mm:ss 18:43\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,035 ms\n",
      "Tokens per second [15.7] input tokens [605] + xml response tokens [32] = total tokens i/o [637]\n",
      "Response: [<response><command>search google scholar using clipboard current tab</command><args></args></response>]\n",
      "\n",
      "Processing call [523] out of [1000] = [52.3%]... ETA mm:ss 18:41\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,875 ms\n",
      "Tokens per second [16.0] input tokens [368] + xml response tokens [30] = total tokens i/o [398]\n",
      "Response: [<response><command>agent router go to calendar</command><args></args></response>]\n",
      "\n",
      "Processing call [524] out of [1000] = [52.4%]... ETA mm:ss 18:38\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,403 ms\n",
      "Tokens per second [15.8] input tokens [605] + xml response tokens [38] = total tokens i/o [643]\n",
      "Response: [<response><command>go to new tab</command><args>prod.remarkablepenguin.com</args></response>]\n",
      "\n",
      "Processing call [525] out of [1000] = [52.5%]... ETA mm:ss 18:36\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,279 ms\n",
      "Tokens per second [15.8] input tokens [603] + xml response tokens [36] = total tokens i/o [639]\n",
      "Response: [<response><command>search current tab</command><args>Saving Pandas DataFrame to CSV</args></response>]\n",
      "\n",
      "Processing call [526] out of [1000] = [52.6%]... ETA mm:ss 18:33\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,408 ms\n",
      "Tokens per second [15.8] input tokens [606] + xml response tokens [38] = total tokens i/o [644]\n",
      "Response: [<response><command>go to new tab</command><args>blog.fantastictornado.info</args></response>]\n",
      "\n",
      "Processing call [527] out of [1000] = [52.7%]... ETA mm:ss 18:31\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,406 ms\n",
      "Tokens per second [15.8] input tokens [609] + xml response tokens [38] = total tokens i/o [647]\n",
      "Response: [<response><command>search kagi new tab</command><args>Saving Pandas DataFrame to CSV</args></response>]\n",
      "\n",
      "Processing call [528] out of [1000] = [52.8%]... ETA mm:ss 18:29\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,409 ms\n",
      "Tokens per second [15.8] input tokens [611] + xml response tokens [38] = total tokens i/o [649]\n",
      "Response: [<response><command>search phind new tab</command><args>Using Pandas for ETL processes</args></response>]\n",
      "\n",
      "Processing call [529] out of [1000] = [52.9%]... ETA mm:ss 18:26\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,344 ms\n",
      "Tokens per second [15.8] input tokens [605] + xml response tokens [37] = total tokens i/o [642]\n",
      "Response: [<response><command>search current tab</command><args>Deprecation Warning: Feature is deprecated</args></response>]\n",
      "\n",
      "Processing call [530] out of [1000] = [53.0%]... ETA mm:ss 18:24\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,722 ms\n",
      "Tokens per second [15.8] input tokens [612] + xml response tokens [43] = total tokens i/o [655]\n",
      "Response: [<response><command>search google scholar current tab</command><args>How do you fix indentation errors that affect code structure in Python?</args></response>]\n",
      "\n",
      "Processing call [531] out of [1000] = [53.1%]... ETA mm:ss 18:22\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,727 ms\n",
      "Tokens per second [15.8] input tokens [614] + xml response tokens [43] = total tokens i/o [657]\n",
      "Response: [<response><command>search phind current tab</command><args>How do you fix indentation errors that affect code structure in Python?</args></response>]\n",
      "\n",
      "Processing call [532] out of [1000] = [53.2%]... ETA mm:ss 18:20\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,040 ms\n",
      "Tokens per second [15.7] input tokens [600] + xml response tokens [32] = total tokens i/o [632]\n",
      "Response: [<response><command>search google scholar using clipboard new tab</command><args></args></response>]\n",
      "\n",
      "Processing call [533] out of [1000] = [53.3%]... ETA mm:ss 18:17\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,190 ms\n",
      "Tokens per second [16.0] input tokens [375] + xml response tokens [35] = total tokens i/o [410]\n",
      "Response: [<response><command>agent router go to weather</command><args>Richmond, Virginia</args></response>]\n",
      "\n",
      "Processing call [534] out of [1000] = [53.4%]... ETA mm:ss 18:15\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,940 ms\n",
      "Tokens per second [16.0] input tokens [373] + xml response tokens [31] = total tokens i/o [404]\n",
      "Response: [<response><command>agent router go to todo list</command><args></args></response>]\n",
      "\n",
      "Processing call [535] out of [1000] = [53.5%]... ETA mm:ss 18:12\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,348 ms\n",
      "Tokens per second [15.8] input tokens [607] + xml response tokens [37] = total tokens i/o [644]\n",
      "Response: [<response><command>search google scholar new tab</command><args>Reading Excel files with Pandas</args></response>]\n",
      "\n",
      "Processing call [536] out of [1000] = [53.6%]... ETA mm:ss 18:10\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,220 ms\n",
      "Tokens per second [15.8] input tokens [605] + xml response tokens [35] = total tokens i/o [640]\n",
      "Response: [<response><command>search phind current tab</command><args>IBM Watson AI tools</args></response>]\n",
      "\n",
      "Processing call [537] out of [1000] = [53.7%]... ETA mm:ss 18:07\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 3,152 ms\n",
      "Tokens per second [15.9] input tokens [621] + xml response tokens [50] = total tokens i/o [671]\n",
      "Response: [<response><command>search perplexity current tab</command><args>TensorFlow latest updates: What are the latest features added to TensorFlow in its most recent update?</args></response>]\n",
      "\n",
      "Processing call [538] out of [1000] = [53.8%]... ETA mm:ss 18:06\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,360 ms\n",
      "Tokens per second [15.7] input tokens [375] + xml response tokens [37] = total tokens i/o [412]\n",
      "Response: [<response><command>agent router go to date and time</command><args>New Orleans, Louisiana</args></response>]\n",
      "\n",
      "Processing call [539] out of [1000] = [53.9%]... ETA mm:ss 18:03\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,346 ms\n",
      "Tokens per second [15.8] input tokens [603] + xml response tokens [37] = total tokens i/o [640]\n",
      "Response: [<response><command>go to current tab</command><args>dev.jubilantquartz.com</args></response>]\n",
      "\n",
      "Processing call [540] out of [1000] = [54.0%]... ETA mm:ss 18:01\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,906 ms\n",
      "Tokens per second [15.8] input tokens [615] + xml response tokens [46] = total tokens i/o [661]\n",
      "Response: [<response><command>search kagi current tab</command><args>Quantum AI computing developments: What are the latest developments in quantum AI computing?</args></response>]\n",
      "\n",
      "Processing call [541] out of [1000] = [54.1%]... ETA mm:ss 17:59\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,186 ms\n",
      "Tokens per second [16.0] input tokens [375] + xml response tokens [35] = total tokens i/o [410]\n",
      "Response: [<response><command>agent router go to receptionist</command><args>Customer Service Agent</args></response>]\n",
      "\n",
      "Processing call [542] out of [1000] = [54.2%]... ETA mm:ss 17:57\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,375 ms\n",
      "Tokens per second [16.0] input tokens [370] + xml response tokens [38] = total tokens i/o [408]\n",
      "Response: [<response><command>agent router go to date and time</command><args>Laredo, Texas</args></response>]\n",
      "\n",
      "Processing call [543] out of [1000] = [54.3%]... ETA mm:ss 17:54\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,880 ms\n",
      "Tokens per second [16.0] input tokens [370] + xml response tokens [30] = total tokens i/o [400]\n",
      "Response: [<response><command>agent router go to calendar</command><args></args></response>]\n",
      "\n",
      "Processing call [544] out of [1000] = [54.4%]... ETA mm:ss 17:52\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,882 ms\n",
      "Tokens per second [15.9] input tokens [397] + xml response tokens [30] = total tokens i/o [427]\n",
      "Response: [<response><command>agent router go to calendar</command><args></args></response>]\n",
      "\n",
      "Processing call [545] out of [1000] = [54.5%]... ETA mm:ss 17:49\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 3,031 ms\n",
      "Tokens per second [15.8] input tokens [617] + xml response tokens [48] = total tokens i/o [665]\n",
      "Response: [<response><command>search google new tab</command><args>What are the causes of translation errors with Unicode characters in Python and how can they be fixed?</args></response>]\n",
      "\n",
      "Processing call [546] out of [1000] = [54.6%]... ETA mm:ss 17:47\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,221 ms\n",
      "Tokens per second [15.8] input tokens [603] + xml response tokens [35] = total tokens i/o [638]\n",
      "Response: [<response><command>search phind current tab</command><args>URLError</args></response>]\n",
      "\n",
      "Processing call [547] out of [1000] = [54.7%]... ETA mm:ss 17:45\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,875 ms\n",
      "Tokens per second [16.0] input tokens [376] + xml response tokens [30] = total tokens i/o [406]\n",
      "Response: [<response><command>agent router go to calendar</command><args></args></response>]\n",
      "\n",
      "Processing call [548] out of [1000] = [54.8%]... ETA mm:ss 17:42\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,840 ms\n",
      "Tokens per second [15.8] input tokens [615] + xml response tokens [45] = total tokens i/o [660]\n",
      "Response: [<response><command>search google scholar new tab</command><args>Voice Recognition technologies: What are the latest advancements in voice recognition technology?</args></response>]\n",
      "\n",
      "Processing call [549] out of [1000] = [54.9%]... ETA mm:ss 17:40\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,344 ms\n",
      "Tokens per second [15.8] input tokens [603] + xml response tokens [37] = total tokens i/o [640]\n",
      "Response: [<response><command>go to current tab</command><args>www.hilariousiceberg.org</args></response>]\n",
      "\n",
      "Processing call [550] out of [1000] = [55.0%]... ETA mm:ss 17:37\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,345 ms\n",
      "Tokens per second [15.8] input tokens [605] + xml response tokens [37] = total tokens i/o [642]\n",
      "Response: [<response><command>search perplexity current tab</command><args>Working with datetime in Pandas</args></response>]\n",
      "\n",
      "Processing call [551] out of [1000] = [55.1%]... ETA mm:ss 17:35\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,219 ms\n",
      "Tokens per second [15.8] input tokens [604] + xml response tokens [35] = total tokens i/o [639]\n",
      "Response: [<response><command>search google scholar new tab</command><args>FileNotFoundError</args></response>]\n",
      "\n",
      "Processing call [552] out of [1000] = [55.2%]... ETA mm:ss 17:33\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,030 ms\n",
      "Tokens per second [15.8] input tokens [601] + xml response tokens [32] = total tokens i/o [633]\n",
      "Response: [<response><command>search phind using clipboard new tab</command><args></args></response>]\n",
      "\n",
      "Processing call [553] out of [1000] = [55.3%]... ETA mm:ss 17:30\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 3,030 ms\n",
      "Tokens per second [15.8] input tokens [617] + xml response tokens [48] = total tokens i/o [665]\n",
      "Response: [<response><command>search google scholar new tab</command><args>Neural Network hyperparameter tuning: What approaches are recommended for neural network hyperparameter tuning?</args></response>]\n",
      "\n",
      "Processing call [554] out of [1000] = [55.4%]... ETA mm:ss 17:28\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,223 ms\n",
      "Tokens per second [15.7] input tokens [603] + xml response tokens [35] = total tokens i/o [638]\n",
      "Response: [<response><command>search new tab</command><args>Data imputation with Pandas</args></response>]\n",
      "\n",
      "Processing call [555] out of [1000] = [55.5%]... ETA mm:ss 17:26\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,284 ms\n",
      "Tokens per second [15.8] input tokens [608] + xml response tokens [36] = total tokens i/o [644]\n",
      "Response: [<response><command>search perplexity new tab</command><args>AI for customer service automation</args></response>]\n",
      "\n",
      "Processing call [556] out of [1000] = [55.6%]... ETA mm:ss 17:23\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,220 ms\n",
      "Tokens per second [15.8] input tokens [604] + xml response tokens [35] = total tokens i/o [639]\n",
      "Response: [<response><command>search kagi current tab</command><args>IBM Watson AI tools</args></response>]\n",
      "\n",
      "Processing call [557] out of [1000] = [55.7%]... ETA mm:ss 17:21\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,341 ms\n",
      "Tokens per second [15.8] input tokens [605] + xml response tokens [37] = total tokens i/o [642]\n",
      "Response: [<response><command>search kagi current tab</command><args>how to make homemade bread</args></response>]\n",
      "\n",
      "Processing call [558] out of [1000] = [55.8%]... ETA mm:ss 17:19\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,280 ms\n",
      "Tokens per second [15.8] input tokens [607] + xml response tokens [36] = total tokens i/o [643]\n",
      "Response: [<response><command>search new tab</command><args>Saving Pandas DataFrame to CSV</args></response>]\n",
      "\n",
      "Processing call [559] out of [1000] = [55.9%]... ETA mm:ss 17:16\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,313 ms\n",
      "Tokens per second [16.0] input tokens [367] + xml response tokens [37] = total tokens i/o [404]\n",
      "Response: [<response><command>agent router go to date and time</command><args>Portland, Oregon</args></response>]\n",
      "\n",
      "Processing call [560] out of [1000] = [56.0%]... ETA mm:ss 17:14\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 3,036 ms\n",
      "Tokens per second [15.8] input tokens [620] + xml response tokens [48] = total tokens i/o [668]\n",
      "Response: [<response><command>search kagi new tab</command><args>ROC Curve analysis: How is ROC curve analysis utilized to evaluate the performance of binary classifiers?</args></response>]\n",
      "\n",
      "Processing call [561] out of [1000] = [56.1%]... ETA mm:ss 17:12\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,030 ms\n",
      "Tokens per second [15.8] input tokens [599] + xml response tokens [32] = total tokens i/o [631]\n",
      "Response: [<response><command>search google scholar using clipboard current tab</command><args></args></response>]\n",
      "\n",
      "Processing call [562] out of [1000] = [56.2%]... ETA mm:ss 17:09\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,875 ms\n",
      "Tokens per second [16.0] input tokens [373] + xml response tokens [30] = total tokens i/o [403]\n",
      "Response: [<response><command>agent router go to math</command><args></args></response>]\n",
      "\n",
      "Processing call [563] out of [1000] = [56.3%]... ETA mm:ss 17:07\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,719 ms\n",
      "Tokens per second [15.8] input tokens [613] + xml response tokens [43] = total tokens i/o [656]\n",
      "Response: [<response><command>search perplexity current tab</command><args>How can you resolve errors related to ZIP file operations in Python?</args></response>]\n",
      "\n",
      "Processing call [564] out of [1000] = [56.4%]... ETA mm:ss 17:05\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,284 ms\n",
      "Tokens per second [15.8] input tokens [601] + xml response tokens [36] = total tokens i/o [637]\n",
      "Response: [<response><command>go to current tab</command><args>wonderfulcherry.io</args></response>]\n",
      "\n",
      "Processing call [565] out of [1000] = [56.5%]... ETA mm:ss 17:02\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,221 ms\n",
      "Tokens per second [15.8] input tokens [605] + xml response tokens [35] = total tokens i/o [640]\n",
      "Response: [<response><command>search google scholar new tab</command><args>AI in financial forecasting</args></response>]\n",
      "\n",
      "Processing call [566] out of [1000] = [56.6%]... ETA mm:ss 17:00\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,254 ms\n",
      "Tokens per second [16.0] input tokens [373] + xml response tokens [36] = total tokens i/o [409]\n",
      "Response: [<response><command>agent router go to date and time</command><args>Rome, Italy</args></response>]\n",
      "\n",
      "Processing call [567] out of [1000] = [56.7%]... ETA mm:ss 16:57\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,278 ms\n",
      "Tokens per second [15.8] input tokens [603] + xml response tokens [36] = total tokens i/o [639]\n",
      "Response: [<response><command>search google current tab</command><args>Reading Excel files with Pandas</args></response>]\n",
      "\n",
      "Processing call [568] out of [1000] = [56.8%]... ETA mm:ss 16:55\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,248 ms\n",
      "Tokens per second [16.0] input tokens [383] + xml response tokens [36] = total tokens i/o [419]\n",
      "Response: [<response><command>agent router go to receptionist</command><args>Help Desk Agent</args></response>]\n",
      "\n",
      "Processing call [569] out of [1000] = [56.9%]... ETA mm:ss 16:52\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,092 ms\n",
      "Tokens per second [15.8] input tokens [603] + xml response tokens [33] = total tokens i/o [636]\n",
      "Response: [<response><command>search perplexity current tab</command><args>Memory Error</args></response>]\n",
      "\n",
      "Processing call [570] out of [1000] = [57.0%]... ETA mm:ss 16:50\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,343 ms\n",
      "Tokens per second [15.8] input tokens [607] + xml response tokens [37] = total tokens i/o [644]\n",
      "Response: [<response><command>go to current tab</command><args>mail.wonderfulvolcano.net</args></response>]\n",
      "\n",
      "Processing call [571] out of [1000] = [57.1%]... ETA mm:ss 16:48\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,346 ms\n",
      "Tokens per second [15.8] input tokens [606] + xml response tokens [37] = total tokens i/o [643]\n",
      "Response: [<response><command>go to new tab</command><args>dev.beautifulunicorn.com</args></response>]\n",
      "\n",
      "Processing call [572] out of [1000] = [57.2%]... ETA mm:ss 16:45\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 3,158 ms\n",
      "Tokens per second [15.8] input tokens [622] + xml response tokens [50] = total tokens i/o [672]\n",
      "Response: [<response><command>search perplexity new tab</command><args>Convolutional Neural Networks: What are the primary applications of convolutional neural networks in image processing?</args></response>]\n",
      "\n",
      "Processing call [573] out of [1000] = [57.3%]... ETA mm:ss 16:44\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,780 ms\n",
      "Tokens per second [15.8] input tokens [616] + xml response tokens [44] = total tokens i/o [660]\n",
      "Response: [<response><command>search google new tab</command><args>What are common causes and solutions for errors related to incorrect syntax in Python?</args></response>]\n",
      "\n",
      "Processing call [574] out of [1000] = [57.4%]... ETA mm:ss 16:41\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,282 ms\n",
      "Tokens per second [15.8] input tokens [607] + xml response tokens [36] = total tokens i/o [643]\n",
      "Response: [<response><command>search kagi new tab</command><args>Pandas DataFrame creation</args></response>]\n",
      "\n",
      "Processing call [575] out of [1000] = [57.5%]... ETA mm:ss 16:39\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,848 ms\n",
      "Tokens per second [15.8] input tokens [617] + xml response tokens [45] = total tokens i/o [662]\n",
      "Response: [<response><command>search kagi new tab</command><args>What are the best practices to manage and prevent memory errors in Python applications?</args></response>]\n",
      "\n",
      "Processing call [576] out of [1000] = [57.6%]... ETA mm:ss 16:37\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,288 ms\n",
      "Tokens per second [15.7] input tokens [605] + xml response tokens [36] = total tokens i/o [641]\n",
      "Response: [<response><command>search kagi new tab</command><args>Pending Deprecation Warning</args></response>]\n",
      "\n",
      "Processing call [577] out of [1000] = [57.7%]... ETA mm:ss 16:35\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,313 ms\n",
      "Tokens per second [16.0] input tokens [371] + xml response tokens [37] = total tokens i/o [408]\n",
      "Response: [<response><command>agent router go to date and time</command><args>Anaheim, California</args></response>]\n",
      "\n",
      "Processing call [578] out of [1000] = [57.8%]... ETA mm:ss 16:32\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,717 ms\n",
      "Tokens per second [15.8] input tokens [613] + xml response tokens [43] = total tokens i/o [656]\n",
      "Response: [<response><command>search google new tab</command><args>Deep Learning optimization: What strategies are used for optimizing deep learning models?</args></response>]\n",
      "\n",
      "Processing call [579] out of [1000] = [57.9%]... ETA mm:ss 16:30\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,248 ms\n",
      "Tokens per second [16.0] input tokens [369] + xml response tokens [36] = total tokens i/o [405]\n",
      "Response: [<response><command>agent router go to weather</command><args>Prague, Czech Republic</args></response>]\n",
      "\n",
      "Processing call [580] out of [1000] = [58.0%]... ETA mm:ss 16:28\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,029 ms\n",
      "Tokens per second [15.8] input tokens [601] + xml response tokens [32] = total tokens i/o [633]\n",
      "Response: [<response><command>search perplexity using clipboard current tab</command><args></args></response>]\n",
      "\n",
      "Processing call [581] out of [1000] = [58.1%]... ETA mm:ss 16:25\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,278 ms\n",
      "Tokens per second [15.8] input tokens [605] + xml response tokens [36] = total tokens i/o [641]\n",
      "Response: [<response><command>search google current tab</command><args>Neural Network hyperparameter tuning</args></response>]\n",
      "\n",
      "Processing call [582] out of [1000] = [58.2%]... ETA mm:ss 16:23\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,193 ms\n",
      "Tokens per second [16.0] input tokens [362] + xml response tokens [35] = total tokens i/o [397]\n",
      "Response: [<response><command>agent router go to receptionist</command><args>Help Desk</args></response>]\n",
      "\n",
      "Processing call [583] out of [1000] = [58.3%]... ETA mm:ss 16:20\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,035 ms\n",
      "Tokens per second [15.7] input tokens [601] + xml response tokens [32] = total tokens i/o [633]\n",
      "Response: [<response><command>search perplexity new tab</command><args>Warning</args></response>]\n",
      "\n",
      "Processing call [584] out of [1000] = [58.4%]... ETA mm:ss 16:18\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,660 ms\n",
      "Tokens per second [15.7] input tokens [612] + xml response tokens [26] = total tokens i/o [638]\n",
      "Response: [<response><command>none</command><args></args></response>]\n",
      "\n",
      "Processing call [585] out of [1000] = [58.5%]... ETA mm:ss 16:15\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 3,033 ms\n",
      "Tokens per second [15.8] input tokens [612] + xml response tokens [48] = total tokens i/o [660]\n",
      "Response: [<response><command>search perplexity current tab</command><args>Azure Machine Learning platform: What features does the Azure Machine Learning platform provide for model deployment?</args></response>]\n",
      "\n",
      "Processing call [586] out of [1000] = [58.6%]... ETA mm:ss 16:13\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,873 ms\n",
      "Tokens per second [16.0] input tokens [381] + xml response tokens [30] = total tokens i/o [411]\n",
      "Response: [<response><command>agent router go to calendar</command><args></args></response>]\n",
      "\n",
      "Processing call [587] out of [1000] = [58.7%]... ETA mm:ss 16:10\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,191 ms\n",
      "Tokens per second [16.0] input tokens [370] + xml response tokens [35] = total tokens i/o [405]\n",
      "Response: [<response><command>agent router go to receptionist</command><args>Service Rep Agent</args></response>]\n",
      "\n",
      "Processing call [588] out of [1000] = [58.8%]... ETA mm:ss 16:08\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,408 ms\n",
      "Tokens per second [15.8] input tokens [607] + xml response tokens [38] = total tokens i/o [645]\n",
      "Response: [<response><command>go to new tab</command><args>prod.remarkablepenguin.com</args></response>]\n",
      "\n",
      "Processing call [589] out of [1000] = [58.9%]... ETA mm:ss 16:06\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,968 ms\n",
      "Tokens per second [15.8] input tokens [600] + xml response tokens [31] = total tokens i/o [631]\n",
      "Response: [<response><command>search current tab</command><args>Reference Error</args></response>]\n",
      "\n",
      "Processing call [590] out of [1000] = [59.0%]... ETA mm:ss 16:03\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,214 ms\n",
      "Tokens per second [15.8] input tokens [603] + xml response tokens [35] = total tokens i/o [638]\n",
      "Response: [<response><command>search phind current tab</command><args>how to improve memory</args></response>]\n",
      "\n",
      "Processing call [591] out of [1000] = [59.1%]... ETA mm:ss 16:01\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,217 ms\n",
      "Tokens per second [15.8] input tokens [600] + xml response tokens [35] = total tokens i/o [635]\n",
      "Response: [<response><command>search perplexity current tab</command><args>FileNotFoundError</args></response>]\n",
      "\n",
      "Processing call [592] out of [1000] = [59.2%]... ETA mm:ss 15:58\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,712 ms\n",
      "Tokens per second [15.9] input tokens [613] + xml response tokens [43] = total tokens i/o [656]\n",
      "Response: [<response><command>search perplexity new tab</command><args>How can you resolve errors related to ZIP file operations in Python?</args></response>]\n",
      "\n",
      "Processing call [593] out of [1000] = [59.3%]... ETA mm:ss 15:56\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,407 ms\n",
      "Tokens per second [15.8] input tokens [603] + xml response tokens [38] = total tokens i/o [641]\n",
      "Response: [<response><command>go to current tab</command><args>test.magnificentwalrus.com</args></response>]\n",
      "\n",
      "Processing call [594] out of [1000] = [59.4%]... ETA mm:ss 15:54\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,247 ms\n",
      "Tokens per second [16.0] input tokens [374] + xml response tokens [36] = total tokens i/o [410]\n",
      "Response: [<response><command>agent router go to weather</command><args>Buenos Aires, Argentina</args></response>]\n",
      "\n",
      "Processing call [595] out of [1000] = [59.5%]... ETA mm:ss 15:51\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,720 ms\n",
      "Tokens per second [15.8] input tokens [614] + xml response tokens [43] = total tokens i/o [657]\n",
      "Response: [<response><command>search kagi new tab</command><args>File Not Found Error: File not found or path is incorrect</args></response>]\n",
      "\n",
      "Processing call [596] out of [1000] = [59.6%]... ETA mm:ss 15:49\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,969 ms\n",
      "Tokens per second [15.8] input tokens [619] + xml response tokens [47] = total tokens i/o [666]\n",
      "Response: [<response><command>search kagi new tab</command><args>Feature Engineering best practices: How does feature engineering improve the performance of machine learning models?</args></response>]\n",
      "\n",
      "Processing call [597] out of [1000] = [59.7%]... ETA mm:ss 15:47\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,131 ms\n",
      "Tokens per second [16.0] input tokens [384] + xml response tokens [34] = total tokens i/o [418]\n",
      "Response: [<response><command>agent router go to receptionist</command><args>Operator Agent</args></response>]\n",
      "\n",
      "Processing call [598] out of [1000] = [59.8%]... ETA mm:ss 15:45\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,353 ms\n",
      "Tokens per second [15.7] input tokens [603] + xml response tokens [37] = total tokens i/o [640]\n",
      "Response: [<response><command>search google scholar current tab</command><args>Reading CSV files in Pandas</args></response>]\n",
      "\n",
      "Processing call [599] out of [1000] = [59.9%]... ETA mm:ss 15:42\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,407 ms\n",
      "Tokens per second [15.8] input tokens [608] + xml response tokens [38] = total tokens i/o [646]\n",
      "Response: [<response><command>go to new tab</command><args>www.excitingstrawberry.com</args></response>]\n",
      "\n",
      "Processing call [600] out of [1000] = [60.0%]... ETA mm:ss 15:40\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,970 ms\n",
      "Tokens per second [15.8] input tokens [621] + xml response tokens [47] = total tokens i/o [668]\n",
      "Response: [<response><command>search google new tab</command><args>How do you address deprecation warnings in Python to ensure code compatibility with future versions?</args></response>]\n",
      "\n",
      "Processing call [601] out of [1000] = [60.1%]... ETA mm:ss 15:38\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,909 ms\n",
      "Tokens per second [15.8] input tokens [619] + xml response tokens [46] = total tokens i/o [665]\n",
      "Response: [<response><command>search phind new tab</command><args>AI for health diagnostics: How does AI contribute to advancements in health diagnostics?</args></response>]\n",
      "\n",
      "Processing call [602] out of [1000] = [60.2%]... ETA mm:ss 15:36\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,224 ms\n",
      "Tokens per second [15.7] input tokens [604] + xml response tokens [35] = total tokens i/o [639]\n",
      "Response: [<response><command>search google current tab</command><args>AWS SageMaker for ML</args></response>]\n",
      "\n",
      "Processing call [603] out of [1000] = [60.3%]... ETA mm:ss 15:34\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,098 ms\n",
      "Tokens per second [15.7] input tokens [605] + xml response tokens [33] = total tokens i/o [638]\n",
      "Response: [<response><command>search google scholar new tab</command><args>EnvironmentError</args></response>]\n",
      "\n",
      "Processing call [604] out of [1000] = [60.4%]... ETA mm:ss 15:31\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,353 ms\n",
      "Tokens per second [15.7] input tokens [602] + xml response tokens [37] = total tokens i/o [639]\n",
      "Response: [<response><command>search phind current tab</command><args>Reading Excel files with Pandas</args></response>]\n",
      "\n",
      "Processing call [605] out of [1000] = [60.5%]... ETA mm:ss 15:29\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,879 ms\n",
      "Tokens per second [16.0] input tokens [376] + xml response tokens [30] = total tokens i/o [406]\n",
      "Response: [<response><command>agent router go to math</command><args></args></response>]\n",
      "\n",
      "Processing call [606] out of [1000] = [60.6%]... ETA mm:ss 15:26\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,193 ms\n",
      "Tokens per second [16.0] input tokens [369] + xml response tokens [35] = total tokens i/o [404]\n",
      "Response: [<response><command>agent router go to weather</command><args>New York, USA</args></response>]\n",
      "\n",
      "Processing call [607] out of [1000] = [60.7%]... ETA mm:ss 15:24\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,097 ms\n",
      "Tokens per second [15.7] input tokens [604] + xml response tokens [33] = total tokens i/o [637]\n",
      "Response: [<response><command>search google scholar new tab</command><args>Runtime Error</args></response>]\n",
      "\n",
      "Processing call [608] out of [1000] = [60.8%]... ETA mm:ss 15:21\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 3,034 ms\n",
      "Tokens per second [15.8] input tokens [621] + xml response tokens [48] = total tokens i/o [669]\n",
      "Response: [<response><command>search perplexity new tab</command><args>Normalization vs. Standardization: When should you choose normalization over standardization in data preprocessing?</args></response>]\n",
      "\n",
      "Processing call [609] out of [1000] = [60.9%]... ETA mm:ss 15:19\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,973 ms\n",
      "Tokens per second [15.7] input tokens [599] + xml response tokens [31] = total tokens i/o [630]\n",
      "Response: [<response><command>search new tab</command><args>ImportWarning</args></response>]\n",
      "\n",
      "Processing call [610] out of [1000] = [61.0%]... ETA mm:ss 15:17\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,220 ms\n",
      "Tokens per second [15.8] input tokens [604] + xml response tokens [35] = total tokens i/o [639]\n",
      "Response: [<response><command>search google new tab</command><args>NoSQL databases for AI</args></response>]\n",
      "\n",
      "Processing call [611] out of [1000] = [61.1%]... ETA mm:ss 15:14\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,971 ms\n",
      "Tokens per second [15.7] input tokens [602] + xml response tokens [31] = total tokens i/o [633]\n",
      "Response: [<response><command>search google using clipboard current tab</command><args></args></response>]\n",
      "\n",
      "Processing call [612] out of [1000] = [61.2%]... ETA mm:ss 15:12\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,191 ms\n",
      "Tokens per second [16.0] input tokens [368] + xml response tokens [35] = total tokens i/o [403]\n",
      "Response: [<response><command>agent router go to receptionist</command><args>Office Coordinator Agent</args></response>]\n",
      "\n",
      "Processing call [613] out of [1000] = [61.3%]... ETA mm:ss 15:09\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 3,413 ms\n",
      "Tokens per second [15.8] input tokens [624] + xml response tokens [54] = total tokens i/o [678]\n",
      "Response: [<response><command>search phind new tab</command><args>Supervised vs. Unsupervised Learning: What are the key differences between supervised and unsupervised learning in AI?</args></response>]\n",
      "\n",
      "Processing call [614] out of [1000] = [61.4%]... ETA mm:ss 15:08\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,193 ms\n",
      "Tokens per second [16.0] input tokens [365] + xml response tokens [35] = total tokens i/o [400]\n",
      "Response: [<response><command>agent router go to weather</command><args>Reno, Nevada</args></response>]\n",
      "\n",
      "Processing call [615] out of [1000] = [61.5%]... ETA mm:ss 15:05\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,221 ms\n",
      "Tokens per second [15.8] input tokens [607] + xml response tokens [35] = total tokens i/o [642]\n",
      "Response: [<response><command>search google new tab</command><args>best movies of all time</args></response>]\n",
      "\n",
      "Processing call [616] out of [1000] = [61.6%]... ETA mm:ss 15:03\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,912 ms\n",
      "Tokens per second [15.8] input tokens [617] + xml response tokens [46] = total tokens i/o [663]\n",
      "Response: [<response><command>search google current tab</command><args>Feature Engineering best practices: How does feature engineering improve the performance of machine learning models?</args></response>]\n",
      "\n",
      "Processing call [617] out of [1000] = [61.7%]... ETA mm:ss 15:01\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,260 ms\n",
      "Tokens per second [15.9] input tokens [388] + xml response tokens [36] = total tokens i/o [424]\n",
      "Response: [<response><command>agent router go to receptionist</command><args>Administrative Assistant Agent</args></response>]\n",
      "\n",
      "Processing call [618] out of [1000] = [61.8%]... ETA mm:ss 14:58\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,162 ms\n",
      "Tokens per second [15.7] input tokens [603] + xml response tokens [34] = total tokens i/o [637]\n",
      "Response: [<response><command>search kagi new tab</command><args>learning Japanese online</args></response>]\n",
      "\n",
      "Processing call [619] out of [1000] = [61.9%]... ETA mm:ss 14:56\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,163 ms\n",
      "Tokens per second [15.7] input tokens [602] + xml response tokens [34] = total tokens i/o [636]\n",
      "Response: [<response><command>search new tab</command><args>Pending Deprecation Warning</args></response>]\n",
      "\n",
      "Processing call [620] out of [1000] = [62.0%]... ETA mm:ss 14:53\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,355 ms\n",
      "Tokens per second [15.7] input tokens [607] + xml response tokens [37] = total tokens i/o [644]\n",
      "Response: [<response><command>search kagi new tab</command><args>Reading Excel files with Pandas</args></response>]\n",
      "\n",
      "Processing call [621] out of [1000] = [62.1%]... ETA mm:ss 14:51\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,916 ms\n",
      "Tokens per second [15.7] input tokens [608] + xml response tokens [30] = total tokens i/o [638]\n",
      "Response: [<response><command>search using clipboard current tab</command><args></args></response>]\n",
      "\n",
      "Processing call [622] out of [1000] = [62.2%]... ETA mm:ss 14:48\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,440 ms\n",
      "Tokens per second [16.0] input tokens [378] + xml response tokens [39] = total tokens i/o [417]\n",
      "Response: [<response><command>agent router go to date and time</command><args>Greensboro, North Carolina</args></response>]\n",
      "\n",
      "Processing call [623] out of [1000] = [62.3%]... ETA mm:ss 14:46\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,223 ms\n",
      "Tokens per second [15.7] input tokens [606] + xml response tokens [35] = total tokens i/o [641]\n",
      "Response: [<response><command>search google new tab</command><args>AWS SageMaker for ML</args></response>]\n",
      "\n",
      "Processing call [624] out of [1000] = [62.4%]... ETA mm:ss 14:44\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,190 ms\n",
      "Tokens per second [16.0] input tokens [363] + xml response tokens [35] = total tokens i/o [398]\n",
      "Response: [<response><command>agent router go to weather</command><args>Reno, Nevada</args></response>]\n",
      "\n",
      "Processing call [625] out of [1000] = [62.5%]... ETA mm:ss 14:41\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,348 ms\n",
      "Tokens per second [15.8] input tokens [600] + xml response tokens [37] = total tokens i/o [637]\n",
      "Response: [<response><command>go to new tab</command><args>wonderfulzebra.info</args></response>]\n",
      "\n",
      "Processing call [626] out of [1000] = [62.6%]... ETA mm:ss 14:39\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,286 ms\n",
      "Tokens per second [15.7] input tokens [608] + xml response tokens [36] = total tokens i/o [644]\n",
      "Response: [<response><command>search phind new tab</command><args>best movies of all time</args></response>]\n",
      "\n",
      "Processing call [627] out of [1000] = [62.7%]... ETA mm:ss 14:36\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,851 ms\n",
      "Tokens per second [15.8] input tokens [615] + xml response tokens [45] = total tokens i/o [660]\n",
      "Response: [<response><command>search kagi current tab</command><args>What are common causes and solutions for errors related to incorrect syntax in Python?</args></response>]\n",
      "\n",
      "Processing call [628] out of [1000] = [62.8%]... ETA mm:ss 14:34\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,035 ms\n",
      "Tokens per second [15.7] input tokens [600] + xml response tokens [32] = total tokens i/o [632]\n",
      "Response: [<response><command>search new tab</command><args>best online games</args></response>]\n",
      "\n",
      "Processing call [629] out of [1000] = [62.9%]... ETA mm:ss 14:32\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,226 ms\n",
      "Tokens per second [15.7] input tokens [602] + xml response tokens [35] = total tokens i/o [637]\n",
      "Response: [<response><command>search current tab</command><args>User Warning: User-defined warning</args></response>]\n",
      "\n",
      "Processing call [630] out of [1000] = [63.0%]... ETA mm:ss 14:29\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,256 ms\n",
      "Tokens per second [16.0] input tokens [377] + xml response tokens [36] = total tokens i/o [413]\n",
      "Response: [<response><command>agent router go to weather</command><args>North Las Vegas, Nevada</args></response>]\n",
      "\n",
      "Processing call [631] out of [1000] = [63.1%]... ETA mm:ss 14:27\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,917 ms\n",
      "Tokens per second [15.8] input tokens [617] + xml response tokens [46] = total tokens i/o [663]\n",
      "Response: [<response><command>search google scholar new tab</command><args>What are bytes warnings in Python, and how are they significant in data handling?</args></response>]\n",
      "\n",
      "Processing call [632] out of [1000] = [63.2%]... ETA mm:ss 14:25\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,292 ms\n",
      "Tokens per second [15.7] input tokens [612] + xml response tokens [36] = total tokens i/o [648]\n",
      "Response: [<response><command>search perplexity new tab</command><args>AI in content moderation</args></response>]\n",
      "\n",
      "Processing call [633] out of [1000] = [63.3%]... ETA mm:ss 14:23\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,035 ms\n",
      "Tokens per second [15.7] input tokens [601] + xml response tokens [32] = total tokens i/o [633]\n",
      "Response: [<response><command>search current tab</command><args>hip hop music</args></response>]\n",
      "\n",
      "Processing call [634] out of [1000] = [63.4%]... ETA mm:ss 14:20\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,191 ms\n",
      "Tokens per second [16.0] input tokens [374] + xml response tokens [35] = total tokens i/o [409]\n",
      "Response: [<response><command>agent router go to receptionist</command><args>Office Coordinator Agent</args></response>]\n",
      "\n",
      "Processing call [635] out of [1000] = [63.5%]... ETA mm:ss 14:18\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,034 ms\n",
      "Tokens per second [15.7] input tokens [601] + xml response tokens [32] = total tokens i/o [633]\n",
      "Response: [<response><command>search google scholar using clipboard current tab</command><args></args></response>]\n",
      "\n",
      "Processing call [636] out of [1000] = [63.6%]... ETA mm:ss 14:15\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,597 ms\n",
      "Tokens per second [15.8] input tokens [609] + xml response tokens [41] = total tokens i/o [650]\n",
      "Response: [<response><command>search google new tab</command><args>How can decoding errors with Unicode characters be resolved in Python?</args></response>]\n",
      "\n",
      "Processing call [637] out of [1000] = [63.7%]... ETA mm:ss 14:13\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,255 ms\n",
      "Tokens per second [16.0] input tokens [366] + xml response tokens [36] = total tokens i/o [402]\n",
      "Response: [<response><command>agent router go to weather</command><args>Warsaw, Poland</args></response>]\n",
      "\n",
      "Processing call [638] out of [1000] = [63.8%]... ETA mm:ss 14:10\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,289 ms\n",
      "Tokens per second [15.7] input tokens [605] + xml response tokens [36] = total tokens i/o [641]\n",
      "Response: [<response><command>search google scholar current tab</command><args>best movies of all time</args></response>]\n",
      "\n",
      "Processing call [639] out of [1000] = [63.9%]... ETA mm:ss 14:08\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,351 ms\n",
      "Tokens per second [15.7] input tokens [602] + xml response tokens [37] = total tokens i/o [639]\n",
      "Response: [<response><command>search kagi new tab</command><args>Neural Network hyperparameter tuning</args></response>]\n",
      "\n",
      "Processing call [640] out of [1000] = [64.0%]... ETA mm:ss 14:06\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 3,038 ms\n",
      "Tokens per second [15.8] input tokens [617] + xml response tokens [48] = total tokens i/o [665]\n",
      "Response: [<response><command>search new tab</command><args>TensorFlow latest updates: What are the latest features added to TensorFlow in its most recent update?</args></response>]\n",
      "\n",
      "Processing call [641] out of [1000] = [64.1%]... ETA mm:ss 14:04\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,854 ms\n",
      "Tokens per second [15.8] input tokens [618] + xml response tokens [45] = total tokens i/o [663]\n",
      "Response: [<response><command>search perplexity new tab</command><args>OpenAI latest projects: What are the latest projects undertaken by OpenAI?</args></response>]\n",
      "\n",
      "Processing call [642] out of [1000] = [64.2%]... ETA mm:ss 14:02\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,922 ms\n",
      "Tokens per second [15.7] input tokens [616] + xml response tokens [46] = total tokens i/o [662]\n",
      "Response: [<response><command>search kagi current tab</command><args>How do you address runtime warnings in Python, especially those indicating risky runtime behavior?</args></response>]\n",
      "\n",
      "Processing call [643] out of [1000] = [64.3%]... ETA mm:ss 14:00\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,256 ms\n",
      "Tokens per second [16.0] input tokens [371] + xml response tokens [36] = total tokens i/o [407]\n",
      "Response: [<response><command>agent router go to date and time</command><args>Philadelphia, Pennsylvania</args></response>]\n",
      "\n",
      "Processing call [644] out of [1000] = [64.4%]... ETA mm:ss 13:57\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,603 ms\n",
      "Tokens per second [15.8] input tokens [611] + xml response tokens [41] = total tokens i/o [652]\n",
      "Response: [<response><command>search google current tab</command><args>How can decoding errors with Unicode characters be resolved in Python?</args></response>]\n",
      "\n",
      "Processing call [645] out of [1000] = [64.5%]... ETA mm:ss 13:55\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,067 ms\n",
      "Tokens per second [16.0] input tokens [381] + xml response tokens [33] = total tokens i/o [414]\n",
      "Response: [<response><command>agent router go to receptionist</command><args>Operator</args></response>]\n",
      "\n",
      "Processing call [646] out of [1000] = [64.6%]... ETA mm:ss 13:53\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,193 ms\n",
      "Tokens per second [16.0] input tokens [375] + xml response tokens [35] = total tokens i/o [410]\n",
      "Response: [<response><command>agent router go to weather</command><args>Oslo, Norway</args></response>]\n",
      "\n",
      "Processing call [647] out of [1000] = [64.7%]... ETA mm:ss 13:50\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,888 ms\n",
      "Tokens per second [15.9] input tokens [409] + xml response tokens [30] = total tokens i/o [439]\n",
      "Response: [<response><command>agent router go to calendar</command><args></args></response>]\n",
      "\n",
      "Processing call [648] out of [1000] = [64.8%]... ETA mm:ss 13:47\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,354 ms\n",
      "Tokens per second [15.7] input tokens [605] + xml response tokens [37] = total tokens i/o [642]\n",
      "Response: [<response><command>search phind current tab</command><args>what are the benefits of exercise?</args></response>]\n",
      "\n",
      "Processing call [649] out of [1000] = [64.9%]... ETA mm:ss 13:45\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,970 ms\n",
      "Tokens per second [15.7] input tokens [602] + xml response tokens [31] = total tokens i/o [633]\n",
      "Response: [<response><command>search google using clipboard current tab</command><args></args></response>]\n",
      "\n",
      "Processing call [650] out of [1000] = [65.0%]... ETA mm:ss 13:43\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,037 ms\n",
      "Tokens per second [15.7] input tokens [604] + xml response tokens [32] = total tokens i/o [636]\n",
      "Response: [<response><command>search perplexity using clipboard new tab</command><args></args></response>]\n",
      "\n",
      "Processing call [651] out of [1000] = [65.1%]... ETA mm:ss 13:40\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,286 ms\n",
      "Tokens per second [15.7] input tokens [605] + xml response tokens [36] = total tokens i/o [641]\n",
      "Response: [<response><command>search perplexity new tab</command><args>AWS SageMaker for ML</args></response>]\n",
      "\n",
      "Processing call [652] out of [1000] = [65.2%]... ETA mm:ss 13:38\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,102 ms\n",
      "Tokens per second [15.7] input tokens [606] + xml response tokens [33] = total tokens i/o [639]\n",
      "Response: [<response><command>search phind new tab</command><args>RuntimeWarning</args></response>]\n",
      "\n",
      "Processing call [653] out of [1000] = [65.3%]... ETA mm:ss 13:35\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 3,169 ms\n",
      "Tokens per second [15.8] input tokens [622] + xml response tokens [50] = total tokens i/o [672]\n",
      "Response: [<response><command>search perplexity new tab</command><args>Convolutional Neural Networks: What are the primary applications of convolutional neural networks in image processing?</args></response>]\n",
      "\n",
      "Processing call [654] out of [1000] = [65.4%]... ETA mm:ss 13:33\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,037 ms\n",
      "Tokens per second [15.7] input tokens [603] + xml response tokens [32] = total tokens i/o [635]\n",
      "Response: [<response><command>search phind new tab</command><args>Warning</args></response>]\n",
      "\n",
      "Processing call [655] out of [1000] = [65.5%]... ETA mm:ss 13:31\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,164 ms\n",
      "Tokens per second [15.7] input tokens [600] + xml response tokens [34] = total tokens i/o [634]\n",
      "Response: [<response><command>search google current tab</command><args>AI in financial forecasting</args></response>]\n",
      "\n",
      "Processing call [656] out of [1000] = [65.6%]... ETA mm:ss 13:28\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,105 ms\n",
      "Tokens per second [15.7] input tokens [601] + xml response tokens [33] = total tokens i/o [634]\n",
      "Response: [<response><command>search new tab</command><args>Connection Refused Error</args></response>]\n",
      "\n",
      "Processing call [657] out of [1000] = [65.7%]... ETA mm:ss 13:26\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,164 ms\n",
      "Tokens per second [15.7] input tokens [604] + xml response tokens [34] = total tokens i/o [638]\n",
      "Response: [<response><command>search google scholar current tab</command><args>AssertionError</args></response>]\n",
      "\n",
      "Processing call [658] out of [1000] = [65.8%]... ETA mm:ss 13:23\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,197 ms\n",
      "Tokens per second [15.9] input tokens [369] + xml response tokens [35] = total tokens i/o [404]\n",
      "Response: [<response><command>agent router go to weather</command><args>Fort Worth, Texas</args></response>]\n",
      "\n",
      "Processing call [659] out of [1000] = [65.9%]... ETA mm:ss 13:21\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,161 ms\n",
      "Tokens per second [15.7] input tokens [603] + xml response tokens [34] = total tokens i/o [637]\n",
      "Response: [<response><command>search perplexity new tab</command><args>Broken Pipe Error</args></response>]\n",
      "\n",
      "Processing call [660] out of [1000] = [66.0%]... ETA mm:ss 13:18\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 3,173 ms\n",
      "Tokens per second [15.8] input tokens [622] + xml response tokens [50] = total tokens i/o [672]\n",
      "Response: [<response><command>search phind new tab</command><args>TensorFlow latest updates: What are the latest features added to TensorFlow in its most recent update?</args></response>]\n",
      "\n",
      "Processing call [661] out of [1000] = [66.1%]... ETA mm:ss 13:17\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,357 ms\n",
      "Tokens per second [15.7] input tokens [606] + xml response tokens [37] = total tokens i/o [643]\n",
      "Response: [<response><command>search phind current tab</command><args>Handling JSON data in Pandas</args></response>]\n",
      "\n",
      "Processing call [662] out of [1000] = [66.2%]... ETA mm:ss 13:14\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,974 ms\n",
      "Tokens per second [15.8] input tokens [620] + xml response tokens [47] = total tokens i/o [667]\n",
      "Response: [<response><command>search google new tab</command><args>Neural Network hyperparameter tuning: What approaches are recommended for neural network hyperparameter tuning?</args></response>]\n",
      "\n",
      "Processing call [663] out of [1000] = [66.3%]... ETA mm:ss 13:12\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,295 ms\n",
      "Tokens per second [15.7] input tokens [602] + xml response tokens [36] = total tokens i/o [638]\n",
      "Response: [<response><command>search google current tab</command><args>what are the benefits of exercise?</args></response>]\n",
      "\n",
      "Processing call [664] out of [1000] = [66.4%]... ETA mm:ss 13:10\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,044 ms\n",
      "Tokens per second [15.7] input tokens [607] + xml response tokens [32] = total tokens i/o [639]\n",
      "Response: [<response><command>search kagi using clipboard new tab</command><args></args></response>]\n",
      "\n",
      "Processing call [665] out of [1000] = [66.5%]... ETA mm:ss 13:07\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,417 ms\n",
      "Tokens per second [15.7] input tokens [605] + xml response tokens [38] = total tokens i/o [643]\n",
      "Response: [<response><command>go to new tab</command><args>test.excitinggiraffe.com</args></response>]\n",
      "\n",
      "Processing call [666] out of [1000] = [66.6%]... ETA mm:ss 13:05\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,730 ms\n",
      "Tokens per second [15.8] input tokens [613] + xml response tokens [43] = total tokens i/o [656]\n",
      "Response: [<response><command>search kagi current tab</command><args>File Not Found Error: File not found or path is incorrect</args></response>]\n",
      "\n",
      "Processing call [667] out of [1000] = [66.7%]... ETA mm:ss 13:03\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,196 ms\n",
      "Tokens per second [15.9] input tokens [371] + xml response tokens [35] = total tokens i/o [406]\n",
      "Response: [<response><command>agent router go to weather</command><args>Dublin, Ireland</args></response>]\n",
      "\n",
      "Processing call [668] out of [1000] = [66.8%]... ETA mm:ss 13:00\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,890 ms\n",
      "Tokens per second [15.9] input tokens [400] + xml response tokens [30] = total tokens i/o [430]\n",
      "Response: [<response><command>agent router go to calendar</command><args></args></response>]\n",
      "\n",
      "Processing call [669] out of [1000] = [66.9%]... ETA mm:ss 12:58\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,412 ms\n",
      "Tokens per second [15.8] input tokens [606] + xml response tokens [38] = total tokens i/o [644]\n",
      "Response: [<response><command>search kagi current tab</command><args>Type Error: Incorrect type comparison</args></response>]\n",
      "\n",
      "Processing call [670] out of [1000] = [67.0%]... ETA mm:ss 12:55\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 3,048 ms\n",
      "Tokens per second [15.7] input tokens [618] + xml response tokens [48] = total tokens i/o [666]\n",
      "Response: [<response><command>search perplexity new tab</command><args>Sentiment Analysis tools: What tools are widely used for sentiment analysis in social media data?</args></response>]\n",
      "\n",
      "Processing call [671] out of [1000] = [67.1%]... ETA mm:ss 12:53\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,165 ms\n",
      "Tokens per second [15.7] input tokens [604] + xml response tokens [34] = total tokens i/o [638]\n",
      "Response: [<response><command>search new tab</command><args>AI for customer service automation</args></response>]\n",
      "\n",
      "Processing call [672] out of [1000] = [67.2%]... ETA mm:ss 12:51\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,352 ms\n",
      "Tokens per second [15.7] input tokens [603] + xml response tokens [37] = total tokens i/o [640]\n",
      "Response: [<response><command>go to current tab</command><args>dev.fantasticcherry.info</args></response>]\n",
      "\n",
      "Processing call [673] out of [1000] = [67.3%]... ETA mm:ss 12:49\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,224 ms\n",
      "Tokens per second [15.7] input tokens [602] + xml response tokens [35] = total tokens i/o [637]\n",
      "Response: [<response><command>search current tab</command><args>User Warning: User-defined warning</args></response>]\n",
      "\n",
      "Processing call [674] out of [1000] = [67.4%]... ETA mm:ss 12:46\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,666 ms\n",
      "Tokens per second [15.8] input tokens [610] + xml response tokens [42] = total tokens i/o [652]\n",
      "Response: [<response><command>search google scholar current tab</command><args>How can decoding errors with Unicode characters be resolved in Python?</args></response>]\n",
      "\n",
      "Processing call [675] out of [1000] = [67.5%]... ETA mm:ss 12:44\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,135 ms\n",
      "Tokens per second [15.9] input tokens [364] + xml response tokens [34] = total tokens i/o [398]\n",
      "Response: [<response><command>agent router go to receptionist</command><args>Office Assistant</args></response>]\n",
      "\n",
      "Processing call [676] out of [1000] = [67.6%]... ETA mm:ss 12:42\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,228 ms\n",
      "Tokens per second [15.7] input tokens [604] + xml response tokens [35] = total tokens i/o [639]\n",
      "Response: [<response><command>search new tab</command><args>Cross-tabulation in Pandas</args></response>]\n",
      "\n",
      "Processing call [677] out of [1000] = [67.7%]... ETA mm:ss 12:39\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,351 ms\n",
      "Tokens per second [15.7] input tokens [603] + xml response tokens [37] = total tokens i/o [640]\n",
      "Response: [<response><command>search perplexity new tab</command><args>Reading CSV files in Pandas</args></response>]\n",
      "\n",
      "Processing call [678] out of [1000] = [67.8%]... ETA mm:ss 12:37\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,354 ms\n",
      "Tokens per second [15.7] input tokens [607] + xml response tokens [37] = total tokens i/o [644]\n",
      "Response: [<response><command>search phind new tab</command><args>Working with datetime in Pandas</args></response>]\n",
      "\n",
      "Processing call [679] out of [1000] = [67.9%]... ETA mm:ss 12:34\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,286 ms\n",
      "Tokens per second [15.7] input tokens [603] + xml response tokens [36] = total tokens i/o [639]\n",
      "Response: [<response><command>go to current tab</command><args>amazingstrawberry.org</args></response>]\n",
      "\n",
      "Processing call [680] out of [1000] = [68.0%]... ETA mm:ss 12:32\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,290 ms\n",
      "Tokens per second [15.7] input tokens [604] + xml response tokens [36] = total tokens i/o [640]\n",
      "Response: [<response><command>search google current tab</command><args>User Warning: User-defined warning</args></response>]\n",
      "\n",
      "Processing call [681] out of [1000] = [68.1%]... ETA mm:ss 12:30\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,290 ms\n",
      "Tokens per second [15.7] input tokens [602] + xml response tokens [36] = total tokens i/o [638]\n",
      "Response: [<response><command>search google current tab</command><args>how to make homemade bread</args></response>]\n",
      "\n",
      "Processing call [682] out of [1000] = [68.2%]... ETA mm:ss 12:27\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,980 ms\n",
      "Tokens per second [15.8] input tokens [621] + xml response tokens [47] = total tokens i/o [668]\n",
      "Response: [<response><command>search phind new tab</command><args>AWS SageMaker for ML: How does AWS SageMaker facilitate machine learning model development?</args></response>]\n",
      "\n",
      "Processing call [683] out of [1000] = [68.3%]... ETA mm:ss 12:25\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,355 ms\n",
      "Tokens per second [15.7] input tokens [603] + xml response tokens [37] = total tokens i/o [640]\n",
      "Response: [<response><command>search phind current tab</command><args>Reading CSV files in Pandas</args></response>]\n",
      "\n",
      "Processing call [684] out of [1000] = [68.4%]... ETA mm:ss 12:23\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,354 ms\n",
      "Tokens per second [15.7] input tokens [606] + xml response tokens [37] = total tokens i/o [643]\n",
      "Response: [<response><command>go to current tab</command><args>blog.jubilantquartz.info</args></response>]\n",
      "\n",
      "Processing call [685] out of [1000] = [68.5%]... ETA mm:ss 12:21\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,328 ms\n",
      "Tokens per second [15.9] input tokens [365] + xml response tokens [37] = total tokens i/o [402]\n",
      "Response: [<response><command>agent router go to date and time</command><args>Los Angeles, USA</args></response>]\n",
      "\n",
      "Processing call [686] out of [1000] = [68.6%]... ETA mm:ss 12:18\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,363 ms\n",
      "Tokens per second [15.7] input tokens [606] + xml response tokens [37] = total tokens i/o [643]\n",
      "Response: [<response><command>go to new tab</command><args>prod.jubilantlemur.io</args></response>]\n",
      "\n",
      "Processing call [687] out of [1000] = [68.7%]... ETA mm:ss 12:16\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,358 ms\n",
      "Tokens per second [15.7] input tokens [611] + xml response tokens [37] = total tokens i/o [648]\n",
      "Response: [<response><command>search perplexity new tab</command><args>best vacation spots in the world</args></response>]\n",
      "\n",
      "Processing call [688] out of [1000] = [68.8%]... ETA mm:ss 12:14\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,789 ms\n",
      "Tokens per second [15.8] input tokens [614] + xml response tokens [44] = total tokens i/o [658]\n",
      "Response: [<response><command>search google new tab</command><args>What are the best practices for handling reset connections in network communications in Python?</args></response>]\n",
      "\n",
      "Processing call [689] out of [1000] = [68.9%]... ETA mm:ss 12:11\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,857 ms\n",
      "Tokens per second [15.8] input tokens [612] + xml response tokens [45] = total tokens i/o [657]\n",
      "Response: [<response><command>search current tab</command><args>Feature Engineering best practices: How does feature engineering improve the performance of machine learning models?</args></response>]\n",
      "\n",
      "Processing call [690] out of [1000] = [69.0%]... ETA mm:ss 12:09\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,352 ms\n",
      "Tokens per second [15.7] input tokens [605] + xml response tokens [37] = total tokens i/o [642]\n",
      "Response: [<response><command>search current tab</command><args>Setting and resetting index in Pandas</args></response>]\n",
      "\n",
      "Processing call [691] out of [1000] = [69.1%]... ETA mm:ss 12:07\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,355 ms\n",
      "Tokens per second [15.7] input tokens [603] + xml response tokens [37] = total tokens i/o [640]\n",
      "Response: [<response><command>go to new tab</command><args>prod.jubilantlemur.io</args></response>]\n",
      "\n",
      "Processing call [692] out of [1000] = [69.2%]... ETA mm:ss 12:05\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,168 ms\n",
      "Tokens per second [15.7] input tokens [601] + xml response tokens [34] = total tokens i/o [635]\n",
      "Response: [<response><command>search current tab</command><args>AWS SageMaker for ML</args></response>]\n",
      "\n",
      "Processing call [693] out of [1000] = [69.3%]... ETA mm:ss 12:02\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,732 ms\n",
      "Tokens per second [15.7] input tokens [614] + xml response tokens [43] = total tokens i/o [657]\n",
      "Response: [<response><command>search phind new tab</command><args>How do you fix indentation errors that affect code structure in Python?</args></response>]\n",
      "\n",
      "Processing call [694] out of [1000] = [69.4%]... ETA mm:ss 12:00\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,199 ms\n",
      "Tokens per second [15.9] input tokens [373] + xml response tokens [35] = total tokens i/o [408]\n",
      "Response: [<response><command>agent router go to weather</command><args>Fort Worth, Texas</args></response>]\n",
      "\n",
      "Processing call [695] out of [1000] = [69.5%]... ETA mm:ss 11:57\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,417 ms\n",
      "Tokens per second [15.7] input tokens [606] + xml response tokens [38] = total tokens i/o [644]\n",
      "Response: [<response><command>go to current tab</command><args>dev.amazinghamburger.info</args></response>]\n",
      "\n",
      "Processing call [696] out of [1000] = [69.6%]... ETA mm:ss 11:55\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,230 ms\n",
      "Tokens per second [15.7] input tokens [602] + xml response tokens [35] = total tokens i/o [637]\n",
      "Response: [<response><command>search new tab</command><args>Reading CSV files in Pandas</args></response>]\n",
      "\n",
      "Processing call [697] out of [1000] = [69.7%]... ETA mm:ss 11:53\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,230 ms\n",
      "Tokens per second [15.7] input tokens [605] + xml response tokens [35] = total tokens i/o [640]\n",
      "Response: [<response><command>search phind current tab</command><args>how to improve memory</args></response>]\n",
      "\n",
      "Processing call [698] out of [1000] = [69.8%]... ETA mm:ss 11:50\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,228 ms\n",
      "Tokens per second [15.7] input tokens [603] + xml response tokens [35] = total tokens i/o [638]\n",
      "Response: [<response><command>search new tab</command><args>Handling JSON data in Pandas</args></response>]\n",
      "\n",
      "Processing call [699] out of [1000] = [69.9%]... ETA mm:ss 11:48\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,290 ms\n",
      "Tokens per second [15.7] input tokens [604] + xml response tokens [36] = total tokens i/o [640]\n",
      "Response: [<response><command>search current tab</command><args>Pandas and geospatial data</args></response>]\n",
      "\n",
      "Processing call [700] out of [1000] = [70.0%]... ETA mm:ss 11:46\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,164 ms\n",
      "Tokens per second [15.7] input tokens [601] + xml response tokens [34] = total tokens i/o [635]\n",
      "Response: [<response><command>search kagi current tab</command><args>RecursionError</args></response>]\n",
      "\n",
      "Processing call [701] out of [1000] = [70.1%]... ETA mm:ss 11:43\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,040 ms\n",
      "Tokens per second [15.7] input tokens [600] + xml response tokens [32] = total tokens i/o [632]\n",
      "Response: [<response><command>search perplexity using clipboard current tab</command><args></args></response>]\n",
      "\n",
      "Processing call [702] out of [1000] = [70.2%]... ETA mm:ss 11:41\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,229 ms\n",
      "Tokens per second [15.7] input tokens [605] + xml response tokens [35] = total tokens i/o [640]\n",
      "Response: [<response><command>search google scholar new tab</command><args>AI in financial forecasting</args></response>]\n",
      "\n",
      "Processing call [703] out of [1000] = [70.3%]... ETA mm:ss 11:38\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,197 ms\n",
      "Tokens per second [15.9] input tokens [375] + xml response tokens [35] = total tokens i/o [410]\n",
      "Response: [<response><command>agent router go to weather</command><args>Cincinnati, Ohio</args></response>]\n",
      "\n",
      "Processing call [704] out of [1000] = [70.4%]... ETA mm:ss 11:36\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,668 ms\n",
      "Tokens per second [15.7] input tokens [614] + xml response tokens [42] = total tokens i/o [656]\n",
      "Response: [<response><command>search google new tab</command><args>File Not Found Error: File not found or path is incorrect</args></response>]\n",
      "\n",
      "Processing call [705] out of [1000] = [70.5%]... ETA mm:ss 11:34\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,356 ms\n",
      "Tokens per second [15.7] input tokens [607] + xml response tokens [37] = total tokens i/o [644]\n",
      "Response: [<response><command>go to current tab</command><args>stage.amazingrainbow.net</args></response>]\n",
      "\n",
      "Processing call [706] out of [1000] = [70.6%]... ETA mm:ss 11:31\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,037 ms\n",
      "Tokens per second [15.7] input tokens [600] + xml response tokens [32] = total tokens i/o [632]\n",
      "Response: [<response><command>search google scholar using clipboard new tab</command><args></args></response>]\n",
      "\n",
      "Processing call [707] out of [1000] = [70.7%]... ETA mm:ss 11:29\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,295 ms\n",
      "Tokens per second [15.7] input tokens [606] + xml response tokens [36] = total tokens i/o [642]\n",
      "Response: [<response><command>search new tab</command><args>System Error: Internal Python system issue</args></response>]\n",
      "\n",
      "Processing call [708] out of [1000] = [70.8%]... ETA mm:ss 11:26\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,898 ms\n",
      "Tokens per second [15.8] input tokens [410] + xml response tokens [30] = total tokens i/o [440]\n",
      "Response: [<response><command>agent router go to calendar</command><args></args></response>]\n",
      "\n",
      "Processing call [709] out of [1000] = [70.9%]... ETA mm:ss 11:24\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,732 ms\n",
      "Tokens per second [15.7] input tokens [610] + xml response tokens [43] = total tokens i/o [653]\n",
      "Response: [<response><command>search kagi new tab</command><args>How can system exit errors be handled gracefully in Python applications?</args></response>]\n",
      "\n",
      "Processing call [710] out of [1000] = [71.0%]... ETA mm:ss 11:22\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,857 ms\n",
      "Tokens per second [15.8] input tokens [618] + xml response tokens [45] = total tokens i/o [663]\n",
      "Response: [<response><command>search perplexity new tab</command><args>What are the best practices to manage and prevent memory errors in Python applications?</args></response>]\n",
      "\n",
      "Processing call [711] out of [1000] = [71.1%]... ETA mm:ss 11:20\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,882 ms\n",
      "Tokens per second [15.9] input tokens [374] + xml response tokens [30] = total tokens i/o [404]\n",
      "Response: [<response><command>agent router go to math</command><args></args></response>]\n",
      "\n",
      "Processing call [712] out of [1000] = [71.2%]... ETA mm:ss 11:17\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,853 ms\n",
      "Tokens per second [15.8] input tokens [610] + xml response tokens [45] = total tokens i/o [655]\n",
      "Response: [<response><command>search google scholar current tab</command><args>How do you handle broken pipe errors in Python, especially in network communications?</args></response>]\n",
      "\n",
      "Processing call [713] out of [1000] = [71.3%]... ETA mm:ss 11:15\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,886 ms\n",
      "Tokens per second [15.9] input tokens [378] + xml response tokens [30] = total tokens i/o [408]\n",
      "Response: [<response><command>agent router go to calendar</command><args></args></response>]\n",
      "\n",
      "Processing call [714] out of [1000] = [71.4%]... ETA mm:ss 11:12\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,891 ms\n",
      "Tokens per second [15.9] input tokens [394] + xml response tokens [30] = total tokens i/o [424]\n",
      "Response: [<response><command>agent router go to calendar</command><args></args></response>]\n",
      "\n",
      "Processing call [715] out of [1000] = [71.5%]... ETA mm:ss 11:10\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,674 ms\n",
      "Tokens per second [15.7] input tokens [614] + xml response tokens [42] = total tokens i/o [656]\n",
      "Response: [<response><command>search google new tab</command><args>How do you fix indentation errors that affect code structure in Python?</args></response>]\n",
      "\n",
      "Processing call [716] out of [1000] = [71.6%]... ETA mm:ss 11:08\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,354 ms\n",
      "Tokens per second [15.7] input tokens [604] + xml response tokens [37] = total tokens i/o [641]\n",
      "Response: [<response><command>go to current tab</command><args>test.magnificenticeberg.info</args></response>]\n",
      "\n",
      "Processing call [717] out of [1000] = [71.7%]... ETA mm:ss 11:05\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,164 ms\n",
      "Tokens per second [15.7] input tokens [606] + xml response tokens [34] = total tokens i/o [640]\n",
      "Response: [<response><command>search perplexity new tab</command><args>SQL database management</args></response>]\n",
      "\n",
      "Processing call [718] out of [1000] = [71.8%]... ETA mm:ss 11:03\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,228 ms\n",
      "Tokens per second [15.7] input tokens [602] + xml response tokens [35] = total tokens i/o [637]\n",
      "Response: [<response><command>search phind current tab</command><args>IsADirectoryError</args></response>]\n",
      "\n",
      "Processing call [719] out of [1000] = [71.9%]... ETA mm:ss 11:00\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,608 ms\n",
      "Tokens per second [15.7] input tokens [608] + xml response tokens [41] = total tokens i/o [649]\n",
      "Response: [<response><command>search google scholar current tab</command><args>Unbound Local Error: Local variable referenced before assignment</args></response>]\n",
      "\n",
      "Processing call [720] out of [1000] = [72.0%]... ETA mm:ss 10:58\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,102 ms\n",
      "Tokens per second [15.7] input tokens [598] + xml response tokens [33] = total tokens i/o [631]\n",
      "Response: [<response><command>search phind current tab</command><args>Memory Error</args></response>]\n",
      "\n",
      "Processing call [721] out of [1000] = [72.1%]... ETA mm:ss 10:56\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,138 ms\n",
      "Tokens per second [15.9] input tokens [388] + xml response tokens [34] = total tokens i/o [422]\n",
      "Response: [<response><command>agent router go to receptionist</command><args>Info Clerk</args></response>]\n",
      "\n",
      "Processing call [722] out of [1000] = [72.2%]... ETA mm:ss 10:53\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,557 ms\n",
      "Tokens per second [15.6] input tokens [610] + xml response tokens [40] = total tokens i/o [650]\n",
      "Response: [<response><command>search perplexity current tab</command><args>Unsorted Index Error: Index is unsorted</args></response>]\n",
      "\n",
      "Processing call [723] out of [1000] = [72.3%]... ETA mm:ss 10:51\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,503 ms\n",
      "Tokens per second [15.2] input tokens [609] + xml response tokens [38] = total tokens i/o [647]\n",
      "Response: [<response><command>search google scholar new tab</command><args>what is climate change and its effects?</args></response>]\n",
      "\n",
      "Processing call [724] out of [1000] = [72.4%]... ETA mm:ss 10:49\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,923 ms\n",
      "Tokens per second [15.6] input tokens [378] + xml response tokens [30] = total tokens i/o [408]\n",
      "Response: [<response><command>agent router go to math</command><args></args></response>]\n",
      "\n",
      "Processing call [725] out of [1000] = [72.5%]... ETA mm:ss 10:46\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,338 ms\n",
      "Tokens per second [15.4] input tokens [603] + xml response tokens [36] = total tokens i/o [639]\n",
      "Response: [<response><command>search google current tab</command><args>why do dogs wag their tail</args></response>]\n",
      "\n",
      "Processing call [726] out of [1000] = [72.6%]... ETA mm:ss 10:44\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,876 ms\n",
      "Tokens per second [15.6] input tokens [613] + xml response tokens [45] = total tokens i/o [658]\n",
      "Response: [<response><command>search phind current tab</command><args>What are common causes and solutions for errors related to incorrect syntax in Python?</args></response>]\n",
      "\n",
      "Processing call [727] out of [1000] = [72.7%]... ETA mm:ss 10:42\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,281 ms\n",
      "Tokens per second [15.8] input tokens [372] + xml response tokens [36] = total tokens i/o [408]\n",
      "Response: [<response><command>agent router go to weather</command><args>Nashville, Tennessee</args></response>]\n",
      "\n",
      "Processing call [728] out of [1000] = [72.8%]... ETA mm:ss 10:39\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,355 ms\n",
      "Tokens per second [15.7] input tokens [602] + xml response tokens [37] = total tokens i/o [639]\n",
      "Response: [<response><command>go to current tab</command><args>blog.fantasticnovember.io</args></response>]\n",
      "\n",
      "Processing call [729] out of [1000] = [72.9%]... ETA mm:ss 10:37\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,232 ms\n",
      "Tokens per second [15.7] input tokens [604] + xml response tokens [35] = total tokens i/o [639]\n",
      "Response: [<response><command>search google new tab</command><args>AI for customer service automation</args></response>]\n",
      "\n",
      "Processing call [730] out of [1000] = [73.0%]... ETA mm:ss 10:34\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,294 ms\n",
      "Tokens per second [15.7] input tokens [601] + xml response tokens [36] = total tokens i/o [637]\n",
      "Response: [<response><command>search kagi current tab</command><args>how to tie a tie</args></response>]\n",
      "\n",
      "Processing call [731] out of [1000] = [73.1%]... ETA mm:ss 10:32\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,265 ms\n",
      "Tokens per second [15.9] input tokens [367] + xml response tokens [36] = total tokens i/o [403]\n",
      "Response: [<response><command>agent router go to weather</command><args>Almaty, Kazakhstan</args></response>]\n",
      "\n",
      "Processing call [732] out of [1000] = [73.2%]... ETA mm:ss 10:30\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,224 ms\n",
      "Tokens per second [15.7] input tokens [603] + xml response tokens [35] = total tokens i/o [638]\n",
      "Response: [<response><command>search new tab</command><args>why do dogs wag their tail</args></response>]\n",
      "\n",
      "Processing call [733] out of [1000] = [73.3%]... ETA mm:ss 10:27\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,039 ms\n",
      "Tokens per second [15.7] input tokens [606] + xml response tokens [32] = total tokens i/o [638]\n",
      "Response: [<response><command>search kagi using clipboard current tab</command><args></args></response>]\n",
      "\n",
      "Processing call [734] out of [1000] = [73.4%]... ETA mm:ss 10:25\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,194 ms\n",
      "Tokens per second [16.0] input tokens [375] + xml response tokens [35] = total tokens i/o [410]\n",
      "Response: [<response><command>agent router go to weather</command><args>Madison, Wisconsin</args></response>]\n",
      "\n",
      "Processing call [735] out of [1000] = [73.5%]... ETA mm:ss 10:22\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,355 ms\n",
      "Tokens per second [15.7] input tokens [605] + xml response tokens [37] = total tokens i/o [642]\n",
      "Response: [<response><command>go to current tab</command><args>blog.beautifulnovember.net</args></response>]\n",
      "\n",
      "Processing call [736] out of [1000] = [73.6%]... ETA mm:ss 10:20\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 3,170 ms\n",
      "Tokens per second [15.8] input tokens [621] + xml response tokens [50] = total tokens i/o [671]\n",
      "Response: [<response><command>search perplexity new tab</command><args>TensorFlow latest updates: What are the latest features added to TensorFlow in its most recent update?</args></response>]\n",
      "\n",
      "Processing call [737] out of [1000] = [73.7%]... ETA mm:ss 10:18\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,166 ms\n",
      "Tokens per second [15.7] input tokens [606] + xml response tokens [34] = total tokens i/o [640]\n",
      "Response: [<response><command>search google new tab</command><args>AI in financial forecasting</args></response>]\n",
      "\n",
      "Processing call [738] out of [1000] = [73.8%]... ETA mm:ss 10:16\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,977 ms\n",
      "Tokens per second [15.7] input tokens [599] + xml response tokens [31] = total tokens i/o [630]\n",
      "Response: [<response><command>search new tab</command><args>Reference Error</args></response>]\n",
      "\n",
      "Processing call [739] out of [1000] = [73.9%]... ETA mm:ss 10:13\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,915 ms\n",
      "Tokens per second [15.7] input tokens [599] + xml response tokens [30] = total tokens i/o [629]\n",
      "Response: [<response><command>search using clipboard current tab</command><args></args></response>]\n",
      "\n",
      "Processing call [740] out of [1000] = [74.0%]... ETA mm:ss 10:11\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,041 ms\n",
      "Tokens per second [15.7] input tokens [602] + xml response tokens [32] = total tokens i/o [634]\n",
      "Response: [<response><command>search phind using clipboard new tab</command><args></args></response>]\n",
      "\n",
      "Processing call [741] out of [1000] = [74.1%]... ETA mm:ss 10:08\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,418 ms\n",
      "Tokens per second [15.7] input tokens [604] + xml response tokens [38] = total tokens i/o [642]\n",
      "Response: [<response><command>search google scholar current tab</command><args>Type Error: Incorrect type comparison</args></response>]\n",
      "\n",
      "Processing call [742] out of [1000] = [74.2%]... ETA mm:ss 10:06\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,924 ms\n",
      "Tokens per second [15.7] input tokens [618] + xml response tokens [46] = total tokens i/o [664]\n",
      "Response: [<response><command>search phind new tab</command><args>AI for health diagnostics: How does AI contribute to advancements in health diagnostics?</args></response>]\n",
      "\n",
      "Processing call [743] out of [1000] = [74.3%]... ETA mm:ss 10:04\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,287 ms\n",
      "Tokens per second [15.7] input tokens [604] + xml response tokens [36] = total tokens i/o [640]\n",
      "Response: [<response><command>search google scholar current tab</command><args>NoSQL databases for AI</args></response>]\n",
      "\n",
      "Processing call [744] out of [1000] = [74.4%]... ETA mm:ss 10:01\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,323 ms\n",
      "Tokens per second [15.9] input tokens [372] + xml response tokens [37] = total tokens i/o [409]\n",
      "Response: [<response><command>agent router go to date and time</command><args>Jacksonville, Florida</args></response>]\n",
      "\n",
      "Processing call [745] out of [1000] = [74.5%]... ETA mm:ss 9:59\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,979 ms\n",
      "Tokens per second [15.8] input tokens [619] + xml response tokens [47] = total tokens i/o [666]\n",
      "Response: [<response><command>search google new tab</command><args>Azure Machine Learning platform: What features does the Azure Machine Learning platform provide for model deployment?</args></response>]\n",
      "\n",
      "Processing call [746] out of [1000] = [74.6%]... ETA mm:ss 9:57\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,356 ms\n",
      "Tokens per second [15.7] input tokens [603] + xml response tokens [37] = total tokens i/o [640]\n",
      "Response: [<response><command>go to current tab</command><args>dev.fantasticcherry.info</args></response>]\n",
      "\n",
      "Processing call [747] out of [1000] = [74.7%]... ETA mm:ss 9:54\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,842 ms\n",
      "Tokens per second [15.5] input tokens [613] + xml response tokens [44] = total tokens i/o [657]\n",
      "Response: [<response><command>search kagi current tab</command><args>Deep Learning optimization: What strategies are used for optimizing deep learning models?</args></response>]\n",
      "\n",
      "Processing call [748] out of [1000] = [74.8%]... ETA mm:ss 9:52\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,417 ms\n",
      "Tokens per second [15.3] input tokens [605] + xml response tokens [37] = total tokens i/o [642]\n",
      "Response: [<response><command>search phind current tab</command><args>OS Error: Operating system error</args></response>]\n",
      "\n",
      "Processing call [749] out of [1000] = [74.9%]... ETA mm:ss 9:50\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,488 ms\n",
      "Tokens per second [15.3] input tokens [604] + xml response tokens [38] = total tokens i/o [642]\n",
      "Response: [<response><command>go to current tab</command><args>prod.incrediblejellyfish.net</args></response>]\n",
      "\n",
      "Processing call [750] out of [1000] = [75.0%]... ETA mm:ss 9:48\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,061 ms\n",
      "Tokens per second [15.5] input tokens [601] + xml response tokens [32] = total tokens i/o [633]\n",
      "Response: [<response><command>search new tab</command><args>AI transparency initiatives</args></response>]\n",
      "\n",
      "Processing call [751] out of [1000] = [75.1%]... ETA mm:ss 9:45\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 3,172 ms\n",
      "Tokens per second [15.8] input tokens [616] + xml response tokens [50] = total tokens i/o [666]\n",
      "Response: [<response><command>search google scholar current tab</command><args>Convolutional Neural Networks: What are the primary applications of convolutional neural networks in image processing?</args></response>]\n",
      "\n",
      "Processing call [752] out of [1000] = [75.2%]... ETA mm:ss 9:43\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,355 ms\n",
      "Tokens per second [15.7] input tokens [607] + xml response tokens [37] = total tokens i/o [644]\n",
      "Response: [<response><command>search google scholar current tab</command><args>Reading CSV files in Pandas</args></response>]\n",
      "\n",
      "Processing call [753] out of [1000] = [75.3%]... ETA mm:ss 9:41\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,881 ms\n",
      "Tokens per second [15.9] input tokens [370] + xml response tokens [30] = total tokens i/o [400]\n",
      "Response: [<response><command>agent router go to calendar</command><args></args></response>]\n",
      "\n",
      "Processing call [754] out of [1000] = [75.4%]... ETA mm:ss 9:38\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,911 ms\n",
      "Tokens per second [15.7] input tokens [601] + xml response tokens [30] = total tokens i/o [631]\n",
      "Response: [<response><command>search using clipboard new tab</command><args></args></response>]\n",
      "\n",
      "Processing call [755] out of [1000] = [75.5%]... ETA mm:ss 9:36\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,432 ms\n",
      "Tokens per second [15.6] input tokens [606] + xml response tokens [38] = total tokens i/o [644]\n",
      "Response: [<response><command>go to current tab</command><args>dev.incrediblestrawberry.info</args></response>]\n",
      "\n",
      "Processing call [756] out of [1000] = [75.6%]... ETA mm:ss 9:33\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,293 ms\n",
      "Tokens per second [15.7] input tokens [601] + xml response tokens [36] = total tokens i/o [637]\n",
      "Response: [<response><command>search google scholar current tab</command><args>how to make pizza dough</args></response>]\n",
      "\n",
      "Processing call [757] out of [1000] = [75.7%]... ETA mm:ss 9:31\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,892 ms\n",
      "Tokens per second [15.9] input tokens [413] + xml response tokens [30] = total tokens i/o [443]\n",
      "Response: [<response><command>agent router go to calendar</command><args></args></response>]\n",
      "\n",
      "Processing call [758] out of [1000] = [75.8%]... ETA mm:ss 9:29\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,352 ms\n",
      "Tokens per second [15.7] input tokens [611] + xml response tokens [37] = total tokens i/o [648]\n",
      "Response: [<response><command>search kagi new tab</command><args>Handling JSON data in Pandas</args></response>]\n",
      "\n",
      "Processing call [759] out of [1000] = [75.9%]... ETA mm:ss 9:26\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,980 ms\n",
      "Tokens per second [15.8] input tokens [613] + xml response tokens [47] = total tokens i/o [660]\n",
      "Response: [<response><command>search google current tab</command><args>Neural Network hyperparameter tuning: What approaches are recommended for neural network hyperparameter tuning?</args></response>]\n",
      "\n",
      "Processing call [760] out of [1000] = [76.0%]... ETA mm:ss 9:24\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,665 ms\n",
      "Tokens per second [15.8] input tokens [614] + xml response tokens [42] = total tokens i/o [656]\n",
      "Response: [<response><command>search google new tab</command><args>How can system exit errors be handled gracefully in Python applications?</args></response>]\n",
      "\n",
      "Processing call [761] out of [1000] = [76.1%]... ETA mm:ss 9:22\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,170 ms\n",
      "Tokens per second [15.7] input tokens [609] + xml response tokens [34] = total tokens i/o [643]\n",
      "Response: [<response><command>search phind new tab</command><args>best online games</args></response>]\n",
      "\n",
      "Processing call [762] out of [1000] = [76.2%]... ETA mm:ss 9:19\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,165 ms\n",
      "Tokens per second [15.7] input tokens [601] + xml response tokens [34] = total tokens i/o [635]\n",
      "Response: [<response><command>search phind new tab</command><args>AssertionError</args></response>]\n",
      "\n",
      "Processing call [763] out of [1000] = [76.3%]... ETA mm:ss 9:17\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,887 ms\n",
      "Tokens per second [15.9] input tokens [368] + xml response tokens [30] = total tokens i/o [398]\n",
      "Response: [<response><command>agent router go to calendar</command><args></args></response>]\n",
      "\n",
      "Processing call [764] out of [1000] = [76.4%]... ETA mm:ss 9:15\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,855 ms\n",
      "Tokens per second [15.8] input tokens [614] + xml response tokens [45] = total tokens i/o [659]\n",
      "Response: [<response><command>search kagi new tab</command><args>Voice Recognition technologies: What are the latest advancements in voice recognition technology?</args></response>]\n",
      "\n",
      "Processing call [765] out of [1000] = [76.5%]... ETA mm:ss 9:12\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,416 ms\n",
      "Tokens per second [15.7] input tokens [607] + xml response tokens [38] = total tokens i/o [645]\n",
      "Response: [<response><command>go to new tab</command><args>www.fantasticjellyfish.net</args></response>]\n",
      "\n",
      "Processing call [766] out of [1000] = [76.6%]... ETA mm:ss 9:10\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,291 ms\n",
      "Tokens per second [15.7] input tokens [608] + xml response tokens [36] = total tokens i/o [644]\n",
      "Response: [<response><command>search phind new tab</command><args>how to make pizza dough</args></response>]\n",
      "\n",
      "Processing call [767] out of [1000] = [76.7%]... ETA mm:ss 9:08\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,386 ms\n",
      "Tokens per second [15.9] input tokens [373] + xml response tokens [38] = total tokens i/o [411]\n",
      "Response: [<response><command>agent router go to date and time</command><args>Seoul, South Korea</args></response>]\n",
      "\n",
      "Processing call [768] out of [1000] = [76.8%]... ETA mm:ss 9:05\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,197 ms\n",
      "Tokens per second [15.9] input tokens [368] + xml response tokens [35] = total tokens i/o [403]\n",
      "Response: [<response><command>agent router go to receptionist</command><args>Service Desk</args></response>]\n",
      "\n",
      "Processing call [769] out of [1000] = [76.9%]... ETA mm:ss 9:03\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,355 ms\n",
      "Tokens per second [15.7] input tokens [603] + xml response tokens [37] = total tokens i/o [640]\n",
      "Response: [<response><command>search kagi new tab</command><args>JavaScript libraries for beginners</args></response>]\n",
      "\n",
      "Processing call [770] out of [1000] = [77.0%]... ETA mm:ss 9:01\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,356 ms\n",
      "Tokens per second [15.7] input tokens [601] + xml response tokens [37] = total tokens i/o [638]\n",
      "Response: [<response><command>go to current tab</command><args>login.incrediblexylophone.info</args></response>]\n",
      "\n",
      "Processing call [771] out of [1000] = [77.1%]... ETA mm:ss 8:58\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,893 ms\n",
      "Tokens per second [15.8] input tokens [415] + xml response tokens [30] = total tokens i/o [445]\n",
      "Response: [<response><command>agent router go to calendar</command><args></args></response>]\n",
      "\n",
      "Processing call [772] out of [1000] = [77.2%]... ETA mm:ss 8:56\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,983 ms\n",
      "Tokens per second [15.8] input tokens [615] + xml response tokens [47] = total tokens i/o [662]\n",
      "Response: [<response><command>search new tab</command><args>What are the causes of translation errors with Unicode characters in Python and how can they be fixed?</args></response>]\n",
      "\n",
      "Processing call [773] out of [1000] = [77.3%]... ETA mm:ss 8:54\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,166 ms\n",
      "Tokens per second [15.7] input tokens [602] + xml response tokens [34] = total tokens i/o [636]\n",
      "Response: [<response><command>search new tab</command><args>Pandas DataFrame creation</args></response>]\n",
      "\n",
      "Processing call [774] out of [1000] = [77.4%]... ETA mm:ss 8:51\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,142 ms\n",
      "Tokens per second [15.9] input tokens [363] + xml response tokens [34] = total tokens i/o [397]\n",
      "Response: [<response><command>agent router go to receptionist</command><args>Service Agent</args></response>]\n",
      "\n",
      "Processing call [775] out of [1000] = [77.5%]... ETA mm:ss 8:49\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,669 ms\n",
      "Tokens per second [15.7] input tokens [614] + xml response tokens [42] = total tokens i/o [656]\n",
      "Response: [<response><command>search google new tab</command><args>File Not Found Error: File not found or path is incorrect</args></response>]\n",
      "\n",
      "Processing call [776] out of [1000] = [77.6%]... ETA mm:ss 8:46\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,041 ms\n",
      "Tokens per second [15.7] input tokens [600] + xml response tokens [32] = total tokens i/o [632]\n",
      "Response: [<response><command>search kagi using clipboard current tab</command><args></args></response>]\n",
      "\n",
      "Processing call [777] out of [1000] = [77.7%]... ETA mm:ss 8:44\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,352 ms\n",
      "Tokens per second [15.7] input tokens [607] + xml response tokens [37] = total tokens i/o [644]\n",
      "Response: [<response><command>search kagi new tab</command><args>why do dogs wag their tail</args></response>]\n",
      "\n",
      "Processing call [778] out of [1000] = [77.8%]... ETA mm:ss 8:42\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,291 ms\n",
      "Tokens per second [15.7] input tokens [603] + xml response tokens [36] = total tokens i/o [639]\n",
      "Response: [<response><command>search google current tab</command><args>Cross-tabulation in Pandas</args></response>]\n",
      "\n",
      "Processing call [779] out of [1000] = [77.9%]... ETA mm:ss 8:39\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,885 ms\n",
      "Tokens per second [15.9] input tokens [381] + xml response tokens [30] = total tokens i/o [411]\n",
      "Response: [<response><command>agent router go to calendar</command><args></args></response>]\n",
      "\n",
      "Processing call [780] out of [1000] = [78.0%]... ETA mm:ss 8:37\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,980 ms\n",
      "Tokens per second [15.7] input tokens [598] + xml response tokens [31] = total tokens i/o [629]\n",
      "Response: [<response><command>search current tab</command><args>Reference Error</args></response>]\n",
      "\n",
      "Processing call [781] out of [1000] = [78.1%]... ETA mm:ss 8:34\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,727 ms\n",
      "Tokens per second [15.8] input tokens [613] + xml response tokens [43] = total tokens i/o [656]\n",
      "Response: [<response><command>search perplexity current tab</command><args>How can system exit errors be handled gracefully in Python applications?</args></response>]\n",
      "\n",
      "Processing call [782] out of [1000] = [78.2%]... ETA mm:ss 8:32\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,416 ms\n",
      "Tokens per second [15.7] input tokens [605] + xml response tokens [38] = total tokens i/o [643]\n",
      "Response: [<response><command>go to new tab</command><args>stage.excitingtornado.info</args></response>]\n",
      "\n",
      "Processing call [783] out of [1000] = [78.3%]... ETA mm:ss 8:30\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,219 ms\n",
      "Tokens per second [15.8] input tokens [605] + xml response tokens [35] = total tokens i/o [640]\n",
      "Response: [<response><command>search phind new tab</command><args>how to improve memory</args></response>]\n",
      "\n",
      "Processing call [784] out of [1000] = [78.4%]... ETA mm:ss 8:27\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,342 ms\n",
      "Tokens per second [15.8] input tokens [604] + xml response tokens [37] = total tokens i/o [641]\n",
      "Response: [<response><command>go to current tab</command><args>mail.spectacularwalrus.info</args></response>]\n",
      "\n",
      "Processing call [785] out of [1000] = [78.5%]... ETA mm:ss 8:25\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,875 ms\n",
      "Tokens per second [16.0] input tokens [367] + xml response tokens [30] = total tokens i/o [397]\n",
      "Response: [<response><command>agent router go to calendar</command><args></args></response>]\n",
      "\n",
      "Processing call [786] out of [1000] = [78.6%]... ETA mm:ss 8:23\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,156 ms\n",
      "Tokens per second [15.8] input tokens [606] + xml response tokens [34] = total tokens i/o [640]\n",
      "Response: [<response><command>search phind new tab</command><args>AssertionError</args></response>]\n",
      "\n",
      "Processing call [787] out of [1000] = [78.7%]... ETA mm:ss 8:20\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,128 ms\n",
      "Tokens per second [16.0] input tokens [389] + xml response tokens [34] = total tokens i/o [423]\n",
      "Response: [<response><command>agent router go to receptionist</command><args>Visitor Coordinator</args></response>]\n",
      "\n",
      "Processing call [788] out of [1000] = [78.8%]... ETA mm:ss 8:18\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,341 ms\n",
      "Tokens per second [15.8] input tokens [607] + xml response tokens [37] = total tokens i/o [644]\n",
      "Response: [<response><command>go to new tab</command><args>dev.beautifulunicorn.com</args></response>]\n",
      "\n",
      "Processing call [789] out of [1000] = [78.9%]... ETA mm:ss 8:15\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,030 ms\n",
      "Tokens per second [15.8] input tokens [604] + xml response tokens [32] = total tokens i/o [636]\n",
      "Response: [<response><command>search kagi using clipboard new tab</command><args></args></response>]\n",
      "\n",
      "Processing call [790] out of [1000] = [79.0%]... ETA mm:ss 8:13\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,625 ms\n",
      "Tokens per second [16.0] input tokens [359] + xml response tokens [26] = total tokens i/o [385]\n",
      "Response: [<response><command>none</command><args></args></response>]\n",
      "\n",
      "Processing call [791] out of [1000] = [79.1%]... ETA mm:ss 8:10\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,343 ms\n",
      "Tokens per second [15.8] input tokens [605] + xml response tokens [37] = total tokens i/o [642]\n",
      "Response: [<response><command>search phind current tab</command><args>Cross-tabulation in Pandas</args></response>]\n",
      "\n",
      "Processing call [792] out of [1000] = [79.2%]... ETA mm:ss 8:08\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,157 ms\n",
      "Tokens per second [15.8] input tokens [602] + xml response tokens [34] = total tokens i/o [636]\n",
      "Response: [<response><command>search phind current tab</command><args>best online games</args></response>]\n",
      "\n",
      "Processing call [793] out of [1000] = [79.3%]... ETA mm:ss 8:06\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,092 ms\n",
      "Tokens per second [15.8] input tokens [602] + xml response tokens [33] = total tokens i/o [635]\n",
      "Response: [<response><command>search google current tab</command><args>AI transparency initiatives</args></response>]\n",
      "\n",
      "Processing call [794] out of [1000] = [79.4%]... ETA mm:ss 8:03\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,342 ms\n",
      "Tokens per second [15.8] input tokens [605] + xml response tokens [37] = total tokens i/o [642]\n",
      "Response: [<response><command>search phind current tab</command><args>OS Error: Operating system error</args></response>]\n",
      "\n",
      "Processing call [795] out of [1000] = [79.5%]... ETA mm:ss 8:01\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,347 ms\n",
      "Tokens per second [15.8] input tokens [606] + xml response tokens [37] = total tokens i/o [643]\n",
      "Response: [<response><command>go to new tab</command><args>login.fantasticxylophone.org</args></response>]\n",
      "\n",
      "Processing call [796] out of [1000] = [79.6%]... ETA mm:ss 7:59\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,596 ms\n",
      "Tokens per second [15.8] input tokens [611] + xml response tokens [41] = total tokens i/o [652]\n",
      "Response: [<response><command>search new tab</command><args>How can system exit errors be handled gracefully in Python applications?</args></response>]\n",
      "\n",
      "Processing call [797] out of [1000] = [79.7%]... ETA mm:ss 7:56\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,032 ms\n",
      "Tokens per second [15.7] input tokens [601] + xml response tokens [32] = total tokens i/o [633]\n",
      "Response: [<response><command>search google scholar using clipboard current tab</command><args></args></response>]\n",
      "\n",
      "Processing call [798] out of [1000] = [79.8%]... ETA mm:ss 7:54\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,153 ms\n",
      "Tokens per second [15.8] input tokens [601] + xml response tokens [34] = total tokens i/o [635]\n",
      "Response: [<response><command>search google scholar current tab</command><args>best online games</args></response>]\n",
      "\n",
      "Processing call [799] out of [1000] = [79.9%]... ETA mm:ss 7:51\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,277 ms\n",
      "Tokens per second [15.8] input tokens [604] + xml response tokens [36] = total tokens i/o [640]\n",
      "Response: [<response><command>search google current tab</command><args>OS Error: Operating system error</args></response>]\n",
      "\n",
      "Processing call [800] out of [1000] = [80.0%]... ETA mm:ss 7:49\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,400 ms\n",
      "Tokens per second [15.8] input tokens [606] + xml response tokens [38] = total tokens i/o [644]\n",
      "Response: [<response><command>search kagi current tab</command><args>Type Error: Incorrect type comparison</args></response>]\n",
      "\n",
      "Processing call [801] out of [1000] = [80.1%]... ETA mm:ss 7:47\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,966 ms\n",
      "Tokens per second [15.8] input tokens [605] + xml response tokens [31] = total tokens i/o [636]\n",
      "Response: [<response><command>search google using clipboard new tab</command><args></args></response>]\n",
      "\n",
      "Processing call [802] out of [1000] = [80.2%]... ETA mm:ss 7:44\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,469 ms\n",
      "Tokens per second [15.8] input tokens [611] + xml response tokens [39] = total tokens i/o [650]\n",
      "Response: [<response><command>search phind new tab</command><args>Setting and resetting index in Pandas</args></response>]\n",
      "\n",
      "Processing call [803] out of [1000] = [80.3%]... ETA mm:ss 7:42\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,779 ms\n",
      "Tokens per second [15.8] input tokens [611] + xml response tokens [44] = total tokens i/o [655]\n",
      "Response: [<response><command>search phind new tab</command><args>Deep Learning optimization: What strategies are used for optimizing deep learning models?</args></response>]\n",
      "\n",
      "Processing call [804] out of [1000] = [80.4%]... ETA mm:ss 7:40\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,158 ms\n",
      "Tokens per second [15.8] input tokens [607] + xml response tokens [34] = total tokens i/o [641]\n",
      "Response: [<response><command>search perplexity new tab</command><args>learning Japanese online</args></response>]\n",
      "\n",
      "Processing call [805] out of [1000] = [80.5%]... ETA mm:ss 7:37\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,312 ms\n",
      "Tokens per second [16.0] input tokens [374] + xml response tokens [37] = total tokens i/o [411]\n",
      "Response: [<response><command>agent router go to date and time</command><args>Madison, Wisconsin</args></response>]\n",
      "\n",
      "Processing call [806] out of [1000] = [80.6%]... ETA mm:ss 7:35\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,093 ms\n",
      "Tokens per second [15.8] input tokens [604] + xml response tokens [33] = total tokens i/o [637]\n",
      "Response: [<response><command>search perplexity new tab</command><args>Runtime Error</args></response>]\n",
      "\n",
      "Processing call [807] out of [1000] = [80.7%]... ETA mm:ss 7:33\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,313 ms\n",
      "Tokens per second [16.0] input tokens [371] + xml response tokens [37] = total tokens i/o [408]\n",
      "Response: [<response><command>agent router go to date and time</command><args>Jakarta, Indonesia</args></response>]\n",
      "\n",
      "Processing call [808] out of [1000] = [80.8%]... ETA mm:ss 7:30\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,157 ms\n",
      "Tokens per second [15.8] input tokens [603] + xml response tokens [34] = total tokens i/o [637]\n",
      "Response: [<response><command>search current tab</command><args>how to make pizza dough</args></response>]\n",
      "\n",
      "Processing call [809] out of [1000] = [80.9%]... ETA mm:ss 7:28\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,249 ms\n",
      "Tokens per second [16.0] input tokens [364] + xml response tokens [36] = total tokens i/o [400]\n",
      "Response: [<response><command>agent router go to receptionist</command><args>Help Desk Agent</args></response>]\n",
      "\n",
      "Processing call [810] out of [1000] = [81.0%]... ETA mm:ss 7:25\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,871 ms\n",
      "Tokens per second [16.0] input tokens [369] + xml response tokens [30] = total tokens i/o [399]\n",
      "Response: [<response><command>agent router go to calendar</command><args></args></response>]\n",
      "\n",
      "Processing call [811] out of [1000] = [81.1%]... ETA mm:ss 7:23\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,711 ms\n",
      "Tokens per second [15.9] input tokens [611] + xml response tokens [43] = total tokens i/o [654]\n",
      "Response: [<response><command>search google current tab</command><args>Responsible AI practices: How can organizations implement responsible AI practices?</args></response>]\n",
      "\n",
      "Processing call [812] out of [1000] = [81.2%]... ETA mm:ss 7:21\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,245 ms\n",
      "Tokens per second [16.0] input tokens [381] + xml response tokens [36] = total tokens i/o [417]\n",
      "Response: [<response><command>agent router go to weather</command><args>Seoul, South Korea</args></response>]\n",
      "\n",
      "Processing call [813] out of [1000] = [81.3%]... ETA mm:ss 7:18\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 3,023 ms\n",
      "Tokens per second [15.9] input tokens [614] + xml response tokens [48] = total tokens i/o [662]\n",
      "Response: [<response><command>search kagi current tab</command><args>Neural Network hyperparameter tuning: What approaches are recommended for neural network hyperparameter tuning?</args></response>]\n",
      "\n",
      "Processing call [814] out of [1000] = [81.4%]... ETA mm:ss 7:16\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,408 ms\n",
      "Tokens per second [15.8] input tokens [610] + xml response tokens [38] = total tokens i/o [648]\n",
      "Response: [<response><command>search perplexity new tab</command><args>System Error: Internal Python system issue</args></response>]\n",
      "\n",
      "Processing call [815] out of [1000] = [81.5%]... ETA mm:ss 7:14\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,875 ms\n",
      "Tokens per second [16.0] input tokens [375] + xml response tokens [30] = total tokens i/o [405]\n",
      "Response: [<response><command>agent router go to calendar</command><args></args></response>]\n",
      "\n",
      "Processing call [816] out of [1000] = [81.6%]... ETA mm:ss 7:11\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,597 ms\n",
      "Tokens per second [15.8] input tokens [618] + xml response tokens [41] = total tokens i/o [659]\n",
      "Response: [<response><command>search google new tab</command><args>AI in gaming: How is AI changing the gaming industry?</args></response>]\n",
      "\n",
      "Processing call [817] out of [1000] = [81.7%]... ETA mm:ss 7:09\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,889 ms\n",
      "Tokens per second [15.9] input tokens [406] + xml response tokens [30] = total tokens i/o [436]\n",
      "Response: [<response><command>agent router go to calendar</command><args></args></response>]\n",
      "\n",
      "Processing call [818] out of [1000] = [81.8%]... ETA mm:ss 7:07\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,380 ms\n",
      "Tokens per second [16.0] input tokens [365] + xml response tokens [38] = total tokens i/o [403]\n",
      "Response: [<response><command>agent router go to date and time</command><args>Tulsa, Oklahoma</args></response>]\n",
      "\n",
      "Processing call [819] out of [1000] = [81.9%]... ETA mm:ss 7:04\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,218 ms\n",
      "Tokens per second [15.8] input tokens [600] + xml response tokens [35] = total tokens i/o [635]\n",
      "Response: [<response><command>search google scholar current tab</command><args>URLError</args></response>]\n",
      "\n",
      "Processing call [820] out of [1000] = [82.0%]... ETA mm:ss 7:02\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,407 ms\n",
      "Tokens per second [15.8] input tokens [606] + xml response tokens [38] = total tokens i/o [644]\n",
      "Response: [<response><command>search kagi current tab</command><args>Using Pandas for ETL processes</args></response>]\n",
      "\n",
      "Processing call [821] out of [1000] = [82.1%]... ETA mm:ss 7:00\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,246 ms\n",
      "Tokens per second [16.0] input tokens [369] + xml response tokens [36] = total tokens i/o [405]\n",
      "Response: [<response><command>agent router go to weather</command><args>Bogota, Colombia</args></response>]\n",
      "\n",
      "Processing call [822] out of [1000] = [82.2%]... ETA mm:ss 6:57\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,407 ms\n",
      "Tokens per second [15.8] input tokens [606] + xml response tokens [38] = total tokens i/o [644]\n",
      "Response: [<response><command>search kagi current tab</command><args>Saving Pandas DataFrame to CSV</args></response>]\n",
      "\n",
      "Processing call [823] out of [1000] = [82.3%]... ETA mm:ss 6:55\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 3,398 ms\n",
      "Tokens per second [15.9] input tokens [620] + xml response tokens [54] = total tokens i/o [674]\n",
      "Response: [<response><command>search google scholar current tab</command><args>Supervised vs. Unsupervised Learning: What are the key differences between supervised and unsupervised learning in AI?</args></response>]\n",
      "\n",
      "Processing call [824] out of [1000] = [82.4%]... ETA mm:ss 6:53\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,035 ms\n",
      "Tokens per second [15.7] input tokens [601] + xml response tokens [32] = total tokens i/o [633]\n",
      "Response: [<response><command>search perplexity using clipboard new tab</command><args></args></response>]\n",
      "\n",
      "Processing call [825] out of [1000] = [82.5%]... ETA mm:ss 6:50\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,344 ms\n",
      "Tokens per second [15.8] input tokens [601] + xml response tokens [37] = total tokens i/o [638]\n",
      "Response: [<response><command>search phind current tab</command><args>Data imputation with Pandas</args></response>]\n",
      "\n",
      "Processing call [826] out of [1000] = [82.6%]... ETA mm:ss 6:48\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,346 ms\n",
      "Tokens per second [15.8] input tokens [605] + xml response tokens [37] = total tokens i/o [642]\n",
      "Response: [<response><command>search phind new tab</command><args>OS Error: Operating system error</args></response>]\n",
      "\n",
      "Processing call [827] out of [1000] = [82.7%]... ETA mm:ss 6:46\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,097 ms\n",
      "Tokens per second [15.7] input tokens [600] + xml response tokens [33] = total tokens i/o [633]\n",
      "Response: [<response><command>search perplexity new tab</command><args>Runtime Error</args></response>]\n",
      "\n",
      "Processing call [828] out of [1000] = [82.8%]... ETA mm:ss 6:43\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,310 ms\n",
      "Tokens per second [16.0] input tokens [368] + xml response tokens [37] = total tokens i/o [405]\n",
      "Response: [<response><command>agent router go to date and time</command><args>New Orleans, Louisiana</args></response>]\n",
      "\n",
      "Processing call [829] out of [1000] = [82.9%]... ETA mm:ss 6:41\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,343 ms\n",
      "Tokens per second [15.8] input tokens [603] + xml response tokens [37] = total tokens i/o [640]\n",
      "Response: [<response><command>search google new tab</command><args>Pandas and geospatial data</args></response>]\n",
      "\n",
      "Processing call [830] out of [1000] = [83.0%]... ETA mm:ss 6:39\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,908 ms\n",
      "Tokens per second [15.7] input tokens [608] + xml response tokens [30] = total tokens i/o [638]\n",
      "Response: [<response><command>search using clipboard new tab</command><args></args></response>]\n",
      "\n",
      "Processing call [831] out of [1000] = [83.1%]... ETA mm:ss 6:36\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,027 ms\n",
      "Tokens per second [15.8] input tokens [600] + xml response tokens [32] = total tokens i/o [632]\n",
      "Response: [<response><command>search new tab</command><args>learning Japanese online</args></response>]\n",
      "\n",
      "Processing call [832] out of [1000] = [83.2%]... ETA mm:ss 6:34\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,834 ms\n",
      "Tokens per second [15.9] input tokens [616] + xml response tokens [45] = total tokens i/o [661]\n",
      "Response: [<response><command>search google current tab</command><args>Machine Learning in biotech: How is machine learning applied in biotechnology research?</args></response>]\n",
      "\n",
      "Processing call [833] out of [1000] = [83.3%]... ETA mm:ss 6:31\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,651 ms\n",
      "Tokens per second [15.8] input tokens [610] + xml response tokens [42] = total tokens i/o [652]\n",
      "Response: [<response><command>search google current tab</command><args>File Not Found Error: File not found or path is incorrect</args></response>]\n",
      "\n",
      "Processing call [834] out of [1000] = [83.4%]... ETA mm:ss 6:29\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,337 ms\n",
      "Tokens per second [15.8] input tokens [604] + xml response tokens [37] = total tokens i/o [641]\n",
      "Response: [<response><command>search google scholar current tab</command><args>JavaScript libraries for beginners</args></response>]\n",
      "\n",
      "Processing call [835] out of [1000] = [83.5%]... ETA mm:ss 6:27\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,216 ms\n",
      "Tokens per second [15.8] input tokens [603] + xml response tokens [35] = total tokens i/o [638]\n",
      "Response: [<response><command>search perplexity current tab</command><args>IsADirectoryError</args></response>]\n",
      "\n",
      "Processing call [836] out of [1000] = [83.6%]... ETA mm:ss 6:24\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,339 ms\n",
      "Tokens per second [15.8] input tokens [606] + xml response tokens [37] = total tokens i/o [643]\n",
      "Response: [<response><command>go to new tab</command><args>beta.beautifulvolcano.org</args></response>]\n",
      "\n",
      "Processing call [837] out of [1000] = [83.7%]... ETA mm:ss 6:22\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,625 ms\n",
      "Tokens per second [16.0] input tokens [359] + xml response tokens [26] = total tokens i/o [385]\n",
      "Response: [<response><command>none</command><args></args></response>]\n",
      "\n",
      "Processing call [838] out of [1000] = [83.8%]... ETA mm:ss 6:20\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,407 ms\n",
      "Tokens per second [15.8] input tokens [608] + xml response tokens [38] = total tokens i/o [646]\n",
      "Response: [<response><command>search google scholar current tab</command><args>System Error: Internal Python system issue</args></response>]\n",
      "\n",
      "Processing call [839] out of [1000] = [83.9%]... ETA mm:ss 6:17\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,281 ms\n",
      "Tokens per second [15.8] input tokens [602] + xml response tokens [36] = total tokens i/o [638]\n",
      "Response: [<response><command>search google scholar current tab</command><args>NoSQL databases for AI</args></response>]\n",
      "\n",
      "Processing call [840] out of [1000] = [84.0%]... ETA mm:ss 6:15\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,290 ms\n",
      "Tokens per second [15.7] input tokens [602] + xml response tokens [36] = total tokens i/o [638]\n",
      "Response: [<response><command>search current tab</command><args>Type Error: Incorrect type comparison</args></response>]\n",
      "\n",
      "Processing call [841] out of [1000] = [84.1%]... ETA mm:ss 6:13\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,345 ms\n",
      "Tokens per second [15.8] input tokens [601] + xml response tokens [37] = total tokens i/o [638]\n",
      "Response: [<response><command>go to current tab</command><args>beta.jubilantyogurt.org</args></response>]\n",
      "\n",
      "Processing call [842] out of [1000] = [84.2%]... ETA mm:ss 6:10\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,776 ms\n",
      "Tokens per second [15.9] input tokens [613] + xml response tokens [44] = total tokens i/o [657]\n",
      "Response: [<response><command>search perplexity current tab</command><args>Responsible AI practices: How can organizations implement responsible AI practices?</args></response>]\n",
      "\n",
      "Processing call [843] out of [1000] = [84.3%]... ETA mm:ss 6:08\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,902 ms\n",
      "Tokens per second [15.9] input tokens [616] + xml response tokens [46] = total tokens i/o [662]\n",
      "Response: [<response><command>search kagi current tab</command><args>Machine Learning in biotech: How is machine learning applied in biotechnology research?</args></response>]\n",
      "\n",
      "Processing call [844] out of [1000] = [84.4%]... ETA mm:ss 6:06\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,246 ms\n",
      "Tokens per second [16.0] input tokens [371] + xml response tokens [36] = total tokens i/o [407]\n",
      "Response: [<response><command>agent router go to receptionist</command><args>Front Desk Agent</args></response>]\n",
      "\n",
      "Processing call [845] out of [1000] = [84.5%]... ETA mm:ss 6:03\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,901 ms\n",
      "Tokens per second [15.9] input tokens [615] + xml response tokens [46] = total tokens i/o [661]\n",
      "Response: [<response><command>search current tab</command><args>Normalization vs. Standardization: When should you choose normalization over standardization in data preprocessing?</args></response>]\n",
      "\n",
      "Processing call [846] out of [1000] = [84.6%]... ETA mm:ss 6:01\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,352 ms\n",
      "Tokens per second [15.7] input tokens [606] + xml response tokens [37] = total tokens i/o [643]\n",
      "Response: [<response><command>go to current tab</command><args>stage.jubilantlemur.io</args></response>]\n",
      "\n",
      "Processing call [847] out of [1000] = [84.7%]... ETA mm:ss 5:59\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,909 ms\n",
      "Tokens per second [15.8] input tokens [616] + xml response tokens [46] = total tokens i/o [662]\n",
      "Response: [<response><command>search google scholar new tab</command><args>AI for health diagnostics: How does AI contribute to advancements in health diagnostics?</args></response>]\n",
      "\n",
      "Processing call [848] out of [1000] = [84.8%]... ETA mm:ss 5:56\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,183 ms\n",
      "Tokens per second [16.0] input tokens [377] + xml response tokens [35] = total tokens i/o [412]\n",
      "Response: [<response><command>agent router go to weather</command><args>Honolulu, USA</args></response>]\n",
      "\n",
      "Processing call [849] out of [1000] = [84.9%]... ETA mm:ss 5:54\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,029 ms\n",
      "Tokens per second [15.8] input tokens [602] + xml response tokens [32] = total tokens i/o [634]\n",
      "Response: [<response><command>search google scholar using clipboard new tab</command><args></args></response>]\n",
      "\n",
      "Processing call [850] out of [1000] = [85.0%]... ETA mm:ss 5:52\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,252 ms\n",
      "Tokens per second [16.0] input tokens [365] + xml response tokens [36] = total tokens i/o [401]\n",
      "Response: [<response><command>agent router go to receptionist</command><args>Front Desk Agent</args></response>]\n",
      "\n",
      "Processing call [851] out of [1000] = [85.1%]... ETA mm:ss 5:49\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,312 ms\n",
      "Tokens per second [16.0] input tokens [379] + xml response tokens [37] = total tokens i/o [416]\n",
      "Response: [<response><command>agent router go to date and time</command><args>San Jose, California</args></response>]\n",
      "\n",
      "Processing call [852] out of [1000] = [85.2%]... ETA mm:ss 5:47\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 3,031 ms\n",
      "Tokens per second [15.8] input tokens [618] + xml response tokens [48] = total tokens i/o [666]\n",
      "Response: [<response><command>search kagi current tab</command><args>How do you address deprecation warnings in Python to ensure code compatibility with future versions?</args></response>]\n",
      "\n",
      "Processing call [853] out of [1000] = [85.3%]... ETA mm:ss 5:45\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,969 ms\n",
      "Tokens per second [15.7] input tokens [603] + xml response tokens [31] = total tokens i/o [634]\n",
      "Response: [<response><command>search google using clipboard new tab</command><args></args></response>]\n",
      "\n",
      "Processing call [854] out of [1000] = [85.4%]... ETA mm:ss 5:42\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,870 ms\n",
      "Tokens per second [16.0] input tokens [381] + xml response tokens [30] = total tokens i/o [411]\n",
      "Response: [<response><command>agent router go to calendar</command><args></args></response>]\n",
      "\n",
      "Processing call [855] out of [1000] = [85.5%]... ETA mm:ss 5:40\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,183 ms\n",
      "Tokens per second [16.0] input tokens [370] + xml response tokens [35] = total tokens i/o [405]\n",
      "Response: [<response><command>agent router go to receptionist</command><args>Concierge</args></response>]\n",
      "\n",
      "Processing call [856] out of [1000] = [85.6%]... ETA mm:ss 5:38\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,871 ms\n",
      "Tokens per second [16.0] input tokens [370] + xml response tokens [30] = total tokens i/o [400]\n",
      "Response: [<response><command>agent router go to calendar</command><args></args></response>]\n",
      "\n",
      "Processing call [857] out of [1000] = [85.7%]... ETA mm:ss 5:35\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,219 ms\n",
      "Tokens per second [15.8] input tokens [602] + xml response tokens [35] = total tokens i/o [637]\n",
      "Response: [<response><command>search google current tab</command><args>how to make pizza dough</args></response>]\n",
      "\n",
      "Processing call [858] out of [1000] = [85.8%]... ETA mm:ss 5:33\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,280 ms\n",
      "Tokens per second [15.8] input tokens [609] + xml response tokens [36] = total tokens i/o [645]\n",
      "Response: [<response><command>search google new tab</command><args>Working with datetime in Pandas</args></response>]\n",
      "\n",
      "Processing call [859] out of [1000] = [85.9%]... ETA mm:ss 5:30\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,844 ms\n",
      "Tokens per second [15.8] input tokens [613] + xml response tokens [45] = total tokens i/o [658]\n",
      "Response: [<response><command>search google scholar current tab</command><args>How do you handle broken pipe errors in Python, especially in network communications?</args></response>]\n",
      "\n",
      "Processing call [860] out of [1000] = [86.0%]... ETA mm:ss 5:28\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,787 ms\n",
      "Tokens per second [15.8] input tokens [615] + xml response tokens [44] = total tokens i/o [659]\n",
      "Response: [<response><command>search phind new tab</command><args>Deep Learning optimization: What strategies are used for optimizing deep learning models?</args></response>]\n",
      "\n",
      "Processing call [861] out of [1000] = [86.1%]... ETA mm:ss 5:26\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,096 ms\n",
      "Tokens per second [15.7] input tokens [602] + xml response tokens [33] = total tokens i/o [635]\n",
      "Response: [<response><command>search google current tab</command><args>best online games</args></response>]\n",
      "\n",
      "Processing call [862] out of [1000] = [86.2%]... ETA mm:ss 5:23\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,098 ms\n",
      "Tokens per second [15.7] input tokens [607] + xml response tokens [33] = total tokens i/o [640]\n",
      "Response: [<response><command>search perplexity new tab</command><args>Memory Error</args></response>]\n",
      "\n",
      "Processing call [863] out of [1000] = [86.3%]... ETA mm:ss 5:21\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,220 ms\n",
      "Tokens per second [15.8] input tokens [601] + xml response tokens [35] = total tokens i/o [636]\n",
      "Response: [<response><command>search perplexity current tab</command><args>URLError</args></response>]\n",
      "\n",
      "Processing call [864] out of [1000] = [86.4%]... ETA mm:ss 5:19\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,029 ms\n",
      "Tokens per second [15.8] input tokens [602] + xml response tokens [32] = total tokens i/o [634]\n",
      "Response: [<response><command>search kagi using clipboard new tab</command><args></args></response>]\n",
      "\n",
      "Processing call [865] out of [1000] = [86.5%]... ETA mm:ss 5:16\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,906 ms\n",
      "Tokens per second [15.8] input tokens [616] + xml response tokens [46] = total tokens i/o [662]\n",
      "Response: [<response><command>search phind current tab</command><args>Machine Learning in biotech: How is machine learning applied in biotechnology research?</args></response>]\n",
      "\n",
      "Processing call [866] out of [1000] = [86.6%]... ETA mm:ss 5:14\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 3,030 ms\n",
      "Tokens per second [15.8] input tokens [619] + xml response tokens [48] = total tokens i/o [667]\n",
      "Response: [<response><command>search phind new tab</command><args>How do you address deprecation warnings in Python to ensure code compatibility with future versions?</args></response>]\n",
      "\n",
      "Processing call [867] out of [1000] = [86.7%]... ETA mm:ss 5:12\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,776 ms\n",
      "Tokens per second [15.9] input tokens [613] + xml response tokens [44] = total tokens i/o [657]\n",
      "Response: [<response><command>search perplexity current tab</command><args>Responsible AI practices: How can organizations implement responsible AI practices?</args></response>]\n",
      "\n",
      "Processing call [868] out of [1000] = [86.8%]... ETA mm:ss 5:10\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,094 ms\n",
      "Tokens per second [15.8] input tokens [605] + xml response tokens [33] = total tokens i/o [638]\n",
      "Response: [<response><command>search kagi new tab</command><args>Connection Error</args></response>]\n",
      "\n",
      "Processing call [869] out of [1000] = [86.9%]... ETA mm:ss 5:07\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,468 ms\n",
      "Tokens per second [15.8] input tokens [609] + xml response tokens [39] = total tokens i/o [648]\n",
      "Response: [<response><command>search google scholar new tab</command><args>Deprecation Warning: Feature is deprecated</args></response>]\n",
      "\n",
      "Processing call [870] out of [1000] = [87.0%]... ETA mm:ss 5:05\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,309 ms\n",
      "Tokens per second [16.0] input tokens [384] + xml response tokens [37] = total tokens i/o [421]\n",
      "Response: [<response><command>agent router go to date and time</command><args>Fort Wayne, Indiana</args></response>]\n",
      "\n",
      "Processing call [871] out of [1000] = [87.1%]... ETA mm:ss 5:02\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,876 ms\n",
      "Tokens per second [16.0] input tokens [365] + xml response tokens [30] = total tokens i/o [395]\n",
      "Response: [<response><command>agent router go to calendar</command><args></args></response>]\n",
      "\n",
      "Processing call [872] out of [1000] = [87.2%]... ETA mm:ss 5:00\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,341 ms\n",
      "Tokens per second [15.8] input tokens [604] + xml response tokens [37] = total tokens i/o [641]\n",
      "Response: [<response><command>search kagi new tab</command><args>OS Error: Operating system error</args></response>]\n",
      "\n",
      "Processing call [873] out of [1000] = [87.3%]... ETA mm:ss 4:58\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,095 ms\n",
      "Tokens per second [15.8] input tokens [598] + xml response tokens [33] = total tokens i/o [631]\n",
      "Response: [<response><command>search google scholar current tab</command><args>EnvironmentError</args></response>]\n",
      "\n",
      "Processing call [874] out of [1000] = [87.4%]... ETA mm:ss 4:55\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,592 ms\n",
      "Tokens per second [15.8] input tokens [608] + xml response tokens [41] = total tokens i/o [649]\n",
      "Response: [<response><command>search current tab</command><args>How do you fix indentation errors that affect code structure in Python?</args></response>]\n",
      "\n",
      "Processing call [875] out of [1000] = [87.5%]... ETA mm:ss 4:53\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,653 ms\n",
      "Tokens per second [15.7] input tokens [601] + xml response tokens [26] = total tokens i/o [627]\n",
      "Response: [<response><command>none</command><args></args></response>]\n",
      "\n",
      "Processing call [876] out of [1000] = [87.6%]... ETA mm:ss 4:51\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,967 ms\n",
      "Tokens per second [15.8] input tokens [615] + xml response tokens [31] = total tokens i/o [646]\n",
      "Response: [<response><command>search google using clipboard new tab</command><args></args></response>]\n",
      "\n",
      "Processing call [877] out of [1000] = [87.7%]... ETA mm:ss 4:48\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,277 ms\n",
      "Tokens per second [15.8] input tokens [607] + xml response tokens [36] = total tokens i/o [643]\n",
      "Response: [<response><command>search kagi new tab</command><args>AI for customer service automation</args></response>]\n",
      "\n",
      "Processing call [878] out of [1000] = [87.8%]... ETA mm:ss 4:46\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,250 ms\n",
      "Tokens per second [16.0] input tokens [367] + xml response tokens [36] = total tokens i/o [403]\n",
      "Response: [<response><command>agent router go to weather</command><args>Irvine, California</args></response>]\n",
      "\n",
      "Processing call [879] out of [1000] = [87.9%]... ETA mm:ss 4:43\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,093 ms\n",
      "Tokens per second [15.8] input tokens [602] + xml response tokens [33] = total tokens i/o [635]\n",
      "Response: [<response><command>search google current tab</command><args>Broken Pipe Error</args></response>]\n",
      "\n",
      "Processing call [880] out of [1000] = [88.0%]... ETA mm:ss 4:41\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,467 ms\n",
      "Tokens per second [15.8] input tokens [606] + xml response tokens [39] = total tokens i/o [645]\n",
      "Response: [<response><command>search phind current tab</command><args>Deprecation Warning: Feature is deprecated</args></response>]\n",
      "\n",
      "Processing call [881] out of [1000] = [88.1%]... ETA mm:ss 4:39\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,031 ms\n",
      "Tokens per second [15.8] input tokens [606] + xml response tokens [32] = total tokens i/o [638]\n",
      "Response: [<response><command>search google scholar using clipboard current tab</command><args></args></response>]\n",
      "\n",
      "Processing call [882] out of [1000] = [88.2%]... ETA mm:ss 4:36\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,380 ms\n",
      "Tokens per second [16.0] input tokens [367] + xml response tokens [38] = total tokens i/o [405]\n",
      "Response: [<response><command>agent router go to date and time</command><args>Baton Rouge, Louisiana</args></response>]\n",
      "\n",
      "Processing call [883] out of [1000] = [88.3%]... ETA mm:ss 4:34\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,316 ms\n",
      "Tokens per second [16.0] input tokens [367] + xml response tokens [37] = total tokens i/o [404]\n",
      "Response: [<response><command>agent router go to receptionist</command><args>Front Desk Clerk Agent</args></response>]\n",
      "\n",
      "Processing call [884] out of [1000] = [88.4%]... ETA mm:ss 4:32\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,913 ms\n",
      "Tokens per second [15.8] input tokens [619] + xml response tokens [46] = total tokens i/o [665]\n",
      "Response: [<response><command>search google new tab</command><args>Feature Engineering best practices: How does feature engineering improve the performance of machine learning models?</args></response>]\n",
      "\n",
      "Processing call [885] out of [1000] = [88.5%]... ETA mm:ss 4:29\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,030 ms\n",
      "Tokens per second [15.8] input tokens [599] + xml response tokens [32] = total tokens i/o [631]\n",
      "Response: [<response><command>search current tab</command><args>Broken Pipe Error</args></response>]\n",
      "\n",
      "Processing call [886] out of [1000] = [88.6%]... ETA mm:ss 4:27\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,842 ms\n",
      "Tokens per second [15.8] input tokens [615] + xml response tokens [45] = total tokens i/o [660]\n",
      "Response: [<response><command>search google current tab</command><args>Quantum AI computing developments: What are the latest developments in quantum AI computing?</args></response>]\n",
      "\n",
      "Processing call [887] out of [1000] = [88.7%]... ETA mm:ss 4:25\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,216 ms\n",
      "Tokens per second [15.8] input tokens [605] + xml response tokens [35] = total tokens i/o [640]\n",
      "Response: [<response><command>search kagi new tab</command><args>IBM Watson AI tools</args></response>]\n",
      "\n",
      "Processing call [888] out of [1000] = [88.8%]... ETA mm:ss 4:22\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,306 ms\n",
      "Tokens per second [16.0] input tokens [368] + xml response tokens [37] = total tokens i/o [405]\n",
      "Response: [<response><command>agent router go to date and time</command><args>Long Beach, California</args></response>]\n",
      "\n",
      "Processing call [889] out of [1000] = [88.9%]... ETA mm:ss 4:20\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,212 ms\n",
      "Tokens per second [15.8] input tokens [603] + xml response tokens [35] = total tokens i/o [638]\n",
      "Response: [<response><command>search perplexity current tab</command><args>FileNotFoundError</args></response>]\n",
      "\n",
      "Processing call [890] out of [1000] = [89.0%]... ETA mm:ss 4:18\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,222 ms\n",
      "Tokens per second [15.8] input tokens [608] + xml response tokens [35] = total tokens i/o [643]\n",
      "Response: [<response><command>search google new tab</command><args>how to tie a tie</args></response>]\n",
      "\n",
      "Processing call [891] out of [1000] = [89.1%]... ETA mm:ss 4:15\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,404 ms\n",
      "Tokens per second [15.8] input tokens [611] + xml response tokens [38] = total tokens i/o [649]\n",
      "Response: [<response><command>search google scholar new tab</command><args>Type Error: Incorrect type comparison</args></response>]\n",
      "\n",
      "Processing call [892] out of [1000] = [89.2%]... ETA mm:ss 4:13\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,344 ms\n",
      "Tokens per second [15.8] input tokens [601] + xml response tokens [37] = total tokens i/o [638]\n",
      "Response: [<response><command>go to current tab</command><args>prod.jubilantlemur.info</args></response>]\n",
      "\n",
      "Processing call [893] out of [1000] = [89.3%]... ETA mm:ss 4:11\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,246 ms\n",
      "Tokens per second [16.0] input tokens [369] + xml response tokens [36] = total tokens i/o [405]\n",
      "Response: [<response><command>agent router go to weather</command><args>Warsaw, Poland</args></response>]\n",
      "\n",
      "Processing call [894] out of [1000] = [89.4%]... ETA mm:ss 4:08\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,900 ms\n",
      "Tokens per second [15.9] input tokens [614] + xml response tokens [46] = total tokens i/o [660]\n",
      "Response: [<response><command>search kagi current tab</command><args>AI for health diagnostics: How does AI contribute to advancements in health diagnostics?</args></response>]\n",
      "\n",
      "Processing call [895] out of [1000] = [89.5%]... ETA mm:ss 4:06\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,343 ms\n",
      "Tokens per second [15.8] input tokens [605] + xml response tokens [37] = total tokens i/o [642]\n",
      "Response: [<response><command>go to new tab</command><args>prod.fantasticunicorn.net</args></response>]\n",
      "\n",
      "Processing call [896] out of [1000] = [89.6%]... ETA mm:ss 4:04\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,347 ms\n",
      "Tokens per second [15.8] input tokens [602] + xml response tokens [37] = total tokens i/o [639]\n",
      "Response: [<response><command>search google current tab</command><args>Pandas and geospatial data</args></response>]\n",
      "\n",
      "Processing call [897] out of [1000] = [89.7%]... ETA mm:ss 4:01\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,032 ms\n",
      "Tokens per second [15.7] input tokens [616] + xml response tokens [32] = total tokens i/o [648]\n",
      "Response: [<response><command>search phind using clipboard new tab</command><args></args></response>]\n",
      "\n",
      "Processing call [898] out of [1000] = [89.8%]... ETA mm:ss 3:59\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,089 ms\n",
      "Tokens per second [15.8] input tokens [602] + xml response tokens [33] = total tokens i/o [635]\n",
      "Response: [<response><command>search new tab</command><args>AI in financial forecasting</args></response>]\n",
      "\n",
      "Processing call [899] out of [1000] = [89.9%]... ETA mm:ss 3:56\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,189 ms\n",
      "Tokens per second [16.0] input tokens [374] + xml response tokens [35] = total tokens i/o [409]\n",
      "Response: [<response><command>agent router go to weather</command><args>Los Angeles, California</args></response>]\n",
      "\n",
      "Processing call [900] out of [1000] = [90.0%]... ETA mm:ss 3:54\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,343 ms\n",
      "Tokens per second [15.8] input tokens [605] + xml response tokens [37] = total tokens i/o [642]\n",
      "Response: [<response><command>go to current tab</command><args>www.amazingquartz.gov</args></response>]\n",
      "\n",
      "Processing call [901] out of [1000] = [90.1%]... ETA mm:ss 3:52\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,470 ms\n",
      "Tokens per second [15.8] input tokens [604] + xml response tokens [39] = total tokens i/o [643]\n",
      "Response: [<response><command>search google scholar current tab</command><args>Setting and resetting index in Pandas</args></response>]\n",
      "\n",
      "Processing call [902] out of [1000] = [90.2%]... ETA mm:ss 3:49\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,593 ms\n",
      "Tokens per second [15.8] input tokens [610] + xml response tokens [41] = total tokens i/o [651]\n",
      "Response: [<response><command>search new tab</command><args>How can you resolve errors related to ZIP file operations in Python?</args></response>]\n",
      "\n",
      "Processing call [903] out of [1000] = [90.3%]... ETA mm:ss 3:47\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,717 ms\n",
      "Tokens per second [15.8] input tokens [615] + xml response tokens [43] = total tokens i/o [658]\n",
      "Response: [<response><command>search kagi new tab</command><args>How can system exit errors be handled gracefully in Python applications?</args></response>]\n",
      "\n",
      "Processing call [904] out of [1000] = [90.4%]... ETA mm:ss 3:45\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,406 ms\n",
      "Tokens per second [15.8] input tokens [608] + xml response tokens [38] = total tokens i/o [646]\n",
      "Response: [<response><command>search kagi new tab</command><args>Type Error: Incorrect type comparison</args></response>]\n",
      "\n",
      "Processing call [905] out of [1000] = [90.5%]... ETA mm:ss 3:42\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 3,033 ms\n",
      "Tokens per second [15.8] input tokens [621] + xml response tokens [48] = total tokens i/o [669]\n",
      "Response: [<response><command>search kagi new tab</command><args>Normalization vs. Standardization: When should you choose normalization over standardization in data preprocessing?</args></response>]\n",
      "\n",
      "Processing call [906] out of [1000] = [90.6%]... ETA mm:ss 3:40\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,661 ms\n",
      "Tokens per second [15.8] input tokens [613] + xml response tokens [42] = total tokens i/o [655]\n",
      "Response: [<response><command>search phind new tab</command><args>How can decoding errors with Unicode characters be resolved in Python?</args></response>]\n",
      "\n",
      "Processing call [907] out of [1000] = [90.7%]... ETA mm:ss 3:38\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 3,033 ms\n",
      "Tokens per second [15.8] input tokens [617] + xml response tokens [48] = total tokens i/o [665]\n",
      "Response: [<response><command>search kagi new tab</command><args>Normalization vs. Standardization: When should you choose normalization over standardization in data preprocessing?</args></response>]\n",
      "\n",
      "Processing call [908] out of [1000] = [90.8%]... ETA mm:ss 3:36\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,217 ms\n",
      "Tokens per second [15.8] input tokens [605] + xml response tokens [35] = total tokens i/o [640]\n",
      "Response: [<response><command>search phind current tab</command><args>IsADirectoryError</args></response>]\n",
      "\n",
      "Processing call [909] out of [1000] = [90.9%]... ETA mm:ss 3:33\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,277 ms\n",
      "Tokens per second [15.8] input tokens [606] + xml response tokens [36] = total tokens i/o [642]\n",
      "Response: [<response><command>search google scholar new tab</command><args>Pending Deprecation Warning</args></response>]\n",
      "\n",
      "Processing call [910] out of [1000] = [91.0%]... ETA mm:ss 3:31\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,096 ms\n",
      "Tokens per second [15.7] input tokens [607] + xml response tokens [33] = total tokens i/o [640]\n",
      "Response: [<response><command>search phind new tab</command><args>Runtime Error</args></response>]\n",
      "\n",
      "Processing call [911] out of [1000] = [91.1%]... ETA mm:ss 3:29\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,216 ms\n",
      "Tokens per second [15.8] input tokens [603] + xml response tokens [35] = total tokens i/o [638]\n",
      "Response: [<response><command>search kagi current tab</command><args>IBM Watson AI tools</args></response>]\n",
      "\n",
      "Processing call [912] out of [1000] = [91.2%]... ETA mm:ss 3:26\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,097 ms\n",
      "Tokens per second [15.7] input tokens [602] + xml response tokens [33] = total tokens i/o [635]\n",
      "Response: [<response><command>search current tab</command><args>Connection Refused Error</args></response>]\n",
      "\n",
      "Processing call [913] out of [1000] = [91.3%]... ETA mm:ss 3:24\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,343 ms\n",
      "Tokens per second [15.8] input tokens [603] + xml response tokens [37] = total tokens i/o [640]\n",
      "Response: [<response><command>go to current tab</command><args>prod.jubilantxylophone.org</args></response>]\n",
      "\n",
      "Processing call [914] out of [1000] = [91.4%]... ETA mm:ss 3:21\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,890 ms\n",
      "Tokens per second [15.9] input tokens [415] + xml response tokens [30] = total tokens i/o [445]\n",
      "Response: [<response><command>agent router go to calendar</command><args></args></response>]\n",
      "\n",
      "Processing call [915] out of [1000] = [91.5%]... ETA mm:ss 3:19\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,906 ms\n",
      "Tokens per second [15.8] input tokens [612] + xml response tokens [46] = total tokens i/o [658]\n",
      "Response: [<response><command>search kagi current tab</command><args>Quantum AI computing developments: What are the latest developments in quantum AI computing?</args></response>]\n",
      "\n",
      "Processing call [916] out of [1000] = [91.6%]... ETA mm:ss 3:17\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,405 ms\n",
      "Tokens per second [15.8] input tokens [605] + xml response tokens [38] = total tokens i/o [643]\n",
      "Response: [<response><command>go to current tab</command><args>dev.incrediblestrawberry.info</args></response>]\n",
      "\n",
      "Processing call [917] out of [1000] = [91.7%]... ETA mm:ss 3:14\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,407 ms\n",
      "Tokens per second [15.8] input tokens [604] + xml response tokens [38] = total tokens i/o [642]\n",
      "Response: [<response><command>search kagi current tab</command><args>Pandas and geospatial data</args></response>]\n",
      "\n",
      "Processing call [918] out of [1000] = [91.8%]... ETA mm:ss 3:12\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,218 ms\n",
      "Tokens per second [15.8] input tokens [604] + xml response tokens [35] = total tokens i/o [639]\n",
      "Response: [<response><command>go to current tab</command><args>hilariouswalrus.net</args></response>]\n",
      "\n",
      "Processing call [919] out of [1000] = [91.9%]... ETA mm:ss 3:10\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,904 ms\n",
      "Tokens per second [15.7] input tokens [600] + xml response tokens [30] = total tokens i/o [630]\n",
      "Response: [<response><command>search using clipboard current tab</command><args></args></response>]\n",
      "\n",
      "Processing call [920] out of [1000] = [92.0%]... ETA mm:ss 3:07\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,093 ms\n",
      "Tokens per second [15.8] input tokens [601] + xml response tokens [33] = total tokens i/o [634]\n",
      "Response: [<response><command>search google scholar current tab</command><args>Reference Error</args></response>]\n",
      "\n",
      "Processing call [921] out of [1000] = [92.1%]... ETA mm:ss 3:05\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,279 ms\n",
      "Tokens per second [15.8] input tokens [609] + xml response tokens [36] = total tokens i/o [645]\n",
      "Response: [<response><command>search google new tab</command><args>how to make homemade bread</args></response>]\n",
      "\n",
      "Processing call [922] out of [1000] = [92.2%]... ETA mm:ss 3:03\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,312 ms\n",
      "Tokens per second [16.0] input tokens [373] + xml response tokens [37] = total tokens i/o [410]\n",
      "Response: [<response><command>agent router go to date and time</command><args>Detroit, Michigan</args></response>]\n",
      "\n",
      "Processing call [923] out of [1000] = [92.3%]... ETA mm:ss 3:00\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,216 ms\n",
      "Tokens per second [15.8] input tokens [600] + xml response tokens [35] = total tokens i/o [635]\n",
      "Response: [<response><command>search phind current tab</command><args>Connection Refused Error</args></response>]\n",
      "\n",
      "Processing call [924] out of [1000] = [92.4%]... ETA mm:ss 2:58\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,253 ms\n",
      "Tokens per second [16.0] input tokens [364] + xml response tokens [36] = total tokens i/o [400]\n",
      "Response: [<response><command>agent router go to weather</command><args>Seoul, South Korea</args></response>]\n",
      "\n",
      "Processing call [925] out of [1000] = [92.5%]... ETA mm:ss 2:56\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,341 ms\n",
      "Tokens per second [15.8] input tokens [606] + xml response tokens [37] = total tokens i/o [643]\n",
      "Response: [<response><command>search perplexity new tab</command><args>Reading CSV files in Pandas</args></response>]\n",
      "\n",
      "Processing call [926] out of [1000] = [92.6%]... ETA mm:ss 2:53\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,405 ms\n",
      "Tokens per second [15.8] input tokens [610] + xml response tokens [38] = total tokens i/o [648]\n",
      "Response: [<response><command>search phind new tab</command><args>what is climate change and its effects?</args></response>]\n",
      "\n",
      "Processing call [927] out of [1000] = [92.7%]... ETA mm:ss 2:51\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,219 ms\n",
      "Tokens per second [15.8] input tokens [604] + xml response tokens [35] = total tokens i/o [639]\n",
      "Response: [<response><command>search current tab</command><args>what are the benefits of exercise?</args></response>]\n",
      "\n",
      "Processing call [928] out of [1000] = [92.8%]... ETA mm:ss 2:48\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,192 ms\n",
      "Tokens per second [16.0] input tokens [375] + xml response tokens [35] = total tokens i/o [410]\n",
      "Response: [<response><command>agent router go to receptionist</command><args>Reception Agent</args></response>]\n",
      "\n",
      "Processing call [929] out of [1000] = [92.9%]... ETA mm:ss 2:46\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,094 ms\n",
      "Tokens per second [15.8] input tokens [605] + xml response tokens [33] = total tokens i/o [638]\n",
      "Response: [<response><command>search google scholar current tab</command><args>EnvironmentError</args></response>]\n",
      "\n",
      "Processing call [930] out of [1000] = [93.0%]... ETA mm:ss 2:44\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,341 ms\n",
      "Tokens per second [15.8] input tokens [606] + xml response tokens [37] = total tokens i/o [643]\n",
      "Response: [<response><command>search perplexity current tab</command><args>best vacation spots in the world</args></response>]\n",
      "\n",
      "Processing call [931] out of [1000] = [93.1%]... ETA mm:ss 2:41\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,185 ms\n",
      "Tokens per second [16.0] input tokens [375] + xml response tokens [35] = total tokens i/o [410]\n",
      "Response: [<response><command>agent router go to weather</command><args>Toledo, Ohio</args></response>]\n",
      "\n",
      "Processing call [932] out of [1000] = [93.2%]... ETA mm:ss 2:39\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,218 ms\n",
      "Tokens per second [15.8] input tokens [600] + xml response tokens [35] = total tokens i/o [635]\n",
      "Response: [<response><command>search kagi current tab</command><args>Connection Refused Error</args></response>]\n",
      "\n",
      "Processing call [933] out of [1000] = [93.3%]... ETA mm:ss 2:37\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 3,406 ms\n",
      "Tokens per second [15.9] input tokens [618] + xml response tokens [54] = total tokens i/o [672]\n",
      "Response: [<response><command>search google scholar current tab</command><args>Supervised vs. Unsupervised Learning: What are the key differences between supervised and unsupervised learning in AI?</args></response>]\n",
      "\n",
      "Processing call [934] out of [1000] = [93.4%]... ETA mm:ss 2:34\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 3,038 ms\n",
      "Tokens per second [15.8] input tokens [613] + xml response tokens [48] = total tokens i/o [661]\n",
      "Response: [<response><command>search kagi new tab</command><args>Neural Network hyperparameter tuning: What approaches are recommended for neural network hyperparameter tuning?</args></response>]\n",
      "\n",
      "Processing call [935] out of [1000] = [93.5%]... ETA mm:ss 2:32\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,596 ms\n",
      "Tokens per second [15.8] input tokens [609] + xml response tokens [41] = total tokens i/o [650]\n",
      "Response: [<response><command>search current tab</command><args>How can you resolve errors related to ZIP file operations in Python?</args></response>]\n",
      "\n",
      "Processing call [936] out of [1000] = [93.6%]... ETA mm:ss 2:30\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,844 ms\n",
      "Tokens per second [15.8] input tokens [617] + xml response tokens [45] = total tokens i/o [662]\n",
      "Response: [<response><command>search google new tab</command><args>How do you address runtime warnings in Python, especially those indicating risky runtime behavior?</args></response>]\n",
      "\n",
      "Processing call [937] out of [1000] = [93.7%]... ETA mm:ss 2:27\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,033 ms\n",
      "Tokens per second [15.7] input tokens [601] + xml response tokens [32] = total tokens i/o [633]\n",
      "Response: [<response><command>search new tab</command><args>hip hop music</args></response>]\n",
      "\n",
      "Processing call [938] out of [1000] = [93.8%]... ETA mm:ss 2:25\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,035 ms\n",
      "Tokens per second [15.7] input tokens [604] + xml response tokens [32] = total tokens i/o [636]\n",
      "Response: [<response><command>search phind using clipboard new tab</command><args></args></response>]\n",
      "\n",
      "Processing call [939] out of [1000] = [93.9%]... ETA mm:ss 2:23\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,347 ms\n",
      "Tokens per second [15.8] input tokens [609] + xml response tokens [37] = total tokens i/o [646]\n",
      "Response: [<response><command>search perplexity new tab</command><args>User Warning: User-defined warning</args></response>]\n",
      "\n",
      "Processing call [940] out of [1000] = [94.0%]... ETA mm:ss 2:20\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,345 ms\n",
      "Tokens per second [15.8] input tokens [605] + xml response tokens [37] = total tokens i/o [642]\n",
      "Response: [<response><command>go to new tab</command><args>login.fantasticvolcano.org</args></response>]\n",
      "\n",
      "Processing call [941] out of [1000] = [94.1%]... ETA mm:ss 2:18\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,093 ms\n",
      "Tokens per second [15.8] input tokens [604] + xml response tokens [33] = total tokens i/o [637]\n",
      "Response: [<response><command>search phind new tab</command><args>BufferError</args></response>]\n",
      "\n",
      "Processing call [942] out of [1000] = [94.2%]... ETA mm:ss 2:16\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,468 ms\n",
      "Tokens per second [15.8] input tokens [607] + xml response tokens [39] = total tokens i/o [646]\n",
      "Response: [<response><command>go to new tab</command><args>test.magnificentpenguin.gov</args></response>]\n",
      "\n",
      "Processing call [943] out of [1000] = [94.3%]... ETA mm:ss 2:13\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,157 ms\n",
      "Tokens per second [15.8] input tokens [604] + xml response tokens [34] = total tokens i/o [638]\n",
      "Response: [<response><command>search google current tab</command><args>Connection Refused Error</args></response>]\n",
      "\n",
      "Processing call [944] out of [1000] = [94.4%]... ETA mm:ss 2:11\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 3,030 ms\n",
      "Tokens per second [15.8] input tokens [615] + xml response tokens [48] = total tokens i/o [663]\n",
      "Response: [<response><command>search phind current tab</command><args>Neural Network hyperparameter tuning: What approaches are recommended for neural network hyperparameter tuning?</args></response>]\n",
      "\n",
      "Processing call [945] out of [1000] = [94.5%]... ETA mm:ss 2:09\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,599 ms\n",
      "Tokens per second [15.8] input tokens [610] + xml response tokens [41] = total tokens i/o [651]\n",
      "Response: [<response><command>search new tab</command><args>File Not Found Error: File not found or path is incorrect</args></response>]\n",
      "\n",
      "Processing call [946] out of [1000] = [94.6%]... ETA mm:ss 2:06\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,094 ms\n",
      "Tokens per second [15.8] input tokens [605] + xml response tokens [33] = total tokens i/o [638]\n",
      "Response: [<response><command>search phind new tab</command><args>Reference Error</args></response>]\n",
      "\n",
      "Processing call [947] out of [1000] = [94.7%]... ETA mm:ss 2:04\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 3,031 ms\n",
      "Tokens per second [15.8] input tokens [614] + xml response tokens [48] = total tokens i/o [662]\n",
      "Response: [<response><command>search google scholar current tab</command><args>Sentiment Analysis tools: What tools are widely used for sentiment analysis in social media data?</args></response>]\n",
      "\n",
      "Processing call [948] out of [1000] = [94.8%]... ETA mm:ss 2:02\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,189 ms\n",
      "Tokens per second [16.0] input tokens [369] + xml response tokens [35] = total tokens i/o [404]\n",
      "Response: [<response><command>agent router go to weather</command><args>Seattle, USA</args></response>]\n",
      "\n",
      "Processing call [949] out of [1000] = [94.9%]... ETA mm:ss 1:59\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,095 ms\n",
      "Tokens per second [15.8] input tokens [598] + xml response tokens [33] = total tokens i/o [631]\n",
      "Response: [<response><command>search google scholar current tab</command><args>EnvironmentError</args></response>]\n",
      "\n",
      "Processing call [950] out of [1000] = [95.0%]... ETA mm:ss 1:57\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,284 ms\n",
      "Tokens per second [15.8] input tokens [605] + xml response tokens [36] = total tokens i/o [641]\n",
      "Response: [<response><command>search kagi new tab</command><args>Pandas DataFrame creation</args></response>]\n",
      "\n",
      "Processing call [951] out of [1000] = [95.1%]... ETA mm:ss 1:55\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,280 ms\n",
      "Tokens per second [15.8] input tokens [602] + xml response tokens [36] = total tokens i/o [638]\n",
      "Response: [<response><command>search phind current tab</command><args>how to make pizza dough</args></response>]\n",
      "\n",
      "Processing call [952] out of [1000] = [95.2%]... ETA mm:ss 1:52\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,462 ms\n",
      "Tokens per second [15.8] input tokens [604] + xml response tokens [39] = total tokens i/o [643]\n",
      "Response: [<response><command>search kagi current tab</command><args>Setting and resetting index in Pandas</args></response>]\n",
      "\n",
      "Processing call [953] out of [1000] = [95.3%]... ETA mm:ss 1:50\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,309 ms\n",
      "Tokens per second [16.0] input tokens [383] + xml response tokens [37] = total tokens i/o [420]\n",
      "Response: [<response><command>agent router go to weather</command><args>New York City, New York</args></response>]\n",
      "\n",
      "Processing call [954] out of [1000] = [95.4%]... ETA mm:ss 1:48\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,371 ms\n",
      "Tokens per second [16.0] input tokens [373] + xml response tokens [38] = total tokens i/o [411]\n",
      "Response: [<response><command>agent router go to date and time</command><args>Fremont, California</args></response>]\n",
      "\n",
      "Processing call [955] out of [1000] = [95.5%]... ETA mm:ss 1:45\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,159 ms\n",
      "Tokens per second [15.7] input tokens [606] + xml response tokens [34] = total tokens i/o [640]\n",
      "Response: [<response><command>search google new tab</command><args>FileNotFoundError</args></response>]\n",
      "\n",
      "Processing call [956] out of [1000] = [95.6%]... ETA mm:ss 1:43\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,163 ms\n",
      "Tokens per second [15.7] input tokens [604] + xml response tokens [34] = total tokens i/o [638]\n",
      "Response: [<response><command>search google new tab</command><args>Connection Refused Error</args></response>]\n",
      "\n",
      "Processing call [957] out of [1000] = [95.7%]... ETA mm:ss 1:40\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,100 ms\n",
      "Tokens per second [15.7] input tokens [598] + xml response tokens [33] = total tokens i/o [631]\n",
      "Response: [<response><command>search google scholar current tab</command><args>Memory Error</args></response>]\n",
      "\n",
      "Processing call [958] out of [1000] = [95.8%]... ETA mm:ss 1:38\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,627 ms\n",
      "Tokens per second [16.0] input tokens [370] + xml response tokens [26] = total tokens i/o [396]\n",
      "Response: [<response><command>none</command><args></args></response>]\n",
      "\n",
      "Processing call [959] out of [1000] = [95.9%]... ETA mm:ss 1:36\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,155 ms\n",
      "Tokens per second [15.8] input tokens [602] + xml response tokens [34] = total tokens i/o [636]\n",
      "Response: [<response><command>search google scholar current tab</command><args>SQL database management</args></response>]\n",
      "\n",
      "Processing call [960] out of [1000] = [96.0%]... ETA mm:ss 1:33\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 3,090 ms\n",
      "Tokens per second [15.9] input tokens [621] + xml response tokens [49] = total tokens i/o [670]\n",
      "Response: [<response><command>search kagi new tab</command><args>What are the causes of translation errors with Unicode characters in Python and how can they be fixed?</args></response>]\n",
      "\n",
      "Processing call [961] out of [1000] = [96.1%]... ETA mm:ss 1:31\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,317 ms\n",
      "Tokens per second [16.0] input tokens [367] + xml response tokens [37] = total tokens i/o [404]\n",
      "Response: [<response><command>agent router go to date and time</command><args>Reno, Nevada</args></response>]\n",
      "\n",
      "Processing call [962] out of [1000] = [96.2%]... ETA mm:ss 1:29\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,281 ms\n",
      "Tokens per second [15.8] input tokens [602] + xml response tokens [36] = total tokens i/o [638]\n",
      "Response: [<response><command>go to current tab</command><args>excitingpenguin.org</args></response>]\n",
      "\n",
      "Processing call [963] out of [1000] = [96.3%]... ETA mm:ss 1:26\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,187 ms\n",
      "Tokens per second [16.0] input tokens [362] + xml response tokens [35] = total tokens i/o [397]\n",
      "Response: [<response><command>agent router go to weather</command><args>Baku, Azerbaijan</args></response>]\n",
      "\n",
      "Processing call [964] out of [1000] = [96.4%]... ETA mm:ss 1:24\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,186 ms\n",
      "Tokens per second [16.0] input tokens [378] + xml response tokens [35] = total tokens i/o [413]\n",
      "Response: [<response><command>agent router go to weather</command><args>Bangkok, Thailand</args></response>]\n",
      "\n",
      "Processing call [965] out of [1000] = [96.5%]... ETA mm:ss 1:22\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,371 ms\n",
      "Tokens per second [16.0] input tokens [364] + xml response tokens [38] = total tokens i/o [402]\n",
      "Response: [<response><command>agent router go to date and time</command><args>Bogota, Colombia</args></response>]\n",
      "\n",
      "Processing call [966] out of [1000] = [96.6%]... ETA mm:ss 1:19\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,340 ms\n",
      "Tokens per second [15.8] input tokens [604] + xml response tokens [37] = total tokens i/o [641]\n",
      "Response: [<response><command>go to current tab</command><args>prod.fantasticcherry.org</args></response>]\n",
      "\n",
      "Processing call [967] out of [1000] = [96.7%]... ETA mm:ss 1:17\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,189 ms\n",
      "Tokens per second [16.0] input tokens [366] + xml response tokens [35] = total tokens i/o [401]\n",
      "Response: [<response><command>agent router go to receptionist</command><args>Secretary Agent</args></response>]\n",
      "\n",
      "Processing call [968] out of [1000] = [96.8%]... ETA mm:ss 1:15\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,341 ms\n",
      "Tokens per second [15.8] input tokens [606] + xml response tokens [37] = total tokens i/o [643]\n",
      "Response: [<response><command>search perplexity current tab</command><args>Neural Network hyperparameter tuning</args></response>]\n",
      "\n",
      "Processing call [969] out of [1000] = [96.9%]... ETA mm:ss 1:12\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 3,030 ms\n",
      "Tokens per second [15.8] input tokens [614] + xml response tokens [48] = total tokens i/o [662]\n",
      "Response: [<response><command>search kagi current tab</command><args>Sentiment Analysis tools: What tools are widely used for sentiment analysis in social media data?</args></response>]\n",
      "\n",
      "Processing call [970] out of [1000] = [97.0%]... ETA mm:ss 1:10\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,969 ms\n",
      "Tokens per second [15.8] input tokens [616] + xml response tokens [47] = total tokens i/o [663]\n",
      "Response: [<response><command>search google current tab</command><args>Google Cloud AI services: What AI services does Google Cloud offer for developers and data scientists?</args></response>]\n",
      "\n",
      "Processing call [971] out of [1000] = [97.1%]... ETA mm:ss 1:08\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,720 ms\n",
      "Tokens per second [15.8] input tokens [616] + xml response tokens [43] = total tokens i/o [659]\n",
      "Response: [<response><command>search google new tab</command><args>Deep Learning optimization: What strategies are used for optimizing deep learning models?</args></response>]\n",
      "\n",
      "Processing call [972] out of [1000] = [97.2%]... ETA mm:ss 1:05\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,195 ms\n",
      "Tokens per second [15.9] input tokens [379] + xml response tokens [35] = total tokens i/o [414]\n",
      "Response: [<response><command>agent router go to weather</command><args>Fort Worth, Texas</args></response>]\n",
      "\n",
      "Processing call [973] out of [1000] = [97.3%]... ETA mm:ss 1:03\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,190 ms\n",
      "Tokens per second [16.0] input tokens [369] + xml response tokens [35] = total tokens i/o [404]\n",
      "Response: [<response><command>agent router go to receptionist</command><args>Guest Services Agent</args></response>]\n",
      "\n",
      "Processing call [974] out of [1000] = [97.4%]... ETA mm:ss 1:01\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,216 ms\n",
      "Tokens per second [15.8] input tokens [606] + xml response tokens [35] = total tokens i/o [641]\n",
      "Response: [<response><command>search kagi new tab</command><args>IsADirectoryError</args></response>]\n",
      "\n",
      "Processing call [975] out of [1000] = [97.5%]... ETA: 58 seconds\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,967 ms\n",
      "Tokens per second [15.8] input tokens [601] + xml response tokens [31] = total tokens i/o [632]\n",
      "Response: [<response><command>search google using clipboard current tab</command><args></args></response>]\n",
      "\n",
      "Processing call [976] out of [1000] = [97.6%]... ETA: 56 seconds\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,467 ms\n",
      "Tokens per second [15.8] input tokens [607] + xml response tokens [39] = total tokens i/o [646]\n",
      "Response: [<response><command>go to new tab</command><args>www.magnificentpenguin.gov</args></response>]\n",
      "\n",
      "Processing call [977] out of [1000] = [97.7%]... ETA: 54 seconds\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,251 ms\n",
      "Tokens per second [16.0] input tokens [367] + xml response tokens [36] = total tokens i/o [403]\n",
      "Response: [<response><command>agent router go to weather</command><args>Wichita, Kansas</args></response>]\n",
      "\n",
      "Processing call [978] out of [1000] = [97.8%]... ETA: 51 seconds\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,283 ms\n",
      "Tokens per second [15.8] input tokens [602] + xml response tokens [36] = total tokens i/o [638]\n",
      "Response: [<response><command>search kagi current tab</command><args>AWS SageMaker for ML</args></response>]\n",
      "\n",
      "Processing call [979] out of [1000] = [97.9%]... ETA: 49 seconds\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,064 ms\n",
      "Tokens per second [16.0] input tokens [362] + xml response tokens [33] = total tokens i/o [395]\n",
      "Response: [<response><command>agent router go to receptionist</command><args>Service</args></response>]\n",
      "\n",
      "Processing call [980] out of [1000] = [98.0%]... ETA: 46 seconds\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,910 ms\n",
      "Tokens per second [15.8] input tokens [616] + xml response tokens [46] = total tokens i/o [662]\n",
      "Response: [<response><command>search new tab</command><args>ROC Curve analysis: How is ROC curve analysis utilized to evaluate the performance of binary classifiers?</args></response>]\n",
      "\n",
      "Processing call [981] out of [1000] = [98.1%]... ETA: 44 seconds\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,409 ms\n",
      "Tokens per second [15.8] input tokens [607] + xml response tokens [38] = total tokens i/o [645]\n",
      "Response: [<response><command>go to current tab</command><args>beta.wonderfuljellyfish.net</args></response>]\n",
      "\n",
      "Processing call [982] out of [1000] = [98.2%]... ETA: 42 seconds\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,031 ms\n",
      "Tokens per second [15.8] input tokens [602] + xml response tokens [32] = total tokens i/o [634]\n",
      "Response: [<response><command>search phind using clipboard current tab</command><args></args></response>]\n",
      "\n",
      "Processing call [983] out of [1000] = [98.3%]... ETA: 39 seconds\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,472 ms\n",
      "Tokens per second [15.8] input tokens [610] + xml response tokens [39] = total tokens i/o [649]\n",
      "Response: [<response><command>search phind new tab</command><args>Recursion Error: Maximum recursion depth exceeded</args></response>]\n",
      "\n",
      "Processing call [984] out of [1000] = [98.4%]... ETA: 37 seconds\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,283 ms\n",
      "Tokens per second [15.8] input tokens [601] + xml response tokens [36] = total tokens i/o [637]\n",
      "Response: [<response><command>go to new tab</command><args>magnificentvolcano.info</args></response>]\n",
      "\n",
      "Processing call [985] out of [1000] = [98.5%]... ETA: 35 seconds\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,373 ms\n",
      "Tokens per second [16.0] input tokens [372] + xml response tokens [38] = total tokens i/o [410]\n",
      "Response: [<response><command>agent router go to date and time</command><args>Dhaka, Bangladesh</args></response>]\n",
      "\n",
      "Processing call [986] out of [1000] = [98.6%]... ETA: 32 seconds\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,406 ms\n",
      "Tokens per second [15.8] input tokens [607] + xml response tokens [38] = total tokens i/o [645]\n",
      "Response: [<response><command>go to new tab</command><args>beta.hilariouspenguin.io</args></response>]\n",
      "\n",
      "Processing call [987] out of [1000] = [98.7%]... ETA: 30 seconds\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,341 ms\n",
      "Tokens per second [15.8] input tokens [605] + xml response tokens [37] = total tokens i/o [642]\n",
      "Response: [<response><command>go to current tab</command><args>dev.hilariousbanana.io</args></response>]\n",
      "\n",
      "Processing call [988] out of [1000] = [98.8%]... ETA: 28 seconds\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,031 ms\n",
      "Tokens per second [15.8] input tokens [598] + xml response tokens [32] = total tokens i/o [630]\n",
      "Response: [<response><command>search google current tab</command><args>Connection Error</args></response>]\n",
      "\n",
      "Processing call [989] out of [1000] = [98.9%]... ETA: 25 seconds\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 3,030 ms\n",
      "Tokens per second [15.8] input tokens [620] + xml response tokens [48] = total tokens i/o [668]\n",
      "Response: [<response><command>search perplexity new tab</command><args>What are the causes of floating point errors in Python, and how can they be minimized?</args></response>]\n",
      "\n",
      "Processing call [990] out of [1000] = [99.0%]... ETA: 23 seconds\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,282 ms\n",
      "Tokens per second [15.8] input tokens [606] + xml response tokens [36] = total tokens i/o [642]\n",
      "Response: [<response><command>search google scholar new tab</command><args>how to make pizza dough</args></response>]\n",
      "\n",
      "Processing call [991] out of [1000] = [99.1%]... ETA: 21 seconds\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,341 ms\n",
      "Tokens per second [15.8] input tokens [604] + xml response tokens [37] = total tokens i/o [641]\n",
      "Response: [<response><command>go to new tab</command><args>incredibledolphin.info</args></response>]\n",
      "\n",
      "Processing call [992] out of [1000] = [99.2%]... ETA: 18 seconds\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,218 ms\n",
      "Tokens per second [15.8] input tokens [605] + xml response tokens [35] = total tokens i/o [640]\n",
      "Response: [<response><command>search google scholar new tab</command><args>AI in financial forecasting</args></response>]\n",
      "\n",
      "Processing call [993] out of [1000] = [99.3%]... ETA: 16 seconds\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,282 ms\n",
      "Tokens per second [15.8] input tokens [604] + xml response tokens [36] = total tokens i/o [640]\n",
      "Response: [<response><command>search google scholar current tab</command><args>buying a new laptop</args></response>]\n",
      "\n",
      "Processing call [994] out of [1000] = [99.4%]... ETA: 14 seconds\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,287 ms\n",
      "Tokens per second [15.7] input tokens [611] + xml response tokens [36] = total tokens i/o [647]\n",
      "Response: [<response><command>search google new tab</command><args>Reinforcement Learning breakthroughs</args></response>]\n",
      "\n",
      "Processing call [995] out of [1000] = [99.5%]... ETA: 11 seconds\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,253 ms\n",
      "Tokens per second [16.0] input tokens [372] + xml response tokens [36] = total tokens i/o [408]\n",
      "Response: [<response><command>agent router go to weather</command><args>Tulsa, Oklahoma</args></response>]\n",
      "\n",
      "Processing call [996] out of [1000] = [99.6%]... ETA: 9 seconds\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,343 ms\n",
      "Tokens per second [15.8] input tokens [602] + xml response tokens [37] = total tokens i/o [639]\n",
      "Response: [<response><command>go to current tab</command><args>mail.jubilantquartz.com</args></response>]\n",
      "\n",
      "Processing call [997] out of [1000] = [99.7%]... ETA: 7 seconds\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,874 ms\n",
      "Tokens per second [16.0] input tokens [374] + xml response tokens [30] = total tokens i/o [404]\n",
      "Response: [<response><command>agent router go to calendar</command><args></args></response>]\n",
      "\n",
      "Processing call [998] out of [1000] = [99.8%]... ETA: 4 seconds\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,279 ms\n",
      "Tokens per second [15.8] input tokens [604] + xml response tokens [36] = total tokens i/o [640]\n",
      "Response: [<response><command>search new tab</command><args>Stop Iteration: Iteration stopped</args></response>]\n",
      "\n",
      "Processing call [999] out of [1000] = [99.9%]... ETA: 2 seconds\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 1,876 ms\n",
      "Tokens per second [16.0] input tokens [367] + xml response tokens [30] = total tokens i/o [397]\n",
      "Response: [<response><command>agent router go to calendar</command><args></args></response>]\n",
      "\n",
      "Processing call [1000] out of [1000] = [100.0%]... ETA: 0 seconds\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n",
      "Asking LLM [Ministral-8B-Instruct-2410]... Done! in 2,222 ms\n",
      "Tokens per second [15.8] input tokens [605] + xml response tokens [35] = total tokens i/o [640]\n",
      "Response: [<response><command>search google scholar new tab</command><args>Is A Directory Error</args></response>]\n",
      "\n",
      "\n",
      "Generating responses for 1,000 rows... Done! in 39:09\n",
      "[2,349.5] ms per item\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Validation stats for model Ministral-8B-Instruct-2410\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "               Is valid xml 100.0%\n",
      "        Contains <response> 100.0%\n",
      "         Contains <command> 100.0%\n",
      "            Contains <args> 100.0%\n",
      "          Response is exact 99.8%\n",
      "Response has correct values 99.8%\n",
      "         Command is correct 99.8%\n",
      "            Args is correct 100.0%\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Validation stats for model Ministral-8B-Instruct-2410: Accuracy per command\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "                                            command     mean  sum  count\n",
      "           search phind using clipboard current tab   60.00%    3      5\n",
      "                        agent router go to calendar  100.00%   46     46\n",
      "              search google using clipboard new tab  100.00%    5      5\n",
      "                 search using clipboard current tab  100.00%    2      2\n",
      "               search phind using clipboard new tab  100.00%    7      7\n",
      "                               search phind new tab  100.00%   53     53\n",
      "                           search phind current tab  100.00%   58     58\n",
      "          search perplexity using clipboard new tab  100.00%    6      6\n",
      "      search perplexity using clipboard current tab  100.00%    4      4\n",
      "                          search perplexity new tab  100.00%   47     47\n",
      "                      search perplexity current tab  100.00%   45     45\n",
      "                                     search new tab  100.00%   47     47\n",
      "                search kagi using clipboard new tab  100.00%    5      5\n",
      "            search kagi using clipboard current tab  100.00%    4      4\n",
      "                                search kagi new tab  100.00%   48     48\n",
      "                            search kagi current tab  100.00%   52     52\n",
      "          search google using clipboard current tab  100.00%    6      6\n",
      "                   agent router go to date and time  100.00%   52     52\n",
      "      search google scholar using clipboard new tab  100.00%    4      4\n",
      "  search google scholar using clipboard current tab  100.00%    8      8\n",
      "                      search google scholar new tab  100.00%   44     44\n",
      "                  search google scholar current tab  100.00%   55     55\n",
      "                              search google new tab  100.00%   58     58\n",
      "                          search google current tab  100.00%   49     49\n",
      "                                 search current tab  100.00%   48     48\n",
      "                                               none  100.00%   10     10\n",
      "                                      go to new tab  100.00%   53     53\n",
      "                                  go to current tab  100.00%   57     57\n",
      "                         agent router go to weather  100.00%   55     55\n",
      "                       agent router go to todo list  100.00%    6      6\n",
      "                    agent router go to receptionist  100.00%   48     48\n",
      "                            agent router go to math  100.00%    9      9\n",
      "                     search using clipboard new tab  100.00%    4      4\n"
     ]
    }
   ],
   "source": [
    "stats_df = run_validation( adapter_plus_model, tokenizer, sample_size=1000, model_name=model_name, device=\"cuda:1\" )\n",
    "\n",
    "# Generating responses for 1,000 rows... Done! in 18:09\n",
    "# [1,089.6] ms per item\n",
    "#\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# - Validation stats for model ministral/Ministral-3b-instruct\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "#\n",
    "#                Is valid xml 100.0%\n",
    "#         Contains <response> 100.0%\n",
    "#          Contains <command> 100.0%\n",
    "#             Contains <args> 100.0%\n",
    "#           Response is exact 99.6%\n",
    "# Response has correct values 99.6%\n",
    "#          Command is correct 99.9%\n",
    "#             Args is correct 99.7%\n",
    "\n",
    "# Generating responses for 1,000 rows... Done! in 34:46\n",
    "# [2086.8] ms per item\n",
    "# \n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# - Validation stats for model mistralai/Mistral-7B-Instruct-v0.2\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# \n",
    "#                Is valid xml 0.0%\n",
    "#           Contains response 100.0%\n",
    "#  Contains <browser-command> 100.0%\n",
    "#             Contains <args> 100.0%\n",
    "#           Response is exact 99.5%\n",
    "# Response has correct values 99.5%\n",
    "#  Browser command is correct 99.6%\n",
    "#             Args is correct 99.9%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554df92b15666503",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Perform a 16bit merge & write to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1daa10cd14621325",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T20:57:11.940676Z",
     "start_time": "2025-01-13T20:57:11.933665Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/var/model/models/Ministral-8B-Instruct-2410', './merged-00-2025-01-09')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "merge_date = \"2025-01-09\"\n",
    "# merge_date = du.get_current_date()\n",
    "\n",
    "os.chdir( f\"{models_root}/{model_name}\" )\n",
    "merged_path = \"./merged-00-\" + merge_date\n",
    "os.getcwd(), merged_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7438bad14a9b0c12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T21:55:16.145071Z",
     "start_time": "2025-01-09T21:55:00.315656Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./merged-00-2025-01-09/config.json\n",
      "Configuration saved in ./merged-00-2025-01-09/generation_config.json\n",
      "The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 4 checkpoint shards. You can find where each parameters has been saved in the index located at ./merged-00-2025-01-09/model.safetensors.index.json.\n"
     ]
    }
   ],
   "source": [
    "adapter_plus_model = adapter_plus_model.merge_and_unload()\n",
    "adapter_plus_model.save_pretrained( merged_path, safe_serialization=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1328c02bea1c4669",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T21:56:19.841981Z",
     "start_time": "2025-01-09T21:56:19.692800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in ./merged-00-2025-01-09/tokenizer_config.json\n",
      "Special tokens file saved in ./merged-00-2025-01-09/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./merged-00-2025-01-09/tokenizer_config.json',\n",
       " './merged-00-2025-01-09/special_tokens_map.json',\n",
       " './merged-00-2025-01-09/tokenizer.json')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained( merged_path, safe_serialization=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0c8159cb-bf7f-4ee9-b020-fa7f21a4f6ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T22:00:48.748732Z",
     "start_time": "2025-01-09T22:00:48.591812Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 15G\r\n",
      "drwxr-xr-x 2 root root 4.0K Jan  9 21:56 .\r\n",
      "drwxrwxr-x 4 1001 1001 4.0K Jan  9 21:55 ..\r\n",
      "-rw-r--r-- 1 root root  678 Jan  9 21:55 config.json\r\n",
      "-rw-r--r-- 1 root root  111 Jan  9 21:55 generation_config.json\r\n",
      "-rw-r--r-- 1 root root 4.7G Jan  9 21:55 model-00001-of-00004.safetensors\r\n",
      "-rw-r--r-- 1 root root 4.7G Jan  9 21:55 model-00002-of-00004.safetensors\r\n",
      "-rw-r--r-- 1 root root 4.7G Jan  9 21:55 model-00003-of-00004.safetensors\r\n",
      "-rw-r--r-- 1 root root 1.1G Jan  9 21:55 model-00004-of-00004.safetensors\r\n",
      "-rw-r--r-- 1 root root  27K Jan  9 21:55 model.safetensors.index.json\r\n",
      "-rw-r--r-- 1 root root  437 Jan  9 21:56 special_tokens_map.json\r\n",
      "-rw-r--r-- 1 root root  17M Jan  9 21:56 tokenizer.json\r\n",
      "-rw-r--r-- 1 root root 178K Jan  9 21:56 tokenizer_config.json\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "! ls -alh /var/model/models/Ministral-8B-Instruct-2410/merged-00-2025-01-09"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2de18b760eb37d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## RESTART 2nd time & load merged model + tokenizer in bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "33223e1010b01223",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T02:35:37.945075Z",
     "start_time": "2025-01-14T02:35:37.938976Z"
    }
   },
   "outputs": [],
   "source": [
    "def reset_kernel( models ):\n",
    "    \n",
    "    import gc\n",
    "    from IPython import get_ipython\n",
    "    \n",
    "    for model in models:\n",
    "        del model\n",
    "    \n",
    "    gc.collect()\n",
    "    get_ipython().kernel.do_shutdown(restart=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c18e54412c1beced",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T02:35:39.778834Z",
     "start_time": "2025-01-14T02:35:39.772383Z"
    }
   },
   "outputs": [],
   "source": [
    "def reset_environment():\n",
    "    \n",
    "    %load_ext autoreload\n",
    "    %autoreload\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f51654cd-879c-412a-b1ab-c8fe6cdd1391",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T17:19:20.986110Z",
     "start_time": "2025-01-13T17:19:20.869606Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 20K\r\n",
      "drwxrwxr-x 5 1001 1001 4.0K Jan 13 16:29 .\r\n",
      "drwxrwxr-x 9 1001 1001 4.0K Jan  9 02:59 ..\r\n",
      "drwxr-xr-x 2 root root 4.0K Jan  9 21:56 merged-00-2025-01-09\r\n",
      "drwxr-xr-x 2 root root 4.0K Jan 13 16:29 merged-00-2025-01-09.awq\r\n",
      "drwxr-xr-x 3 root root 4.0K Jan  9 18:37 training-results-2025.01.09\r\n"
     ]
    }
   ],
   "source": [
    "! ls -alh /var/model/models/Ministral-8B-Instruct-2410/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ba1e4c850cbecd80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T22:02:25.535473Z",
     "start_time": "2025-01-09T22:02:25.274642Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "reset_kernel( [ base_model, tokenizer, adapter_plus_model ] )\n",
    "reset_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40f85a90c6d1f92e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T17:37:44.988826Z",
     "start_time": "2025-01-13T17:37:40.107679Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/var/model/models\n",
      "60\n",
      "60\n",
      "/var/model/models/Ministral-8B-Instruct-2410/merged-00-2025-01-09\n",
      "Loading without BitsAndBytesConfig...\n",
      "HF_HOME: /var/model/models\n",
      "HF_HUB_ETAG_TIMEOUT: 60\n",
      "HF_HUB_DOWNLOAD_TIMEOUT: 60\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bffc54560fc84648b8222cdd0dd5da16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "set_gib_env_vars()\n",
    "set_hf_env_vars()\n",
    "\n",
    "os.chdir( f\"{models_root}/{model_name}/merged-00-{merge_date}\" )\n",
    "print( os.getcwd() )\n",
    "\n",
    "merged_model, merged_tokenizer = get_base_model_and_tokenizer( \n",
    "    use_bnb_quantization=False, \n",
    "    device_map=\"cuda:1\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a198e607c60f99e4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Raw merged model in bfloat16\n",
    "```\n",
    "Wed Dec 18 15:10:23 2024\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  NVIDIA GeForce RTX 4090        On  | 00000000:01:00.0 Off |                  Off |\n",
    "|  0%   37C    P8              25W / 450W |      6MiB / 24564MiB |      0%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "|   1  NVIDIA GeForce RTX 4090        On  | 00000000:02:00.0 Off |                  Off |\n",
    "|  0%   45C    P8              21W / 450W |   6776MiB / 24564MiB |      0%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "|    1   N/A  N/A     33585      C   /usr/bin/python3                           6766MiB |\n",
    "+---------------------------------------------------------------------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ea31d974c4ce0c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T17:22:38.544567Z",
     "start_time": "2025-01-13T17:22:38.540064Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%autoreload` not found.\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "stats_df = run_validation( merged_model, merged_tokenizer, model_name=model_name, device=\"cuda:1\", sample_size=100 )\n",
    "stats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ce0f1348c4d0d8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "```\n",
    "Generating responses for 100 rows... Done! in 50 seconds\n",
    "[501.0] ms per item\n",
    "\n",
    "------------------------------------------------------------------------------------------------------------------------\n",
    "- Validation stats for model ministral/Ministral-3b-instruct\n",
    "------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "               Is valid xml 100.0%\n",
    "        Contains <response> 100.0%\n",
    "         Contains <command> 100.0%\n",
    "            Contains <args> 100.0%\n",
    "          Response is exact 100.0%\n",
    "Response has correct values 100.0%\n",
    "         Command is correct 100.0%\n",
    "            Args is correct 100.0%\n",
    "\n",
    "Wed Dec 18 15:27:21 2024\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  NVIDIA GeForce RTX 4090        On  | 00000000:01:00.0 Off |                  Off |\n",
    "|  0%   36C    P8              25W / 450W |      6MiB / 24564MiB |      0%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "|   1  NVIDIA GeForce RTX 4090        On  | 00000000:02:00.0 Off |                  Off |\n",
    "|  0%   43C    P8              21W / 450W |   7020MiB / 24564MiB |      0%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "|    1   N/A  N/A     33585      C   /usr/bin/python3                           7010MiB |\n",
    "+---------------------------------------------------------------------------------------+\n",
    "```\n",
    "\n",
    "```\n",
    "Generating responses for 100 rows... Done! in 02:19\n",
    "[1390.6] ms per item\n",
    "\n",
    "------------------------------------------------------------------------------------------------------------------------\n",
    "- Validation stats for model mistralai/Mistral-7B-Instruct-v0.2\n",
    "------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "               Is valid xml 0.0%\n",
    "          Contains response 100.0%\n",
    " Contains <browser-command> 100.0%\n",
    "            Contains <args> 100.0%\n",
    "          Response is exact 100.0%\n",
    "Response has correct values 100.0%\n",
    " Browser command is correct 100.0%\n",
    "            Args is correct 100.0%\n",
    "\n",
    "Exact same model loaded two different ways:\n",
    "0: Using TGI with & w/o --dtype bfloat16 flag\n",
    "   docker run --name huggingface-tgi --gpus all --shm-size 1g -p 3000:3000 -v `pwd`:/data/model  ghcr.io/huggingface/text-generation-inference:1.3.4 --dtype bfloat16 --sharded false --num-shard 1 --port 3000 --model-id /data/model --quantize awq\n",
    "\n",
    "1: Using jupyter notebook with raw model file: \n",
    "   low_cpu_mem_usage=True, \n",
    "   use_cache=True, \n",
    "   attn_implementation=\"flash_attention_2\",\n",
    "   torch_dtype=torch.bfloat16\n",
    "\n",
    "Wed Jan 24 11:27:02 2024\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  NVIDIA GeForce RTX 4090        On  | 00000000:01:00.0 Off |                  Off |\n",
    "|  0%   39C    P2              69W / 450W |  23146MiB / 24564MiB |      0%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "|   1  NVIDIA GeForce RTX 4090        On  | 00000000:02:00.0 Off |                  Off |\n",
    "|  0%   43C    P8              24W / 450W |  15366MiB / 24564MiB |      0%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "|    0   N/A  N/A     10768      C   /opt/conda/bin/python3.10                 23136MiB |\n",
    "|    1   N/A  N/A      7765      C   /usr/bin/python3                          15356MiB |\n",
    "+---------------------------------------------------------------------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ca518ebd22ebc3",
   "metadata": {},
   "source": [
    "## Run benchmark on TGI service listening on port 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "521c17d6e20e575f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T16:03:32.355713Z",
     "start_time": "2025-01-13T16:03:32.151076Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "reset_kernel( [ merged_model, merged_tokenizer ] )\n",
    "reset_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d45ff5f83a4cddc5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T02:33:04.704175Z",
     "start_time": "2025-01-10T02:33:04.200779Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reusing ConfigurationManager() singleton...\n",
      "Inserting DUPLICATE placeholders into the list. Requested length [500] > list length [74]\n",
      "Inserting DUPLICATE placeholders into the list. Requested length [500] > list length [148]\n",
      "Inserting DUPLICATE placeholders into the list. Requested length [500] > list length [296]\n",
      "Commands file for command [go to current tab] exists: True\n",
      "Commands file for command [go to new tab] exists: True\n",
      "Commands file for command [search current tab] exists: True\n",
      "Commands file for command [search new tab] exists: True\n",
      "Commands file for command [search google current tab] exists: True\n",
      "Commands file for command [search google new tab] exists: True\n",
      "Commands file for command [search google scholar current tab] exists: True\n",
      "Commands file for command [search google scholar new tab] exists: True\n",
      "Commands file for command [search kagi new tab] exists: True\n",
      "Commands file for command [search kagi current tab] exists: True\n",
      "Commands file for command [search perplexity current tab] exists: True\n",
      "Commands file for command [search perplexity new tab] exists: True\n",
      "Commands file for command [search phind current tab] exists: True\n",
      "Commands file for command [search phind new tab] exists: True\n",
      "\n",
      "Commands file for command [search using clipboard current tab] exists: True\n",
      "Commands file for command [search using clipboard new tab] exists: True\n",
      "Commands file for command [search google using clipboard current tab] exists: True\n",
      "Commands file for command [search google using clipboard new tab] exists: True\n",
      "Commands file for command [search google scholar using clipboard current tab] exists: True\n",
      "Commands file for command [search google scholar using clipboard new tab] exists: True\n",
      "Commands file for command [search kagi using clipboard current tab] exists: True\n",
      "Commands file for command [search kagi using clipboard new tab] exists: True\n",
      "Commands file for command [search perplexity using clipboard current tab] exists: True\n",
      "Commands file for command [search perplexity using clipboard new tab] exists: True\n",
      "Commands file for command [search phind using clipboard current tab] exists: True\n",
      "Commands file for command [search phind using clipboard new tab] exists: True\n",
      "Commands file for command [none] exists: True\n",
      "\n",
      "Commands file for command [agent router go to search function mapping] exists: True\n",
      "\n",
      "Commands file for command [agent router go to date and time] exists: True\n",
      "Commands file for command [agent router go to weather] exists: True\n",
      "Commands file for command [agent router go to calendar] exists: True\n",
      "Commands file for command [agent router go to receptionist] exists: True\n",
      "\n",
      "Commands file for command [agent router go to todo list] exists: True\n",
      "Commands file for command [agent router go to math] exists: True\n",
      "Commands file for command [none] exists: True\n",
      "\n",
      "Generating responses for 10 rows...\n",
      "Using TGI w/ model_name [Ministral-8B-Instruct-2410]...\n",
      "Processing call [001] out of [10] = [10.0%]... ETA: 0 seconds\n",
      "Asking LLM [Ministral-8B-Instruct-2410]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/inference/_client.py:2232: FutureWarning: `stop_sequences` is a deprecated argument for `text_generation` task and will be removed in version '0.28.0'. Use `stop` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "(MaxRetryError(\"HTTPConnectionPool(host='127.0.0.1', port=3000): Max retries exceeded with url: /v1 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f78d1acd1e0>: Failed to establish a new connection: [Errno 111] Connection refused'))\"), '(Request ID: a83b0645-60e5-49c4-bde2-5df9b885c13f)')",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mConnectionRefusedError\u001B[0m                    Traceback (most recent call last)",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connection.py:199\u001B[0m, in \u001B[0;36mHTTPConnection._new_conn\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    198\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 199\u001B[0m     sock \u001B[38;5;241m=\u001B[39m \u001B[43mconnection\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate_connection\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    200\u001B[0m \u001B[43m        \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dns_host\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mport\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    201\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    202\u001B[0m \u001B[43m        \u001B[49m\u001B[43msource_address\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msource_address\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    203\u001B[0m \u001B[43m        \u001B[49m\u001B[43msocket_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msocket_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    204\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    205\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m socket\u001B[38;5;241m.\u001B[39mgaierror \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/util/connection.py:85\u001B[0m, in \u001B[0;36mcreate_connection\u001B[0;34m(address, timeout, source_address, socket_options)\u001B[0m\n\u001B[1;32m     84\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 85\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m err\n\u001B[1;32m     86\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     87\u001B[0m     \u001B[38;5;66;03m# Break explicitly a reference cycle\u001B[39;00m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/util/connection.py:73\u001B[0m, in \u001B[0;36mcreate_connection\u001B[0;34m(address, timeout, source_address, socket_options)\u001B[0m\n\u001B[1;32m     72\u001B[0m     sock\u001B[38;5;241m.\u001B[39mbind(source_address)\n\u001B[0;32m---> 73\u001B[0m \u001B[43msock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconnect\u001B[49m\u001B[43m(\u001B[49m\u001B[43msa\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     74\u001B[0m \u001B[38;5;66;03m# Break explicitly a reference cycle\u001B[39;00m\n",
      "\u001B[0;31mConnectionRefusedError\u001B[0m: [Errno 111] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mNewConnectionError\u001B[0m                        Traceback (most recent call last)",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py:789\u001B[0m, in \u001B[0;36mHTTPConnectionPool.urlopen\u001B[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001B[0m\n\u001B[1;32m    788\u001B[0m \u001B[38;5;66;03m# Make the request on the HTTPConnection object\u001B[39;00m\n\u001B[0;32m--> 789\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    790\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    791\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    792\u001B[0m \u001B[43m    \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    793\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout_obj\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    794\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    795\u001B[0m \u001B[43m    \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    796\u001B[0m \u001B[43m    \u001B[49m\u001B[43mchunked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunked\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    797\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mretries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    798\u001B[0m \u001B[43m    \u001B[49m\u001B[43mresponse_conn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresponse_conn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    799\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpreload_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpreload_content\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    800\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdecode_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecode_content\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    801\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mresponse_kw\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    802\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    804\u001B[0m \u001B[38;5;66;03m# Everything went great!\u001B[39;00m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py:495\u001B[0m, in \u001B[0;36mHTTPConnectionPool._make_request\u001B[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001B[0m\n\u001B[1;32m    494\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 495\u001B[0m     \u001B[43mconn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    496\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    497\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    498\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    499\u001B[0m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    500\u001B[0m \u001B[43m        \u001B[49m\u001B[43mchunked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunked\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    501\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpreload_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpreload_content\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    502\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdecode_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecode_content\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    503\u001B[0m \u001B[43m        \u001B[49m\u001B[43menforce_content_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43menforce_content_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    504\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    506\u001B[0m \u001B[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001B[39;00m\n\u001B[1;32m    507\u001B[0m \u001B[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001B[39;00m\n\u001B[1;32m    508\u001B[0m \u001B[38;5;66;03m# With this behaviour, the received response is still readable.\u001B[39;00m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connection.py:441\u001B[0m, in \u001B[0;36mHTTPConnection.request\u001B[0;34m(self, method, url, body, headers, chunked, preload_content, decode_content, enforce_content_length)\u001B[0m\n\u001B[1;32m    440\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mputheader(header, value)\n\u001B[0;32m--> 441\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mendheaders\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    443\u001B[0m \u001B[38;5;66;03m# If we're given a body we start sending that in chunks.\u001B[39;00m\n",
      "File \u001B[0;32m/usr/lib/python3.10/http/client.py:1278\u001B[0m, in \u001B[0;36mHTTPConnection.endheaders\u001B[0;34m(self, message_body, encode_chunked)\u001B[0m\n\u001B[1;32m   1277\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m CannotSendHeader()\n\u001B[0;32m-> 1278\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_send_output\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmessage_body\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencode_chunked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencode_chunked\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/lib/python3.10/http/client.py:1038\u001B[0m, in \u001B[0;36mHTTPConnection._send_output\u001B[0;34m(self, message_body, encode_chunked)\u001B[0m\n\u001B[1;32m   1037\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_buffer[:]\n\u001B[0;32m-> 1038\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmsg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1040\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m message_body \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1041\u001B[0m \n\u001B[1;32m   1042\u001B[0m     \u001B[38;5;66;03m# create a consistent interface to message_body\u001B[39;00m\n",
      "File \u001B[0;32m/usr/lib/python3.10/http/client.py:976\u001B[0m, in \u001B[0;36mHTTPConnection.send\u001B[0;34m(self, data)\u001B[0m\n\u001B[1;32m    975\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mauto_open:\n\u001B[0;32m--> 976\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconnect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    977\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connection.py:279\u001B[0m, in \u001B[0;36mHTTPConnection.connect\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    278\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mconnect\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 279\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msock \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_new_conn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    280\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tunnel_host:\n\u001B[1;32m    281\u001B[0m         \u001B[38;5;66;03m# If we're tunneling it means we're connected to our proxy.\u001B[39;00m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connection.py:214\u001B[0m, in \u001B[0;36mHTTPConnection._new_conn\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    213\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m--> 214\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m NewConnectionError(\n\u001B[1;32m    215\u001B[0m         \u001B[38;5;28mself\u001B[39m, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFailed to establish a new connection: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    216\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[1;32m    218\u001B[0m \u001B[38;5;66;03m# Audit hooks are only available in Python 3.8+\u001B[39;00m\n",
      "\u001B[0;31mNewConnectionError\u001B[0m: <urllib3.connection.HTTPConnection object at 0x7f78d1acd1e0>: Failed to establish a new connection: [Errno 111] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mMaxRetryError\u001B[0m                             Traceback (most recent call last)",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/requests/adapters.py:667\u001B[0m, in \u001B[0;36mHTTPAdapter.send\u001B[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001B[0m\n\u001B[1;32m    666\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 667\u001B[0m     resp \u001B[38;5;241m=\u001B[39m \u001B[43mconn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43murlopen\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    668\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    669\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    670\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    671\u001B[0m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    672\u001B[0m \u001B[43m        \u001B[49m\u001B[43mredirect\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    673\u001B[0m \u001B[43m        \u001B[49m\u001B[43massert_same_host\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    674\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpreload_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    675\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdecode_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    676\u001B[0m \u001B[43m        \u001B[49m\u001B[43mretries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_retries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    677\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    678\u001B[0m \u001B[43m        \u001B[49m\u001B[43mchunked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunked\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    679\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    681\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (ProtocolError, \u001B[38;5;167;01mOSError\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py:843\u001B[0m, in \u001B[0;36mHTTPConnectionPool.urlopen\u001B[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001B[0m\n\u001B[1;32m    841\u001B[0m     new_e \u001B[38;5;241m=\u001B[39m ProtocolError(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mConnection aborted.\u001B[39m\u001B[38;5;124m\"\u001B[39m, new_e)\n\u001B[0;32m--> 843\u001B[0m retries \u001B[38;5;241m=\u001B[39m \u001B[43mretries\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mincrement\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    844\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43merror\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnew_e\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_pool\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_stacktrace\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msys\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexc_info\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m]\u001B[49m\n\u001B[1;32m    845\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    846\u001B[0m retries\u001B[38;5;241m.\u001B[39msleep()\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/util/retry.py:519\u001B[0m, in \u001B[0;36mRetry.increment\u001B[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001B[0m\n\u001B[1;32m    518\u001B[0m     reason \u001B[38;5;241m=\u001B[39m error \u001B[38;5;129;01mor\u001B[39;00m ResponseError(cause)\n\u001B[0;32m--> 519\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m MaxRetryError(_pool, url, reason) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mreason\u001B[39;00m  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[1;32m    521\u001B[0m log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIncremented Retry for (url=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m): \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, url, new_retry)\n",
      "\u001B[0;31mMaxRetryError\u001B[0m: HTTPConnectionPool(host='127.0.0.1', port=3000): Max retries exceeded with url: /v1 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f78d1acd1e0>: Failed to establish a new connection: [Errno 111] Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mConnectionError\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[21], line 14\u001B[0m\n\u001B[1;32m     12\u001B[0m sample_size    \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m10\u001B[39m\n\u001B[1;32m     13\u001B[0m validate_df    \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_json( \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/var/model/genie-in-the-box/src/ephemera/prompts/data/voice-commands-xml-validate.jsonl\u001B[39m\u001B[38;5;124m\"\u001B[39m, lines\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m )\u001B[38;5;241m.\u001B[39msample( sample_size, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m )\n\u001B[0;32m---> 14\u001B[0m validate_df    \u001B[38;5;241m=\u001B[39m \u001B[43mtgi_validator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate_responses\u001B[49m\u001B[43m(\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidate_df\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mswitch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtgi\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_name\u001B[49m\u001B[43m \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     15\u001B[0m validate_df    \u001B[38;5;241m=\u001B[39m tgi_validator\u001B[38;5;241m.\u001B[39mvalidate_responses( validate_df )\n\u001B[1;32m     17\u001B[0m tgi_validator\u001B[38;5;241m.\u001B[39mprint_validation_stats( validate_df, title\u001B[38;5;241m=\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mValidation Stats for \u001B[39m\u001B[38;5;132;01m{\u001B[39;00msample_size\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m rows with `\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m` on TGI:3000\u001B[39m\u001B[38;5;124m\"\u001B[39m )\n",
      "File \u001B[0;32m/var/model/genie-in-the-box/src/ephemera/prompts/xml_fine_tuning_prompt_generator.py:1134\u001B[0m, in \u001B[0;36mXmlFineTuningPromptGenerator.generate_responses\u001B[0;34m(self, df, tokenizer, model, switch, model_name, max_new_tokens, temperature, top_k, top_p, device, silent)\u001B[0m\n\u001B[1;32m   1132\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m switch \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtgi\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m   1133\u001B[0m     \u001B[38;5;28mprint\u001B[39m( \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUsing TGI w/ model_name [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m]...\u001B[39m\u001B[38;5;124m\"\u001B[39m )\n\u001B[0;32m-> 1134\u001B[0m     df[ \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mresponse\u001B[39m\u001B[38;5;124m\"\u001B[39m ]  \u001B[38;5;241m=\u001B[39m \u001B[43mdf\u001B[49m\u001B[43m[\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mprompt\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mcell\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_response_to_prompt\u001B[49m\u001B[43m(\u001B[49m\u001B[43m \u001B[49m\u001B[43mcell\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrows\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mswitch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mswitch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_new_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_new_tokens\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtemperature\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_k\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtop_k\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_p\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtop_p\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msilent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msilent\u001B[49m\u001B[43m \u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1135\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m switch \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mopenai\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m   1136\u001B[0m     \u001B[38;5;28mprint\u001B[39m( \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUsing OPENAI w/ model_name [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m]...\u001B[39m\u001B[38;5;124m\"\u001B[39m )\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py:4924\u001B[0m, in \u001B[0;36mSeries.apply\u001B[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001B[0m\n\u001B[1;32m   4789\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply\u001B[39m(\n\u001B[1;32m   4790\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   4791\u001B[0m     func: AggFuncType,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   4796\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m   4797\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame \u001B[38;5;241m|\u001B[39m Series:\n\u001B[1;32m   4798\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   4799\u001B[0m \u001B[38;5;124;03m    Invoke function on values of Series.\u001B[39;00m\n\u001B[1;32m   4800\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   4915\u001B[0m \u001B[38;5;124;03m    dtype: float64\u001B[39;00m\n\u001B[1;32m   4916\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m   4917\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mSeriesApply\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   4918\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4919\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4920\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconvert_dtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert_dtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4921\u001B[0m \u001B[43m        \u001B[49m\u001B[43mby_row\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mby_row\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4922\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4923\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m-> 4924\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py:1427\u001B[0m, in \u001B[0;36mSeriesApply.apply\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1424\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply_compat()\n\u001B[1;32m   1426\u001B[0m \u001B[38;5;66;03m# self.func is Callable\u001B[39;00m\n\u001B[0;32m-> 1427\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_standard\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py:1507\u001B[0m, in \u001B[0;36mSeriesApply.apply_standard\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1501\u001B[0m \u001B[38;5;66;03m# row-wise access\u001B[39;00m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m \u001B[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001B[39;00m\n\u001B[1;32m   1504\u001B[0m \u001B[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001B[39;00m\n\u001B[1;32m   1505\u001B[0m \u001B[38;5;66;03m#  Categorical (GH51645).\u001B[39;00m\n\u001B[1;32m   1506\u001B[0m action \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mignore\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(obj\u001B[38;5;241m.\u001B[39mdtype, CategoricalDtype) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 1507\u001B[0m mapped \u001B[38;5;241m=\u001B[39m \u001B[43mobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_map_values\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1508\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmapper\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcurried\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mna_action\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maction\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert_dtype\u001B[49m\n\u001B[1;32m   1509\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1511\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(mapped) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(mapped[\u001B[38;5;241m0\u001B[39m], ABCSeries):\n\u001B[1;32m   1512\u001B[0m     \u001B[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001B[39;00m\n\u001B[1;32m   1513\u001B[0m     \u001B[38;5;66;03m#  See also GH#25959 regarding EA support\u001B[39;00m\n\u001B[1;32m   1514\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\u001B[38;5;241m.\u001B[39m_constructor_expanddim(\u001B[38;5;28mlist\u001B[39m(mapped), index\u001B[38;5;241m=\u001B[39mobj\u001B[38;5;241m.\u001B[39mindex)\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/base.py:921\u001B[0m, in \u001B[0;36mIndexOpsMixin._map_values\u001B[0;34m(self, mapper, na_action, convert)\u001B[0m\n\u001B[1;32m    918\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(arr, ExtensionArray):\n\u001B[1;32m    919\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m arr\u001B[38;5;241m.\u001B[39mmap(mapper, na_action\u001B[38;5;241m=\u001B[39mna_action)\n\u001B[0;32m--> 921\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43malgorithms\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43marr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapper\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mna_action\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mna_action\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/algorithms.py:1743\u001B[0m, in \u001B[0;36mmap_array\u001B[0;34m(arr, mapper, na_action, convert)\u001B[0m\n\u001B[1;32m   1741\u001B[0m values \u001B[38;5;241m=\u001B[39m arr\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mobject\u001B[39m, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m na_action \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1743\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_infer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapper\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1745\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m lib\u001B[38;5;241m.\u001B[39mmap_infer_mask(\n\u001B[1;32m   1746\u001B[0m         values, mapper, mask\u001B[38;5;241m=\u001B[39misna(values)\u001B[38;5;241m.\u001B[39mview(np\u001B[38;5;241m.\u001B[39muint8), convert\u001B[38;5;241m=\u001B[39mconvert\n\u001B[1;32m   1747\u001B[0m     )\n",
      "File \u001B[0;32mlib.pyx:2972\u001B[0m, in \u001B[0;36mpandas._libs.lib.map_infer\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m/var/model/genie-in-the-box/src/ephemera/prompts/xml_fine_tuning_prompt_generator.py:1134\u001B[0m, in \u001B[0;36mXmlFineTuningPromptGenerator.generate_responses.<locals>.<lambda>\u001B[0;34m(cell)\u001B[0m\n\u001B[1;32m   1132\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m switch \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtgi\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m   1133\u001B[0m     \u001B[38;5;28mprint\u001B[39m( \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUsing TGI w/ model_name [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m]...\u001B[39m\u001B[38;5;124m\"\u001B[39m )\n\u001B[0;32m-> 1134\u001B[0m     df[ \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mresponse\u001B[39m\u001B[38;5;124m\"\u001B[39m ]  \u001B[38;5;241m=\u001B[39m df[ \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprompt\u001B[39m\u001B[38;5;124m\"\u001B[39m ]\u001B[38;5;241m.\u001B[39mapply( \u001B[38;5;28;01mlambda\u001B[39;00m cell: \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_response_to_prompt\u001B[49m\u001B[43m(\u001B[49m\u001B[43m \u001B[49m\u001B[43mcell\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrows\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mswitch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mswitch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_new_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_new_tokens\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtemperature\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_k\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtop_k\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_p\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtop_p\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msilent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msilent\u001B[49m\u001B[43m \u001B[49m\u001B[43m)\u001B[49m )\n\u001B[1;32m   1135\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m switch \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mopenai\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m   1136\u001B[0m     \u001B[38;5;28mprint\u001B[39m( \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUsing OPENAI w/ model_name [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m]...\u001B[39m\u001B[38;5;124m\"\u001B[39m )\n",
      "File \u001B[0;32m/var/model/genie-in-the-box/src/ephemera/prompts/xml_fine_tuning_prompt_generator.py:1117\u001B[0m, in \u001B[0;36mXmlFineTuningPromptGenerator._get_response_to_prompt\u001B[0;34m(self, prompt, rows, switch, model_name, timer, tokenizer, model, max_new_tokens, temperature, top_k, top_p, device, silent)\u001B[0m\n\u001B[1;32m   1114\u001B[0m         \u001B[38;5;28mprint\u001B[39m( \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mETA: Error \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m )\n\u001B[1;32m   1116\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m switch \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtgi\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m-> 1117\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mquery_llm_tgi\u001B[49m\u001B[43m(\u001B[49m\u001B[43m \u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_new_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_new_tokens\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtemperature\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_k\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtop_k\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_p\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtop_p\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msilent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msilent\u001B[49m\u001B[43m \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1118\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m switch \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mopenai\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m   1119\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_query_llm_openai( prompt[ \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m ], model_name\u001B[38;5;241m=\u001B[39mmodel_name )\n",
      "File \u001B[0;32m/var/model/genie-in-the-box/src/ephemera/prompts/xml_fine_tuning_prompt_generator.py:1039\u001B[0m, in \u001B[0;36mXmlFineTuningPromptGenerator.query_llm_tgi\u001B[0;34m(self, prompt, model_name, max_new_tokens, temperature, top_k, top_p, silent)\u001B[0m\n\u001B[1;32m   1036\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m prompt\u001B[38;5;241m.\u001B[39msplit( \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m ):\n\u001B[1;32m   1037\u001B[0m         \u001B[38;5;28mprint\u001B[39m( line )\n\u001B[0;32m-> 1039\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m token \u001B[38;5;129;01min\u001B[39;00m \u001B[43mclient\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtext_generation\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1040\u001B[0m \u001B[43m    \u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_new_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_new_tokens\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtemperature\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_k\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtop_k\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_p\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtop_p\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop_sequences\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m</response>\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m]\u001B[49m\n\u001B[1;32m   1041\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m   1042\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdebug:\n\u001B[1;32m   1043\u001B[0m         \u001B[38;5;28mprint\u001B[39m( token, end\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m )\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/inference/_client.py:2302\u001B[0m, in \u001B[0;36mInferenceClient.text_generation\u001B[0;34m(self, prompt, details, stream, model, adapter_id, best_of, decoder_input_details, do_sample, frequency_penalty, grammar, max_new_tokens, repetition_penalty, return_full_text, seed, stop, stop_sequences, temperature, top_k, top_n_tokens, top_p, truncate, typical_p, watermark)\u001B[0m\n\u001B[1;32m   2300\u001B[0m \u001B[38;5;66;03m# Handle errors separately for more precise error messages\u001B[39;00m\n\u001B[1;32m   2301\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 2302\u001B[0m     bytes_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpost\u001B[49m\u001B[43m(\u001B[49m\u001B[43mjson\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpayload\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtext-generation\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n\u001B[1;32m   2303\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m HTTPError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m   2304\u001B[0m     match \u001B[38;5;241m=\u001B[39m MODEL_KWARGS_NOT_USED_REGEX\u001B[38;5;241m.\u001B[39msearch(\u001B[38;5;28mstr\u001B[39m(e))\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/inference/_client.py:281\u001B[0m, in \u001B[0;36mInferenceClient.post\u001B[0;34m(self, json, data, model, task, stream)\u001B[0m\n\u001B[1;32m    279\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m _open_as_binary(data) \u001B[38;5;28;01mas\u001B[39;00m data_as_binary:\n\u001B[1;32m    280\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 281\u001B[0m         response \u001B[38;5;241m=\u001B[39m \u001B[43mget_session\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpost\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    282\u001B[0m \u001B[43m            \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    283\u001B[0m \u001B[43m            \u001B[49m\u001B[43mjson\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mjson\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    284\u001B[0m \u001B[43m            \u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_as_binary\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    285\u001B[0m \u001B[43m            \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    286\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcookies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcookies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    287\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    288\u001B[0m \u001B[43m            \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    289\u001B[0m \u001B[43m            \u001B[49m\u001B[43mproxies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    290\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    291\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTimeoutError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n\u001B[1;32m    292\u001B[0m         \u001B[38;5;66;03m# Convert any `TimeoutError` to a `InferenceTimeoutError`\u001B[39;00m\n\u001B[1;32m    293\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m InferenceTimeoutError(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInference call timed out: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00murl\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merror\u001B[39;00m  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py:637\u001B[0m, in \u001B[0;36mSession.post\u001B[0;34m(self, url, data, json, **kwargs)\u001B[0m\n\u001B[1;32m    626\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpost\u001B[39m(\u001B[38;5;28mself\u001B[39m, url, data\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, json\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    627\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Sends a POST request. Returns :class:`Response` object.\u001B[39;00m\n\u001B[1;32m    628\u001B[0m \n\u001B[1;32m    629\u001B[0m \u001B[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    634\u001B[0m \u001B[38;5;124;03m    :rtype: requests.Response\u001B[39;00m\n\u001B[1;32m    635\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 637\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mPOST\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mjson\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mjson\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py:589\u001B[0m, in \u001B[0;36mSession.request\u001B[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001B[0m\n\u001B[1;32m    584\u001B[0m send_kwargs \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    585\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtimeout\u001B[39m\u001B[38;5;124m\"\u001B[39m: timeout,\n\u001B[1;32m    586\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mallow_redirects\u001B[39m\u001B[38;5;124m\"\u001B[39m: allow_redirects,\n\u001B[1;32m    587\u001B[0m }\n\u001B[1;32m    588\u001B[0m send_kwargs\u001B[38;5;241m.\u001B[39mupdate(settings)\n\u001B[0;32m--> 589\u001B[0m resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprep\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43msend_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    591\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m resp\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py:703\u001B[0m, in \u001B[0;36mSession.send\u001B[0;34m(self, request, **kwargs)\u001B[0m\n\u001B[1;32m    700\u001B[0m start \u001B[38;5;241m=\u001B[39m preferred_clock()\n\u001B[1;32m    702\u001B[0m \u001B[38;5;66;03m# Send the request\u001B[39;00m\n\u001B[0;32m--> 703\u001B[0m r \u001B[38;5;241m=\u001B[39m \u001B[43madapter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    705\u001B[0m \u001B[38;5;66;03m# Total elapsed time of the request (approximately)\u001B[39;00m\n\u001B[1;32m    706\u001B[0m elapsed \u001B[38;5;241m=\u001B[39m preferred_clock() \u001B[38;5;241m-\u001B[39m start\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_http.py:93\u001B[0m, in \u001B[0;36mUniqueRequestIdAdapter.send\u001B[0;34m(self, request, *args, **kwargs)\u001B[0m\n\u001B[1;32m     91\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Catch any RequestException to append request id to the error message for debugging.\"\"\"\u001B[39;00m\n\u001B[1;32m     92\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 93\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     94\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m requests\u001B[38;5;241m.\u001B[39mRequestException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     95\u001B[0m     request_id \u001B[38;5;241m=\u001B[39m request\u001B[38;5;241m.\u001B[39mheaders\u001B[38;5;241m.\u001B[39mget(X_AMZN_TRACE_ID)\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/requests/adapters.py:700\u001B[0m, in \u001B[0;36mHTTPAdapter.send\u001B[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001B[0m\n\u001B[1;32m    696\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(e\u001B[38;5;241m.\u001B[39mreason, _SSLError):\n\u001B[1;32m    697\u001B[0m         \u001B[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001B[39;00m\n\u001B[1;32m    698\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m SSLError(e, request\u001B[38;5;241m=\u001B[39mrequest)\n\u001B[0;32m--> 700\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mConnectionError\u001B[39;00m(e, request\u001B[38;5;241m=\u001B[39mrequest)\n\u001B[1;32m    702\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m ClosedPoolError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    703\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mConnectionError\u001B[39;00m(e, request\u001B[38;5;241m=\u001B[39mrequest)\n",
      "\u001B[0;31mConnectionError\u001B[0m: (MaxRetryError(\"HTTPConnectionPool(host='127.0.0.1', port=3000): Max retries exceeded with url: /v1 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f78d1acd1e0>: Failed to establish a new connection: [Errno 111] Connection refused'))\"), '(Request ID: a83b0645-60e5-49c4-bde2-5df9b885c13f)')"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "\n",
    "# from ephemera.prompts.xml_fine_tuning_prompt_generator import XmlFineTuningPromptGenerator\n",
    "import pandas as pd\n",
    "\n",
    "# tgi_validator  = XmlFineTuningPromptGenerator( path_prefix=\"/var/model/genie-in-the-box\", tgi_url=\"http://192.168.1.21:3000\", debug=True )\n",
    "# tgi_validator  = XmlFineTuningPromptGenerator( path_prefix=\"/var/model/genie-in-the-box\", tgi_url=\"http://172.17.0.3:3000\", debug=True )\n",
    "tgi_validator  = XmlFineTuningPromptGenerator( path_prefix=\"/var/model/genie-in-the-box\", tgi_url=\"http://127.0.0.1:3000/v1\", debug=True ) \n",
    "\n",
    "# model_name     = \"ministral/Ministral-3b-instruct-raw-bfloat16\"\n",
    "\n",
    "sample_size    = 10\n",
    "validate_df    = pd.read_json( \"/var/model/genie-in-the-box/src/ephemera/prompts/data/voice-commands-xml-validate.jsonl\", lines=True ).sample( sample_size, random_state=42 )\n",
    "validate_df    = tgi_validator.generate_responses( validate_df, switch=\"tgi\", model_name=model_name )\n",
    "validate_df    = tgi_validator.validate_responses( validate_df )\n",
    "\n",
    "tgi_validator.print_validation_stats( validate_df, title=f\"Validation Stats for {sample_size} rows with `{model_name}` on TGI:3000\" )\n",
    "\n",
    "# Generating responses for 10 rows... Done! in 7 seconds\n",
    "# [771.2] ms per item\n",
    "# \n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# - Validation Stats for 10 rows with `mistralai/Mistral-7B-Instruct-v0.2-raw-bfloat16` on TGI:3000\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# \n",
    "#                Is valid xml 100.0%\n",
    "#           Contains response 100.0%\n",
    "#  Contains <browser-command> 100.0%\n",
    "#             Contains <args> 100.0%\n",
    "#           Response is exact 100.0%\n",
    "# Response has correct values 100.0%\n",
    "#  Browser command is correct 100.0%\n",
    "#             Args is correct 100.0%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db301705e3d2e4b",
   "metadata": {},
   "source": [
    "## Quantize using AutoRound and write to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea0a71d6733d82f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T20:02:04.078228Z",
     "start_time": "2025-01-13T20:02:02.688631Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: auto-round in /usr/local/lib/python3.10/dist-packages (0.4.4)\r\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from auto-round) (1.2.0.dev0)\r\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from auto-round) (3.2.0)\r\n",
      "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from auto-round) (9.0.0)\r\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from auto-round) (0.2.0)\r\n",
      "Requirement already satisfied: numpy<2.0 in /usr/local/lib/python3.10/dist-packages (from auto-round) (1.26.4)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from auto-round) (4.67.1)\r\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from auto-round) (24.2)\r\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from auto-round) (11.1.0)\r\n",
      "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from auto-round) (0.60.0)\r\n",
      "Requirement already satisfied: tbb in /usr/local/lib/python3.10/dist-packages (from auto-round) (2022.0.0)\r\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from auto-round) (2.5.1)\r\n",
      "Requirement already satisfied: transformers>=4.38 in /usr/local/lib/python3.10/dist-packages (from auto-round) (4.46.3)\r\n",
      "Requirement already satisfied: threadpoolctl in /usr/local/lib/python3.10/dist-packages (from auto-round) (3.5.0)\r\n",
      "Requirement already satisfied: lm-eval<0.5,>=0.4.2 in /usr/local/lib/python3.10/dist-packages (from auto-round) (0.4.7)\r\n",
      "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (from lm-eval<0.5,>=0.4.2->auto-round) (0.4.3)\r\n",
      "Requirement already satisfied: jsonlines in /usr/local/lib/python3.10/dist-packages (from lm-eval<0.5,>=0.4.2->auto-round) (4.0.0)\r\n",
      "Requirement already satisfied: numexpr in /usr/local/lib/python3.10/dist-packages (from lm-eval<0.5,>=0.4.2->auto-round) (2.10.2)\r\n",
      "Requirement already satisfied: peft>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from lm-eval<0.5,>=0.4.2->auto-round) (0.14.1.dev0)\r\n",
      "Requirement already satisfied: pybind11>=2.6.2 in /usr/local/lib/python3.10/dist-packages (from lm-eval<0.5,>=0.4.2->auto-round) (2.13.6)\r\n",
      "Requirement already satisfied: pytablewriter in /usr/local/lib/python3.10/dist-packages (from lm-eval<0.5,>=0.4.2->auto-round) (1.2.1)\r\n",
      "Requirement already satisfied: rouge-score>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from lm-eval<0.5,>=0.4.2->auto-round) (0.1.2)\r\n",
      "Requirement already satisfied: sacrebleu>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from lm-eval<0.5,>=0.4.2->auto-round) (2.5.1)\r\n",
      "Requirement already satisfied: scikit-learn>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from lm-eval<0.5,>=0.4.2->auto-round) (1.6.0)\r\n",
      "Requirement already satisfied: sqlitedict in /usr/local/lib/python3.10/dist-packages (from lm-eval<0.5,>=0.4.2->auto-round) (2.1.0)\r\n",
      "Requirement already satisfied: tqdm-multiprocess in /usr/local/lib/python3.10/dist-packages (from lm-eval<0.5,>=0.4.2->auto-round) (0.0.11)\r\n",
      "Requirement already satisfied: zstandard in /usr/local/lib/python3.10/dist-packages (from lm-eval<0.5,>=0.4.2->auto-round) (0.23.0)\r\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from lm-eval<0.5,>=0.4.2->auto-round) (0.3.8)\r\n",
      "Requirement already satisfied: word2number in /usr/local/lib/python3.10/dist-packages (from lm-eval<0.5,>=0.4.2->auto-round) (1.1)\r\n",
      "Requirement already satisfied: more_itertools in /usr/local/lib/python3.10/dist-packages (from lm-eval<0.5,>=0.4.2->auto-round) (10.5.0)\r\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->auto-round) (6.1.0)\r\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate->auto-round) (6.0.2)\r\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate->auto-round) (0.26.5)\r\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate->auto-round) (0.4.5)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets->auto-round) (3.16.1)\r\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->auto-round) (18.1.0)\r\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->auto-round) (2.2.3)\r\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets->auto-round) (2.32.3)\r\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->auto-round) (3.5.0)\r\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets->auto-round) (0.70.16)\r\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets->auto-round) (2024.9.0)\r\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->auto-round) (3.11.10)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->auto-round) (4.12.2)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->auto-round) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->auto-round) (3.1.4)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch->auto-round) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch->auto-round) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch->auto-round) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch->auto-round) (9.1.0.70)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.10/dist-packages (from torch->auto-round) (12.4.5.8)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.10/dist-packages (from torch->auto-round) (11.2.1.3)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.10/dist-packages (from torch->auto-round) (10.3.5.147)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.10/dist-packages (from torch->auto-round) (11.6.1.9)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.10/dist-packages (from torch->auto-round) (12.3.1.170)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.10/dist-packages (from torch->auto-round) (2.21.5)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch->auto-round) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch->auto-round) (12.4.127)\r\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->auto-round) (3.1.0)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->auto-round) (1.13.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->auto-round) (1.3.0)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.38->auto-round) (2024.11.6)\r\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.38->auto-round) (0.20.3)\r\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->auto-round) (0.43.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb->auto-round) (1.2.0)\r\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-round) (2.4.4)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-round) (1.3.2)\r\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-round) (5.0.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-round) (24.2.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-round) (1.5.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-round) (6.1.0)\r\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-round) (0.2.1)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-round) (1.18.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->auto-round) (3.4.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->auto-round) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->auto-round) (2.2.3)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->auto-round) (2024.8.30)\r\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score>=0.0.4->lm-eval<0.5,>=0.4.2->auto-round) (2.1.0)\r\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score>=0.0.4->lm-eval<0.5,>=0.4.2->auto-round) (3.9.1)\r\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score>=0.0.4->lm-eval<0.5,>=0.4.2->auto-round) (1.17.0)\r\n",
      "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.5.0->lm-eval<0.5,>=0.4.2->auto-round) (3.1.1)\r\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.5.0->lm-eval<0.5,>=0.4.2->auto-round) (0.9.0)\r\n",
      "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.5.0->lm-eval<0.5,>=0.4.2->auto-round) (0.4.6)\r\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.5.0->lm-eval<0.5,>=0.4.2->auto-round) (5.3.0)\r\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.1->lm-eval<0.5,>=0.4.2->auto-round) (1.14.1)\r\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.1->lm-eval<0.5,>=0.4.2->auto-round) (1.4.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->auto-round) (3.0.2)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->auto-round) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->auto-round) (2024.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->auto-round) (2024.2)\r\n",
      "Requirement already satisfied: setuptools>=38.3.0 in /usr/lib/python3/dist-packages (from pytablewriter->lm-eval<0.5,>=0.4.2->auto-round) (59.6.0)\r\n",
      "Requirement already satisfied: DataProperty<2,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from pytablewriter->lm-eval<0.5,>=0.4.2->auto-round) (1.1.0)\r\n",
      "Requirement already satisfied: mbstrdecoder<2,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytablewriter->lm-eval<0.5,>=0.4.2->auto-round) (1.1.3)\r\n",
      "Requirement already satisfied: pathvalidate<4,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from pytablewriter->lm-eval<0.5,>=0.4.2->auto-round) (3.2.3)\r\n",
      "Requirement already satisfied: tabledata<2,>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from pytablewriter->lm-eval<0.5,>=0.4.2->auto-round) (1.3.4)\r\n",
      "Requirement already satisfied: tcolorpy<1,>=0.0.5 in /usr/local/lib/python3.10/dist-packages (from pytablewriter->lm-eval<0.5,>=0.4.2->auto-round) (0.1.7)\r\n",
      "Requirement already satisfied: typepy<2,>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm-eval<0.5,>=0.4.2->auto-round) (1.3.4)\r\n",
      "Requirement already satisfied: chardet<6,>=3.0.4 in /usr/local/lib/python3.10/dist-packages (from mbstrdecoder<2,>=1.0.0->pytablewriter->lm-eval<0.5,>=0.4.2->auto-round) (5.2.0)\r\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score>=0.0.4->lm-eval<0.5,>=0.4.2->auto-round) (8.1.7)\r\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001B[0m\u001B[33m\r\n",
      "\u001B[0m"
     ]
    }
   ],
   "source": [
    "!pip install auto-round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1ae7eb57f92a5d3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T20:06:20.944363Z",
     "start_time": "2025-01-13T20:06:19.600728Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optimum in /usr/local/lib/python3.10/dist-packages (1.23.3)\r\n",
      "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from optimum) (15.0.1)\r\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from optimum) (1.13.1)\r\n",
      "Requirement already satisfied: transformers>=4.29 in /usr/local/lib/python3.10/dist-packages (from optimum) (4.46.3)\r\n",
      "Requirement already satisfied: torch>=1.11 in /usr/local/lib/python3.10/dist-packages (from optimum) (2.5.1)\r\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from optimum) (24.2)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optimum) (1.26.4)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from optimum) (0.26.5)\r\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from optimum) (3.2.0)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.8.0->optimum) (3.16.1)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.8.0->optimum) (2024.9.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.8.0->optimum) (6.0.2)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.8.0->optimum) (2.32.3)\r\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.8.0->optimum) (4.67.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.8.0->optimum) (4.12.2)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->optimum) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->optimum) (3.1.4)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->optimum) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->optimum) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->optimum) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->optimum) (9.1.0.70)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->optimum) (12.4.5.8)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->optimum) (11.2.1.3)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->optimum) (10.3.5.147)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->optimum) (11.6.1.9)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->optimum) (12.3.1.170)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->optimum) (2.21.5)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->optimum) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->optimum) (12.4.127)\r\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->optimum) (3.1.0)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->optimum) (1.3.0)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.29->optimum) (2024.11.6)\r\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.29->optimum) (0.20.3)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.29->optimum) (0.4.5)\r\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->optimum) (10.0)\r\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->optimum) (18.1.0)\r\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->optimum) (0.3.8)\r\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->optimum) (2.2.3)\r\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->optimum) (3.5.0)\r\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets->optimum) (0.70.16)\r\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->optimum) (3.11.10)\r\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum) (2.4.4)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum) (1.3.2)\r\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum) (5.0.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum) (24.2.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum) (1.5.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum) (6.1.0)\r\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum) (0.2.1)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum) (1.18.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.8.0->optimum) (3.4.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.8.0->optimum) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.8.0->optimum) (2.2.3)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.8.0->optimum) (2024.8.30)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11->optimum) (3.0.2)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->optimum) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->optimum) (2024.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->optimum) (2024.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->optimum) (1.17.0)\r\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001B[0m\u001B[33m\r\n",
      "\u001B[0m"
     ]
    }
   ],
   "source": [
    "! pip install --upgrade optimum\n",
    "# ! pip install --upgrade auto-gptq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f7a43e3e8b189ba7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T20:05:27.295282Z",
     "start_time": "2025-01-13T20:05:27.292803Z"
    }
   },
   "outputs": [],
   "source": [
    "# ! pip show optimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2ffe96ed3e423f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T18:01:06.351567Z",
     "start_time": "2025-01-13T17:39:14.046351Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-13 17:39:14,365 INFO utils.py L149: Note: NumExpr detected 32 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "2025-01-13 17:39:14,365 INFO utils.py L162: NumExpr defaulting to 16 threads.\n",
      "\u001B[38;20m2025-01-13 17:39:20 INFO utils.py L577: Using GPU device\u001B[0m\n",
      "\u001B[38;20m2025-01-13 17:39:20 INFO autoround.py L230: using torch.float16 for quantization tuning\u001B[0m\n",
      "\u001B[38;20m2025-01-13 17:39:20 INFO autoround.py L300: start to cache block inputs\u001B[0m\n",
      "2025-01-13 17:39:21,023 INFO config.py L54: PyTorch version 2.5.1 available.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6005e3508674c9cb80d66d00881da83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/373 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7ee2fce6c5a401abfd94581c20b72de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dataset_infos.json:   0%|          | 0.00/921 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef3f84e0b46d426a9c5b63bd865a11ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "()-00000-of-00001-4746b8785c874cc7.parquet:   0%|          | 0.00/33.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b17d80877b8f49eb801aef30eb837efb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d225b0320834176b5e106e56fd1acf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2542728fc9ce47fcbe19cb8203c62384",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[38;20m2025-01-13 17:39:42 INFO autoround.py L305: caching done\u001B[0m\n",
      "Quantizing model.layers.0:   0%|                                                                                                                                            | 0/36 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at ../aten/src/ATen/Context.cpp:208.)\n",
      "  return F.linear(input, self.weight, self.bias)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/mistral/modeling_mistral.py:106: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at ../aten/src/ATen/Context.cpp:208.)\n",
      "  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at ../aten/src/ATen/Context.cpp:208.)\n",
      "  return F.linear(input, self.weight, self.bias)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/mistral/modeling_mistral.py:106: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at ../aten/src/ATen/Context.cpp:208.)\n",
      "  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n",
      "/usr/local/lib/python3.10/dist-packages/auto_round/quantizer.py:269: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at ../aten/src/ATen/Context.cpp:208.)\n",
      "  return F.linear(x, weight, bias)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:825: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at ../aten/src/ATen/Context.cpp:208.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "Quantizing model.layers.35: 100%|| 36/36 [21:23<00:00, 35.67s/it]\u001B[38;20m2025-01-13 18:01:06 INFO autoround.py L340: quantization tuning time 1305.894364118576\u001B[0m\n",
      "\u001B[38;20m2025-01-13 18:01:06 INFO autoround.py L356: Summary: quantized 252/253 in the model,  ['lm_head'] have not been quantized\u001B[0m\n",
      "Quantizing model.layers.35: 100%|| 36/36 [21:23<00:00, 35.67s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(MistralForCausalLM(\n",
       "   (model): MistralModel(\n",
       "     (embed_tokens): Embedding(131072, 4096)\n",
       "     (layers): ModuleList(\n",
       "       (0-35): 36 x MistralDecoderLayer(\n",
       "         (self_attn): MistralFlashAttention2(\n",
       "           (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "           (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "           (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "           (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "           (rotary_emb): MistralRotaryEmbedding()\n",
       "         )\n",
       "         (mlp): MistralMLP(\n",
       "           (gate_proj): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "           (up_proj): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "           (down_proj): Linear(in_features=12288, out_features=4096, bias=False)\n",
       "           (act_fn): SiLU()\n",
       "         )\n",
       "         (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "         (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "       )\n",
       "     )\n",
       "     (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "   )\n",
       "   (lm_head): Linear(in_features=4096, out_features=131072, bias=False)\n",
       " ),\n",
       " {'model.layers.0.self_attn.q_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[2.125e-03, -1.947e-03, -2.403e-03,  ..., -2.043e-03, 2.209e-03, -1.841e-03],\n",
       "           [-6.210e-03, 4.631e-03, 3.475e-03,  ..., 2.571e-03, -2.653e-03, 3.838e-03],\n",
       "           [-8.057e-03, -6.172e-03, -3.178e-03,  ..., -2.548e-03, 3.721e-03, -4.608e-03],\n",
       "           ...,\n",
       "           [-5.302e-03, 2.321e-03, 3.145e-03,  ..., 2.426e-03, 2.304e-03, 2.546e-03],\n",
       "           [-2.497e-03, 2.703e-03, 1.966e-03,  ..., -2.825e-03, 2.861e-03, -3.393e-03],\n",
       "           [3.670e-03, 1.602e-03, -1.872e-03,  ..., -2.254e-03, 1.989e-03, -2.275e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.0.self_attn.k_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[3.803e-03, 2.821e-03, -2.861e-03,  ..., 2.466e-03, 2.413e-03, 2.598e-03],\n",
       "           [2.689e-03, -2.831e-03, 3.244e-03,  ..., -1.907e-03, 1.863e-03, 2.043e-03],\n",
       "           [-3.080e-03, 2.075e-03, -2.531e-03,  ..., 1.984e-03, 1.643e-03, 2.039e-03],\n",
       "           ...,\n",
       "           [-5.032e-03, 4.181e-03, -5.390e-03,  ..., 4.883e-03, 6.153e-03, 4.822e-03],\n",
       "           [-5.989e-03, 3.490e-03, -4.734e-03,  ..., 4.280e-03, 5.215e-03, 4.044e-03],\n",
       "           [-5.306e-03, -3.633e-03, -4.349e-03,  ..., -3.866e-03, -4.440e-03, -4.421e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.0.self_attn.v_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-4.630e-04, -5.989e-04, -4.420e-04,  ..., -5.226e-04, 6.571e-04, 6.242e-04],\n",
       "           [3.610e-04, -3.655e-04, -6.080e-04,  ..., 6.094e-04, -7.286e-04, -6.742e-04],\n",
       "           [5.136e-04, -4.911e-04, 5.488e-04,  ..., 7.262e-04, 8.297e-04, -6.843e-04],\n",
       "           ...,\n",
       "           [1.012e-03, 6.943e-04, -7.715e-04,  ..., 9.880e-04, -9.651e-04, -8.731e-04],\n",
       "           [-5.307e-04, -5.846e-04, -6.900e-04,  ..., 8.774e-04, 1.107e-03, -9.050e-04],\n",
       "           [4.849e-04, 5.860e-04, -5.598e-04,  ..., -9.575e-04, -8.779e-04, -9.031e-04]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.0.self_attn.o_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[1.923e-02, 1.854e-02, 1.944e-02,  ..., -2.408e-02, -2.171e-02, -2.831e-02],\n",
       "           [-3.157e-03, 2.642e-03, 3.298e-03,  ..., -2.178e-03, 3.160e-03, -1.601e-03],\n",
       "           [-4.002e-03, -4.822e-03, -4.753e-03,  ..., 1.824e-03, 1.891e-03, 2.293e-03],\n",
       "           ...,\n",
       "           [-7.439e-04, 1.040e-03, 9.060e-04,  ..., 1.331e-03, 9.260e-04, -6.213e-04],\n",
       "           [6.747e-04, 1.033e-03, -1.066e-03,  ..., -1.419e-03, -8.168e-04, -7.124e-04],\n",
       "           [-1.416e-03, 8.855e-04, 7.634e-04,  ..., 1.249e-03, -9.918e-04, 6.404e-04]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.0.mlp.gate_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[1.566e-02, 7.343e-03, -7.229e-03,  ..., 6.321e-03, -6.882e-03, -8.812e-03],\n",
       "           [-4.520e-03, 3.094e-03, -3.857e-03,  ..., 1.543e-03, -1.310e-03, -1.440e-03],\n",
       "           [7.755e-03, 4.906e-03, -3.618e-03,  ..., -2.808e-03, 1.899e-03, -3.252e-03],\n",
       "           ...,\n",
       "           [1.521e-03, 1.407e-03, -1.756e-03,  ..., 1.497e-03, 1.822e-03, 1.666e-03],\n",
       "           [-1.421e-03, 1.506e-03, 1.558e-03,  ..., 1.542e-03, 1.549e-03, -1.722e-03],\n",
       "           [-1.637e-03, -1.410e-03, -1.432e-03,  ..., -1.516e-03, 1.695e-03, -1.674e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.0.mlp.up_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-6.645e-03, 2.689e-03, 2.380e-03,  ..., -2.882e-03, -3.033e-03, 3.044e-03],\n",
       "           [2.949e-03, 2.680e-03, -2.306e-03,  ..., -1.034e-03, -1.192e-03, 1.082e-03],\n",
       "           [4.299e-03, 2.197e-03, 1.353e-03,  ..., -1.129e-03, 8.841e-04, -9.918e-04],\n",
       "           ...,\n",
       "           [1.074e-03, -1.370e-03, 1.316e-03,  ..., -2.026e-03, 1.547e-03, -1.712e-03],\n",
       "           [-1.409e-03, -1.083e-03, 1.226e-03,  ..., -1.303e-03, 1.657e-03, 1.814e-03],\n",
       "           [-1.648e-03, 1.554e-03, -1.562e-03,  ..., -1.358e-03, -9.928e-04, -1.298e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.0.mlp.down_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-2.997e-02, -2.458e-02, -8.354e-03,  ..., -2.399e-03, 5.421e-03, 3.756e-03],\n",
       "           [-1.537e-02, -2.699e-03, -3.586e-03,  ..., 1.091e-03, -1.350e-03, -2.518e-03],\n",
       "           [-3.500e-02, -1.261e-02, -3.128e-03,  ..., -3.967e-03, -9.604e-04, -1.057e-03],\n",
       "           ...,\n",
       "           [1.407e-03, 1.366e-03, -9.966e-04,  ..., -1.370e-03, 1.328e-03, -1.236e-03],\n",
       "           [1.138e-03, -1.276e-03, 1.163e-03,  ..., 1.490e-03, 1.538e-03, -1.664e-03],\n",
       "           [9.575e-04, 1.290e-03, -1.298e-03,  ..., 1.742e-03, -1.464e-03, 1.373e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.1.self_attn.q_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[3.613e-03, -3.593e-03, -3.069e-03,  ..., 1.868e-03, -2.064e-03, 1.838e-03],\n",
       "           [1.473e-03, -1.089e-03, 1.079e-03,  ..., -8.645e-04, 9.890e-04, 1.057e-03],\n",
       "           [-1.633e-03, -1.441e-03, -1.417e-03,  ..., 1.143e-03, -1.224e-03, -1.000e-03],\n",
       "           ...,\n",
       "           [-1.228e-03, -1.514e-03, 1.497e-03,  ..., -1.259e-03, 2.235e-03, 1.524e-03],\n",
       "           [1.963e-03, -1.680e-03, -1.594e-03,  ..., -1.451e-03, 2.056e-03, -1.837e-03],\n",
       "           [1.373e-03, -1.667e-03, 2.169e-03,  ..., -1.555e-03, 1.691e-03, 1.881e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.1.self_attn.k_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[4.463e-03, -2.932e-03, -2.087e-03,  ..., -1.384e-03, 1.289e-03, -1.946e-03],\n",
       "           [-2.773e-03, -1.849e-03, -2.111e-03,  ..., 2.182e-03, 1.870e-03, 2.285e-03],\n",
       "           [-4.589e-03, -3.843e-03, -2.832e-03,  ..., 2.171e-03, -2.451e-03, 2.069e-03],\n",
       "           ...,\n",
       "           [-2.651e-03, -3.990e-03, -3.248e-03,  ..., 3.386e-03, 4.196e-03, -3.881e-03],\n",
       "           [-2.735e-03, 4.948e-03, 3.843e-03,  ..., 3.420e-03, -3.922e-03, 3.801e-03],\n",
       "           [2.867e-03, -4.108e-03, 5.127e-03,  ..., -4.425e-03, -3.622e-03, 3.399e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.1.self_attn.v_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[6.247e-04, 9.360e-04, -9.985e-04,  ..., 1.263e-03, 1.269e-03, -8.001e-04],\n",
       "           [6.957e-04, -1.505e-03, -1.249e-03,  ..., -9.556e-04, 7.591e-04, 7.806e-04],\n",
       "           [9.542e-04, -1.022e-03, -9.265e-04,  ..., 1.034e-03, -1.215e-03, -1.032e-03],\n",
       "           ...,\n",
       "           [-8.483e-04, -1.260e-03, -9.823e-04,  ..., 8.183e-04, 1.075e-03, -8.807e-04],\n",
       "           [6.909e-04, -9.074e-04, -7.262e-04,  ..., -1.033e-03, 1.128e-03, 8.607e-04],\n",
       "           [-7.319e-04, 1.110e-03, 7.105e-04,  ..., 9.618e-04, 8.826e-04, -1.420e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.1.self_attn.o_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-1.683e-02, 1.674e-02, 2.042e-02,  ..., 1.210e-02, -6.462e-03, -1.213e-02],\n",
       "           [-2.796e-03, 2.954e-03, 3.447e-03,  ..., 2.104e-03, -1.613e-03, -2.178e-03],\n",
       "           [-2.186e-03, -2.396e-03, -2.716e-03,  ..., -2.022e-03, -1.078e-03, -2.296e-03],\n",
       "           ...,\n",
       "           [1.642e-03, -1.475e-03, 1.325e-03,  ..., 1.344e-03, -1.152e-03, -1.030e-03],\n",
       "           [1.193e-03, 1.152e-03, 1.364e-03,  ..., 1.249e-03, -1.357e-03, 9.494e-04],\n",
       "           [-1.135e-03, -1.225e-03, 1.399e-03,  ..., -1.380e-03, -1.033e-03, 9.136e-04]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.1.mlp.gate_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[1.808e-02, -6.741e-03, -5.066e-03,  ..., 3.138e-03, -3.433e-03, 4.177e-03],\n",
       "           [8.995e-03, 6.836e-03, 4.154e-03,  ..., 2.855e-03, 2.119e-03, 2.485e-03],\n",
       "           [-1.120e-02, 9.354e-03, 7.618e-03,  ..., -2.995e-03, -3.754e-03, -3.696e-03],\n",
       "           ...,\n",
       "           [1.440e-03, 1.422e-03, 1.321e-03,  ..., 1.656e-03, -1.479e-03, 1.481e-03],\n",
       "           [-1.674e-03, -1.761e-03, -1.312e-03,  ..., -1.678e-03, -1.514e-03, 1.732e-03],\n",
       "           [-1.281e-03, -1.117e-03, -1.534e-03,  ..., 1.310e-03, 1.547e-03, 1.370e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.1.mlp.up_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[7.790e-03, 1.774e-03, 9.751e-04,  ..., 8.345e-04, -1.110e-03, -1.106e-03],\n",
       "           [-3.098e-03, -2.424e-03, -1.984e-03,  ..., -1.201e-03, 1.047e-03, -1.078e-03],\n",
       "           [3.719e-03, -3.132e-03, 2.037e-03,  ..., 7.300e-04, -8.664e-04, 1.061e-03],\n",
       "           ...,\n",
       "           [-1.472e-03, 1.455e-03, -1.319e-03,  ..., 1.312e-03, -2.018e-03, 1.155e-03],\n",
       "           [-1.467e-03, 1.475e-03, 1.205e-03,  ..., -1.483e-03, 1.593e-03, -1.712e-03],\n",
       "           [-1.299e-03, -1.438e-03, 1.328e-03,  ..., 1.306e-03, -1.507e-03, -1.634e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.1.mlp.down_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-2.463e-02, -2.092e-02, 2.011e-02,  ..., 2.111e-03, 2.518e-03, -2.573e-03],\n",
       "           [1.080e-02, 4.547e-03, 4.147e-03,  ..., -7.844e-04, -6.776e-04, 7.725e-04],\n",
       "           [-1.287e-02, 4.475e-03, 1.001e-02,  ..., -9.418e-04, 7.515e-04, 6.461e-04],\n",
       "           ...,\n",
       "           [-1.427e-03, 1.341e-03, -1.142e-03,  ..., -1.232e-03, 1.848e-03, -1.385e-03],\n",
       "           [1.409e-03, -1.234e-03, 1.422e-03,  ..., -1.368e-03, 1.705e-03, 1.127e-03],\n",
       "           [1.076e-03, -1.307e-03, 1.059e-03,  ..., -1.579e-03, -1.261e-03, 1.542e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.2.self_attn.q_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-2.716e-03, -2.274e-03, -1.892e-03,  ..., -1.617e-03, 2.296e-03, 1.839e-03],\n",
       "           [1.592e-03, -1.829e-03, -1.220e-03,  ..., 1.455e-03, -1.410e-03, 1.647e-03],\n",
       "           [3.386e-03, -2.678e-03, -2.420e-03,  ..., -1.530e-03, 1.884e-03, -1.497e-03],\n",
       "           ...,\n",
       "           [-1.982e-03, -1.984e-03, 2.363e-03,  ..., -1.801e-03, 2.235e-03, -1.898e-03],\n",
       "           [-6.283e-03, -1.966e-03, -2.209e-03,  ..., -1.787e-03, 1.935e-03, -2.665e-03],\n",
       "           [-3.317e-03, -2.316e-03, 2.197e-03,  ..., 2.964e-03, 2.043e-03, 1.950e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.2.self_attn.k_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[1.844e-03, -2.262e-03, -2.165e-03,  ..., 2.306e-03, 1.989e-03, -2.485e-03],\n",
       "           [2.390e-03, -1.925e-03, 1.652e-03,  ..., 1.619e-03, -1.202e-03, 1.575e-03],\n",
       "           [2.522e-03, -1.760e-03, 1.914e-03,  ..., 1.480e-03, -1.428e-03, -1.917e-03],\n",
       "           ...,\n",
       "           [5.302e-03, 5.768e-03, 7.446e-03,  ..., -3.489e-03, 2.489e-03, -2.758e-03],\n",
       "           [6.832e-03, 4.204e-03, 3.473e-03,  ..., 2.281e-03, -2.167e-03, 1.847e-03],\n",
       "           [5.821e-03, -3.845e-03, -4.635e-03,  ..., -3.500e-03, 2.542e-03, 2.317e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.2.self_attn.v_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[7.181e-04, -1.097e-03, 1.096e-03,  ..., 1.398e-03, 1.395e-03, 1.695e-03],\n",
       "           [-6.733e-04, 8.516e-04, 8.764e-04,  ..., 1.158e-03, 1.255e-03, -1.422e-03],\n",
       "           [8.421e-04, -9.871e-04, -1.582e-03,  ..., -1.729e-03, -1.496e-03, 1.601e-03],\n",
       "           ...,\n",
       "           [7.129e-04, 8.321e-04, -9.704e-04,  ..., -9.623e-04, -1.520e-03, -1.382e-03],\n",
       "           [-9.880e-04, 9.661e-04, 1.243e-03,  ..., 1.819e-03, -1.198e-03, 1.200e-03],\n",
       "           [1.003e-03, -8.292e-04, -1.128e-03,  ..., -1.046e-03, 9.260e-04, 1.070e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.2.self_attn.o_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[1.904e-02, 1.755e-02, 1.830e-02,  ..., 1.595e-02, 1.160e-02, 7.671e-03],\n",
       "           [-1.766e-03, -1.289e-03, -1.267e-03,  ..., -1.622e-03, -8.826e-04, 1.053e-03],\n",
       "           [-2.380e-03, -2.270e-03, -2.161e-03,  ..., 2.884e-03, 1.712e-03, 1.015e-03],\n",
       "           ...,\n",
       "           [-1.577e-03, 1.120e-03, -1.457e-03,  ..., -1.057e-03, -1.382e-03, -1.420e-03],\n",
       "           [1.529e-03, -1.358e-03, -1.225e-03,  ..., 1.051e-03, 1.356e-03, 1.436e-03],\n",
       "           [-1.707e-03, 1.926e-03, -1.369e-03,  ..., 1.539e-03, -1.039e-03, -1.129e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.2.mlp.gate_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-3.296e-02, -7.359e-03, 5.924e-03,  ..., -7.084e-03, -6.477e-03, 1.096e-02],\n",
       "           [9.682e-03, -3.626e-03, -2.371e-03,  ..., -1.768e-03, -2.712e-03, 2.214e-03],\n",
       "           [6.214e-03, -3.723e-03, -2.100e-03,  ..., -1.708e-03, 1.848e-03, -1.745e-03],\n",
       "           ...,\n",
       "           [-1.209e-03, 1.580e-03, 1.287e-03,  ..., 1.751e-03, 1.848e-03, -1.672e-03],\n",
       "           [-1.650e-03, 1.243e-03, -1.519e-03,  ..., 1.955e-03, 1.870e-03, -1.652e-03],\n",
       "           [-1.724e-03, -1.601e-03, -1.907e-03,  ..., -1.929e-03, -1.494e-03, 1.584e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.2.mlp.up_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-2.800e-02, -9.346e-03, 6.447e-03,  ..., -1.112e-02, 8.919e-03, 1.450e-02],\n",
       "           [-6.882e-03, -1.863e-03, -1.115e-03,  ..., -8.702e-04, 9.294e-04, 1.034e-03],\n",
       "           [-5.844e-03, 2.077e-03, 1.912e-03,  ..., -1.055e-03, 9.384e-04, -8.850e-04],\n",
       "           ...,\n",
       "           [1.459e-03, 1.528e-03, -1.248e-03,  ..., 1.455e-03, -1.695e-03, -1.205e-03],\n",
       "           [-1.580e-03, -1.316e-03, -1.451e-03,  ..., -1.799e-03, -1.999e-03, -1.290e-03],\n",
       "           [-1.455e-03, 1.376e-03, 1.161e-03,  ..., -1.700e-03, -1.842e-03, 1.561e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.2.mlp.down_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-3.879e-02, -2.876e-02, 1.202e-02,  ..., 3.107e-03, 3.361e-03, 2.659e-03],\n",
       "           [-5.348e-03, 2.426e-03, 5.280e-03,  ..., 9.260e-04, -1.236e-03, 8.240e-04],\n",
       "           [-1.778e-02, -8.392e-03, 3.136e-03,  ..., -8.554e-04, 6.509e-04, -7.348e-04],\n",
       "           ...,\n",
       "           [1.120e-03, -1.312e-03, -1.038e-03,  ..., -1.332e-03, -1.561e-03, -1.357e-03],\n",
       "           [1.311e-03, 1.538e-03, -1.341e-03,  ..., 1.362e-03, -1.472e-03, -1.703e-03],\n",
       "           [9.422e-04, 1.417e-03, -1.598e-03,  ..., 1.388e-03, -1.521e-03, 1.396e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.3.self_attn.q_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-3.778e-03, 2.308e-03, 2.481e-03,  ..., 1.807e-03, 2.131e-03, -1.920e-03],\n",
       "           [2.813e-03, -3.349e-03, -2.695e-03,  ..., 1.861e-03, 1.606e-03, -2.161e-03],\n",
       "           [1.883e-03, -2.289e-03, -1.740e-03,  ..., -2.214e-03, -2.171e-03, -2.291e-03],\n",
       "           ...,\n",
       "           [1.713e-03, 1.995e-03, 1.714e-03,  ..., 2.012e-03, 2.537e-03, 2.039e-03],\n",
       "           [2.739e-03, 1.843e-03, -2.075e-03,  ..., 2.645e-03, -2.317e-03, -2.174e-03],\n",
       "           [2.155e-03, -1.910e-03, -1.337e-03,  ..., 2.316e-03, 2.520e-03, -2.151e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.3.self_attn.k_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-1.555e-03, 1.410e-03, 1.670e-03,  ..., -1.554e-03, -1.690e-03, -1.503e-03],\n",
       "           [-2.460e-03, 1.765e-03, 1.882e-03,  ..., -1.902e-03, -1.867e-03, 1.897e-03],\n",
       "           [-1.920e-03, -2.422e-03, -2.037e-03,  ..., 1.925e-03, 2.319e-03, -2.077e-03],\n",
       "           ...,\n",
       "           [3.963e-03, -3.956e-03, -3.265e-03,  ..., 3.614e-03, -4.383e-03, -3.141e-03],\n",
       "           [3.952e-03, 3.510e-03, 2.773e-03,  ..., -3.918e-03, -2.872e-03, -3.336e-03],\n",
       "           [-3.956e-03, 3.237e-03, -3.168e-03,  ..., -2.760e-03, -3.281e-03, 3.275e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.3.self_attn.v_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[9.027e-04, 9.651e-04, 9.046e-04,  ..., 1.050e-03, 9.055e-04, -1.085e-03],\n",
       "           [-1.368e-03, -8.984e-04, -1.323e-03,  ..., -8.616e-04, 8.168e-04, 8.807e-04],\n",
       "           [7.663e-04, 7.429e-04, -8.650e-04,  ..., -9.522e-04, -1.143e-03, -9.832e-04],\n",
       "           ...,\n",
       "           [-7.682e-04, -7.997e-04, 9.546e-04,  ..., -1.089e-03, 1.436e-03, -1.216e-03],\n",
       "           [5.708e-04, -1.050e-03, -9.141e-04,  ..., 1.428e-03, -1.221e-03, 9.842e-04],\n",
       "           [-9.804e-04, 9.208e-04, 1.103e-03,  ..., -1.254e-03, -1.283e-03, -1.343e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.3.self_attn.o_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-7.809e-03, 1.015e-02, -1.581e-02,  ..., -5.890e-03, 8.568e-03, -8.461e-03],\n",
       "           [-1.684e-03, -1.528e-03, -1.465e-03,  ..., 1.478e-03, -1.524e-03, -1.966e-03],\n",
       "           [2.024e-03, -1.307e-03, -2.729e-03,  ..., 1.752e-03, 1.546e-03, 1.263e-03],\n",
       "           ...,\n",
       "           [1.372e-03, -1.093e-03, -1.091e-03,  ..., 1.103e-03, -1.106e-03, 1.603e-03],\n",
       "           [-1.259e-03, -1.063e-03, 1.412e-03,  ..., -1.348e-03, -1.079e-03, 1.304e-03],\n",
       "           [1.406e-03, 1.154e-03, -1.383e-03,  ..., 1.504e-03, 1.271e-03, -1.549e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.3.mlp.gate_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[6.199e-03, -3.674e-03, 2.550e-03,  ..., 9.260e-04, -9.351e-04, 1.122e-03],\n",
       "           [-3.839e-03, 2.382e-03, 1.587e-03,  ..., 1.053e-03, 9.871e-04, 1.488e-03],\n",
       "           [-8.110e-03, 3.199e-03, -2.254e-03,  ..., 1.038e-03, 1.266e-03, 1.099e-03],\n",
       "           ...,\n",
       "           [1.587e-03, -1.486e-03, 1.578e-03,  ..., 1.383e-03, -1.604e-03, -1.676e-03],\n",
       "           [1.404e-03, -1.534e-03, 1.734e-03,  ..., -1.408e-03, 1.509e-03, 1.657e-03],\n",
       "           [1.634e-03, -1.538e-03, -2.111e-03,  ..., 1.512e-03, 2.245e-03, -1.538e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.3.mlp.up_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[5.543e-03, -2.729e-03, -1.585e-03,  ..., -7.715e-04, 9.260e-04, 8.235e-04],\n",
       "           [3.422e-03, 1.457e-03, 1.247e-03,  ..., 1.230e-03, -1.473e-03, 1.052e-03],\n",
       "           [-4.379e-03, 3.443e-03, 1.734e-03,  ..., -9.861e-04, -9.589e-04, 9.928e-04],\n",
       "           ...,\n",
       "           [1.242e-03, -1.350e-03, -1.326e-03,  ..., -1.592e-03, 1.365e-03, -1.541e-03],\n",
       "           [-1.273e-03, 1.577e-03, -1.331e-03,  ..., -1.442e-03, 1.999e-03, 1.574e-03],\n",
       "           [-1.143e-03, -1.943e-03, -2.487e-03,  ..., -1.414e-03, -1.641e-03, 1.569e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.3.mlp.down_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-2.856e-02, -1.453e-02, 1.978e-02,  ..., -3.534e-03, -3.128e-03, -3.529e-03],\n",
       "           [7.568e-03, -2.533e-03, -2.533e-03,  ..., -9.842e-04, 2.228e-03, -1.686e-03],\n",
       "           [7.294e-03, 2.539e-03, 2.501e-03,  ..., -9.766e-04, 1.151e-03, -9.909e-04],\n",
       "           ...,\n",
       "           [-1.429e-03, 1.311e-03, -1.093e-03,  ..., -1.449e-03, -1.401e-03, 1.740e-03],\n",
       "           [-1.078e-03, 1.513e-03, 1.032e-03,  ..., -1.563e-03, 1.672e-03, -1.375e-03],\n",
       "           [-1.390e-03, 1.495e-03, 1.655e-03,  ..., -1.493e-03, -1.483e-03, -1.522e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.4.self_attn.q_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-1.406e-03, 1.371e-03, -1.431e-03,  ..., 1.122e-03, 1.647e-03, 1.288e-03],\n",
       "           [1.202e-03, 9.918e-04, -1.293e-03,  ..., 1.397e-03, 1.300e-03, -1.406e-03],\n",
       "           [-1.625e-03, -1.611e-03, -1.334e-03,  ..., -1.681e-03, -1.307e-03, -1.922e-03],\n",
       "           ...,\n",
       "           [-4.353e-03, 1.764e-03, 1.936e-03,  ..., 1.625e-03, 1.545e-03, 1.448e-03],\n",
       "           [-1.892e-03, 1.986e-03, 2.125e-03,  ..., -1.733e-03, 2.058e-03, 2.306e-03],\n",
       "           [1.624e-03, -1.573e-03, 1.913e-03,  ..., -1.721e-03, -1.744e-03, 1.656e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.4.self_attn.k_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[1.955e-03, -1.644e-03, 2.060e-03,  ..., -1.348e-03, -1.549e-03, 1.762e-03],\n",
       "           [3.664e-03, 3.597e-03, 2.192e-03,  ..., -1.584e-03, 1.768e-03, -1.987e-03],\n",
       "           [3.986e-03, -2.029e-03, -2.275e-03,  ..., 1.495e-03, -1.746e-03, 1.672e-03],\n",
       "           ...,\n",
       "           [-4.532e-03, 2.678e-03, 1.945e-03,  ..., -1.376e-03, -1.216e-03, 1.496e-03],\n",
       "           [-4.288e-03, -4.005e-03, 4.360e-03,  ..., -2.186e-03, 2.321e-03, -2.165e-03],\n",
       "           [-3.742e-03, 5.527e-03, 4.154e-03,  ..., -3.281e-03, 2.977e-03, -2.539e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.4.self_attn.v_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-7.901e-04, -1.145e-03, 1.089e-03,  ..., -1.715e-03, 2.060e-03, 2.111e-03],\n",
       "           [1.009e-03, 1.182e-03, -1.686e-03,  ..., 1.685e-03, 1.870e-03, 1.383e-03],\n",
       "           [-1.427e-03, -1.476e-03, -1.360e-03,  ..., 1.577e-03, -1.761e-03, 1.651e-03],\n",
       "           ...,\n",
       "           [8.826e-04, -1.062e-03, 1.130e-03,  ..., 1.378e-03, -1.244e-03, -9.785e-04],\n",
       "           [-6.881e-04, -1.019e-03, 1.279e-03,  ..., -1.007e-03, -1.037e-03, 8.178e-04],\n",
       "           [7.000e-04, -1.215e-03, -1.365e-03,  ..., 1.229e-03, 1.418e-03, 1.033e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.4.self_attn.o_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[6.592e-03, 7.515e-03, -5.371e-03,  ..., -3.223e-03, -1.868e-02, 1.508e-02],\n",
       "           [1.444e-03, -1.225e-03, 2.106e-03,  ..., -1.694e-03, -1.136e-03, -2.508e-03],\n",
       "           [1.697e-03, 1.058e-03, 1.582e-03,  ..., 1.157e-03, -2.785e-03, 1.926e-03],\n",
       "           ...,\n",
       "           [-1.607e-03, -1.365e-03, -1.481e-03,  ..., 1.007e-03, 1.997e-03, 1.531e-03],\n",
       "           [-1.448e-03, -1.559e-03, 1.437e-03,  ..., 7.591e-04, 1.106e-03, 1.027e-03],\n",
       "           [1.555e-03, -1.686e-03, 1.592e-03,  ..., -9.446e-04, 1.328e-03, -1.018e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.4.mlp.gate_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-5.615e-03, 2.882e-03, 1.957e-03,  ..., 7.815e-04, 7.429e-04, 1.074e-03],\n",
       "           [-7.351e-03, 3.185e-03, 1.942e-03,  ..., 1.092e-03, 1.079e-03, -9.718e-04],\n",
       "           [2.098e-03, -2.840e-03, -1.781e-03,  ..., -2.069e-03, 2.617e-03, -2.993e-03],\n",
       "           ...,\n",
       "           [-1.241e-03, 1.482e-03, 1.679e-03,  ..., -1.204e-03, -1.431e-03, 1.340e-03],\n",
       "           [1.582e-03, 1.635e-03, -1.393e-03,  ..., 1.800e-03, -2.300e-03, 1.733e-03],\n",
       "           [1.502e-03, 1.648e-03, 1.455e-03,  ..., -1.596e-03, -2.050e-03, 1.342e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.4.mlp.up_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[6.893e-03, -3.651e-03, -2.010e-03,  ..., 1.261e-03, 8.917e-04, -9.551e-04],\n",
       "           [-8.591e-03, -2.579e-03, 1.540e-03,  ..., 1.184e-03, 1.081e-03, 9.918e-04],\n",
       "           [1.534e-03, -1.457e-03, -1.310e-03,  ..., 1.657e-03, 1.449e-03, 1.630e-03],\n",
       "           ...,\n",
       "           [-1.799e-03, -2.003e-03, -1.806e-03,  ..., -1.381e-03, 1.531e-03, 1.238e-03],\n",
       "           [1.382e-03, 1.310e-03, -1.358e-03,  ..., 1.480e-03, 1.680e-03, -1.599e-03],\n",
       "           [1.058e-03, -1.513e-03, 1.357e-03,  ..., -1.691e-03, 1.484e-03, 1.646e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.4.mlp.down_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-2.580e-02, -1.849e-02, -1.648e-02,  ..., 3.567e-03, -4.566e-03, -3.139e-03],\n",
       "           [6.409e-03, -3.736e-03, -2.110e-03,  ..., 1.679e-03, -1.035e-03, 2.180e-03],\n",
       "           [-5.035e-03, 2.651e-03, -1.965e-03,  ..., 7.915e-04, -6.633e-04, 7.977e-04],\n",
       "           ...,\n",
       "           [1.283e-03, 1.316e-03, 1.261e-03,  ..., -1.599e-03, -1.989e-03, 1.475e-03],\n",
       "           [-1.707e-03, 1.428e-03, -1.753e-03,  ..., 1.538e-03, 1.517e-03, -1.224e-03],\n",
       "           [-1.105e-03, 1.675e-03, -1.632e-03,  ..., -2.001e-03, 1.822e-03, 1.682e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.5.self_attn.q_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-4.940e-04, -9.303e-04, 6.704e-04,  ..., -8.783e-04, -1.033e-03, 1.065e-03],\n",
       "           [7.467e-04, -8.774e-04, 9.403e-04,  ..., 1.449e-03, -9.537e-04, -9.332e-04],\n",
       "           [6.528e-04, -7.939e-04, 7.439e-04,  ..., -1.083e-03, 1.188e-03, -8.502e-04],\n",
       "           ...,\n",
       "           [1.551e-03, -1.647e-03, -1.606e-03,  ..., -2.462e-03, 1.959e-03, 1.710e-03],\n",
       "           [-1.294e-03, -1.457e-03, 1.240e-03,  ..., -1.927e-03, -2.161e-03, -2.491e-03],\n",
       "           [1.204e-03, -1.430e-03, 1.303e-03,  ..., 1.885e-03, 2.457e-03, 2.811e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.5.self_attn.k_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-1.361e-03, 1.321e-03, 1.120e-03,  ..., -1.266e-03, -1.546e-03, 1.347e-03],\n",
       "           [-2.129e-03, -1.325e-03, -9.413e-04,  ..., 1.137e-03, -1.439e-03, 1.459e-03],\n",
       "           [-1.466e-03, 1.235e-03, -9.985e-04,  ..., 1.101e-03, -1.253e-03, -1.204e-03],\n",
       "           ...,\n",
       "           [2.796e-03, 1.407e-03, -1.593e-03,  ..., -2.287e-03, -2.214e-03, -2.234e-03],\n",
       "           [1.857e-03, -1.802e-03, -2.190e-03,  ..., -2.365e-03, -2.748e-03, -2.705e-03],\n",
       "           [1.162e-03, 1.876e-03, 1.868e-03,  ..., -2.232e-03, 2.214e-03, -2.310e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.5.self_attn.v_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-6.547e-04, 8.440e-04, -7.634e-04,  ..., 1.308e-03, 9.823e-04, -1.156e-03],\n",
       "           [6.032e-04, 1.113e-03, -8.936e-04,  ..., 1.320e-03, -1.252e-03, -1.067e-03],\n",
       "           [-7.358e-04, -8.702e-04, 9.861e-04,  ..., 1.126e-03, 1.406e-03, -9.637e-04],\n",
       "           ...,\n",
       "           [-9.708e-04, -8.307e-04, 8.636e-04,  ..., -9.184e-04, -1.151e-03, -1.158e-03],\n",
       "           [7.119e-04, -7.715e-04, 9.441e-04,  ..., -1.169e-03, -1.213e-03, -1.187e-03],\n",
       "           [8.650e-04, 9.027e-04, 7.944e-04,  ..., 1.054e-03, 1.122e-03, 1.254e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.5.self_attn.o_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[5.070e-03, -4.902e-03, 7.805e-03,  ..., -9.392e-03, 9.064e-03, -6.390e-03],\n",
       "           [-3.647e-03, 1.202e-03, 2.523e-03,  ..., 3.613e-03, 2.134e-03, 2.287e-03],\n",
       "           [7.811e-04, -1.002e-03, 1.226e-03,  ..., -2.024e-03, 1.312e-03, -1.238e-03],\n",
       "           ...,\n",
       "           [1.338e-03, 1.661e-03, 1.250e-03,  ..., -1.328e-03, 1.091e-03, 1.130e-03],\n",
       "           [1.016e-03, 1.122e-03, -1.229e-03,  ..., 1.152e-03, 1.240e-03, 1.387e-03],\n",
       "           [-1.202e-03, 1.349e-03, 1.131e-03,  ..., -1.225e-03, -1.232e-03, 1.245e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.5.mlp.gate_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-5.432e-03, 2.890e-03, 1.598e-03,  ..., -9.637e-04, -1.060e-03, -9.470e-04],\n",
       "           [-6.458e-03, -1.978e-03, -2.010e-03,  ..., 1.358e-03, 1.245e-03, 1.157e-03],\n",
       "           [-6.516e-03, 3.126e-03, 2.453e-03,  ..., -1.982e-03, -1.600e-03, 1.885e-03],\n",
       "           ...,\n",
       "           [1.704e-03, 1.678e-03, -1.370e-03,  ..., 2.638e-03, -1.624e-03, -1.774e-03],\n",
       "           [-2.071e-03, 1.408e-03, -1.777e-03,  ..., -1.649e-03, -1.173e-03, -1.346e-03],\n",
       "           [-1.490e-03, -1.859e-03, 1.615e-03,  ..., 2.060e-03, 1.577e-03, -1.646e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.5.mlp.up_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[3.296e-03, -1.616e-03, -1.066e-03,  ..., -7.463e-04, 6.914e-04, -8.893e-04],\n",
       "           [-2.565e-03, -1.427e-03, 1.423e-03,  ..., -1.127e-03, -8.607e-04, 1.115e-03],\n",
       "           [3.435e-03, -1.730e-03, -1.663e-03,  ..., -8.678e-04, 7.324e-04, -8.740e-04],\n",
       "           ...,\n",
       "           [1.129e-03, 1.168e-03, -1.473e-03,  ..., -1.900e-03, 1.700e-03, 1.608e-03],\n",
       "           [-2.062e-03, -1.423e-03, -1.450e-03,  ..., -1.621e-03, 1.913e-03, -1.549e-03],\n",
       "           [-1.609e-03, 1.174e-03, -1.411e-03,  ..., -1.843e-03, -1.429e-03, -1.610e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.5.mlp.down_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[2.214e-02, 2.235e-02, 1.404e-02,  ..., 3.304e-03, -2.567e-03, -4.883e-03],\n",
       "           [6.245e-03, -3.569e-03, 4.074e-03,  ..., 1.606e-03, 1.406e-03, -1.669e-03],\n",
       "           [3.883e-03, 3.143e-03, 3.725e-03,  ..., 7.973e-04, 6.938e-04, -1.379e-03],\n",
       "           ...,\n",
       "           [1.173e-03, 1.282e-03, -1.646e-03,  ..., 1.726e-03, 1.716e-03, 1.740e-03],\n",
       "           [-1.233e-03, 1.799e-03, -1.207e-03,  ..., -1.317e-03, 1.744e-03, 1.618e-03],\n",
       "           [1.350e-03, -1.453e-03, -1.235e-03,  ..., -1.477e-03, -1.432e-03, 1.425e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.6.self_attn.q_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-2.304e-03, 3.077e-03, -3.975e-03,  ..., 2.604e-03, 3.006e-03, -3.588e-03],\n",
       "           [1.997e-03, 2.419e-03, 1.843e-03,  ..., -2.884e-03, -2.150e-03, 1.708e-03],\n",
       "           [-2.737e-03, -4.082e-03, -2.485e-03,  ..., 2.691e-03, -2.451e-03, -2.720e-03],\n",
       "           ...,\n",
       "           [2.678e-03, 2.222e-03, -2.939e-03,  ..., 4.025e-03, 2.544e-03, -3.799e-03],\n",
       "           [4.875e-03, -2.584e-03, -2.506e-03,  ..., -2.489e-03, -3.065e-03, 3.674e-03],\n",
       "           [-3.166e-03, 2.558e-03, 2.680e-03,  ..., -3.199e-03, 3.283e-03, 2.516e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.6.self_attn.k_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[2.281e-03, 2.817e-03, -1.896e-03,  ..., -1.853e-03, -2.289e-03, 2.428e-03],\n",
       "           [-1.904e-03, 1.769e-03, 1.649e-03,  ..., 2.155e-03, -1.760e-03, 1.925e-03],\n",
       "           [2.214e-03, 1.705e-03, -2.403e-03,  ..., 2.262e-03, -1.946e-03, 1.906e-03],\n",
       "           ...,\n",
       "           [2.897e-03, -3.588e-03, 3.300e-03,  ..., -4.864e-03, 2.974e-03, 3.338e-03],\n",
       "           [4.604e-03, -2.565e-03, 3.036e-03,  ..., -4.230e-03, -2.968e-03, 3.525e-03],\n",
       "           [3.305e-03, 2.501e-03, 3.214e-03,  ..., 3.384e-03, -3.534e-03, -3.319e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.6.self_attn.v_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[7.167e-04, -1.122e-03, -9.141e-04,  ..., -1.421e-03, -1.255e-03, -1.271e-03],\n",
       "           [-7.267e-04, 9.212e-04, -1.049e-03,  ..., 1.184e-03, -1.054e-03, -1.087e-03],\n",
       "           [9.661e-04, 8.602e-04, -9.155e-04,  ..., -1.407e-03, 1.431e-03, -1.376e-03],\n",
       "           ...,\n",
       "           [-8.707e-04, 1.141e-03, -9.670e-04,  ..., -1.287e-03, -1.204e-03, 1.066e-03],\n",
       "           [-6.895e-04, -8.883e-04, 1.132e-03,  ..., 1.860e-03, 1.062e-03, -1.432e-03],\n",
       "           [-7.443e-04, -9.742e-04, 1.115e-03,  ..., -1.235e-03, 1.415e-03, -1.537e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.6.self_attn.o_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-3.290e-03, 3.952e-03, -4.116e-03,  ..., 2.764e-03, -3.397e-03, -6.100e-03],\n",
       "           [7.219e-04, 1.203e-03, 8.445e-04,  ..., -8.621e-04, -5.159e-04, 5.488e-04],\n",
       "           [7.377e-04, 1.188e-03, 8.569e-04,  ..., -1.087e-03, 5.884e-04, -1.586e-03],\n",
       "           ...,\n",
       "           [-1.681e-03, 1.469e-03, -1.040e-03,  ..., -1.433e-03, -1.335e-03, 1.443e-03],\n",
       "           [-1.170e-03, 1.792e-03, -1.026e-03,  ..., -1.482e-03, 1.370e-03, 1.310e-03],\n",
       "           [-1.142e-03, -1.296e-03, -1.008e-03,  ..., 1.181e-03, 9.851e-04, -1.711e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.6.mlp.gate_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[4.650e-03, 1.556e-03, 1.620e-03,  ..., 7.453e-04, -9.661e-04, 7.410e-04],\n",
       "           [-1.720e-03, 1.284e-03, 1.254e-03,  ..., 1.101e-03, 1.190e-03, 9.089e-04],\n",
       "           [5.817e-03, -3.897e-03, 4.120e-03,  ..., 2.020e-03, 2.020e-03, -2.195e-03],\n",
       "           ...,\n",
       "           [-1.317e-03, 1.292e-03, 1.569e-03,  ..., 2.045e-03, 1.721e-03, -1.861e-03],\n",
       "           [1.428e-03, 1.645e-03, -1.601e-03,  ..., 1.678e-03, -2.136e-03, 1.708e-03],\n",
       "           [1.801e-03, 1.787e-03, 1.711e-03,  ..., 2.188e-03, 1.700e-03, -1.764e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.6.mlp.up_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-5.009e-03, -2.846e-03, 2.310e-03,  ..., -1.163e-03, 1.116e-03, 1.216e-03],\n",
       "           [1.909e-03, -1.728e-03, -1.348e-03,  ..., -9.708e-04, -1.243e-03, 1.369e-03],\n",
       "           [-3.468e-03, -1.984e-03, -1.517e-03,  ..., 1.021e-03, -1.087e-03, -7.839e-04],\n",
       "           ...,\n",
       "           [-1.200e-03, 1.518e-03, -1.333e-03,  ..., -1.519e-03, 1.402e-03, -1.494e-03],\n",
       "           [1.366e-03, 1.426e-03, -1.571e-03,  ..., -1.483e-03, 1.411e-03, 1.725e-03],\n",
       "           [1.379e-03, 1.785e-03, -1.159e-03,  ..., -1.438e-03, -1.961e-03, -1.330e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.6.mlp.down_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-2.783e-02, 1.701e-02, 1.120e-02,  ..., 5.005e-03, 4.189e-03, 3.902e-03],\n",
       "           [6.466e-03, 3.614e-03, -4.299e-03,  ..., 1.425e-03, -2.888e-03, -1.849e-03],\n",
       "           [-4.955e-03, -4.196e-03, 2.211e-03,  ..., 8.106e-04, 1.053e-03, 1.120e-03],\n",
       "           ...,\n",
       "           [-1.882e-03, -1.923e-03, 1.654e-03,  ..., -1.924e-03, 1.772e-03, -1.641e-03],\n",
       "           [1.482e-03, -1.337e-03, -1.840e-03,  ..., -2.207e-03, -1.623e-03, -1.390e-03],\n",
       "           [-1.748e-03, -1.552e-03, -1.207e-03,  ..., 1.279e-03, 1.671e-03, -1.832e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.7.self_attn.q_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[1.388e-03, 1.373e-03, 1.310e-03,  ..., -1.968e-03, 1.228e-03, 1.472e-03],\n",
       "           [-1.439e-03, -1.562e-03, -1.959e-03,  ..., -1.966e-03, -1.893e-03, 1.660e-03],\n",
       "           [1.295e-03, -1.456e-03, 1.325e-03,  ..., -1.494e-03, 1.639e-03, -1.346e-03],\n",
       "           ...,\n",
       "           [2.529e-03, 1.968e-03, 1.857e-03,  ..., -2.352e-03, 2.903e-03, 2.022e-03],\n",
       "           [3.963e-03, 2.596e-03, -2.502e-03,  ..., -1.896e-03, -2.649e-03, 2.151e-03],\n",
       "           [-3.384e-03, -2.794e-03, 2.981e-03,  ..., 2.184e-03, -1.784e-03, 2.075e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.7.self_attn.k_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-2.178e-03, -2.636e-03, -2.138e-03,  ..., -1.877e-03, 1.541e-03, 1.913e-03],\n",
       "           [-2.283e-03, -1.961e-03, -1.930e-03,  ..., -1.702e-03, -1.589e-03, -1.598e-03],\n",
       "           [-1.657e-03, 1.955e-03, -1.728e-03,  ..., 1.628e-03, -1.493e-03, -1.639e-03],\n",
       "           ...,\n",
       "           [-3.857e-03, 8.537e-03, 7.240e-03,  ..., 3.027e-03, -3.088e-03, 2.684e-03],\n",
       "           [7.843e-03, 6.920e-03, 5.722e-03,  ..., 3.168e-03, -3.395e-03, -2.768e-03],\n",
       "           [6.977e-03, -7.179e-03, -5.531e-03,  ..., 2.956e-03, 2.687e-03, -3.704e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.7.self_attn.v_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-7.939e-04, 7.634e-04, -9.146e-04,  ..., -1.277e-03, -1.295e-03, 1.345e-03],\n",
       "           [-4.387e-04, -8.311e-04, -1.146e-03,  ..., 1.976e-03, -1.358e-03, 1.215e-03],\n",
       "           [5.050e-04, 1.062e-03, 1.045e-03,  ..., 1.484e-03, -1.242e-03, -1.375e-03],\n",
       "           ...,\n",
       "           [5.722e-04, 7.367e-04, 1.005e-03,  ..., 1.100e-03, -1.011e-03, -1.023e-03],\n",
       "           [-1.631e-03, -1.259e-03, 1.151e-03,  ..., 1.036e-03, -1.002e-03, -1.225e-03],\n",
       "           [-1.214e-03, -1.019e-03, 1.126e-03,  ..., 1.383e-03, -1.102e-03, 1.122e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.7.self_attn.o_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[6.508e-03, -4.230e-03, -3.355e-03,  ..., -6.744e-03, 5.272e-03, 5.356e-03],\n",
       "           [-7.310e-04, 1.195e-03, -8.283e-04,  ..., -1.305e-03, -1.080e-03, -2.491e-03],\n",
       "           [1.232e-03, -1.356e-03, -1.054e-03,  ..., -1.273e-03, -1.040e-03, 1.218e-03],\n",
       "           ...,\n",
       "           [1.444e-03, 1.460e-03, -1.690e-03,  ..., -1.215e-03, -1.301e-03, -1.796e-03],\n",
       "           [1.287e-03, 1.817e-03, 2.153e-03,  ..., -1.495e-03, 1.382e-03, -1.599e-03],\n",
       "           [-1.295e-03, -1.343e-03, 1.540e-03,  ..., -9.918e-04, 1.358e-03, -1.311e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.7.mlp.gate_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[5.013e-03, 2.274e-03, -1.622e-03,  ..., -8.087e-04, 8.612e-04, 1.243e-03],\n",
       "           [5.028e-03, 1.534e-03, -1.534e-03,  ..., -9.336e-04, -1.339e-03, -1.404e-03],\n",
       "           [1.368e-03, 1.488e-03, -1.304e-03,  ..., -1.121e-03, 1.421e-03, 1.380e-03],\n",
       "           ...,\n",
       "           [1.672e-03, 1.839e-03, -1.829e-03,  ..., 1.609e-03, 1.747e-03, 1.862e-03],\n",
       "           [-1.508e-03, 1.777e-03, -1.595e-03,  ..., -1.762e-03, -1.538e-03, -1.941e-03],\n",
       "           [1.730e-03, -1.640e-03, 1.819e-03,  ..., -1.854e-03, 1.788e-03, 1.836e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.7.mlp.up_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[6.039e-03, 3.653e-03, 2.682e-03,  ..., 1.244e-03, -1.319e-03, 1.700e-03],\n",
       "           [3.750e-03, 2.638e-03, -1.801e-03,  ..., 1.497e-03, 1.416e-03, -1.952e-03],\n",
       "           [1.689e-03, 1.744e-03, 1.249e-03,  ..., -1.558e-03, -1.251e-03, 1.423e-03],\n",
       "           ...,\n",
       "           [-1.746e-03, 1.802e-03, 1.440e-03,  ..., -1.434e-03, -1.452e-03, -1.417e-03],\n",
       "           [-1.237e-03, 1.578e-03, 1.411e-03,  ..., -1.821e-03, 1.423e-03, 1.974e-03],\n",
       "           [-1.307e-03, -1.390e-03, -1.218e-03,  ..., 1.386e-03, 1.600e-03, 1.118e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.7.mlp.down_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[2.863e-02, -1.884e-02, 1.588e-02,  ..., -3.767e-03, 7.336e-03, -4.051e-03],\n",
       "           [-3.881e-03, 3.040e-03, -3.067e-03,  ..., -1.328e-03, 8.683e-04, 7.720e-04],\n",
       "           [8.858e-03, -3.103e-03, 2.541e-03,  ..., 7.129e-04, 1.328e-03, -8.740e-04],\n",
       "           ...,\n",
       "           [-1.756e-03, 1.527e-03, -1.711e-03,  ..., 1.578e-03, -1.496e-03, -1.601e-03],\n",
       "           [-1.512e-03, 1.922e-03, -1.577e-03,  ..., -1.411e-03, 1.451e-03, -1.617e-03],\n",
       "           [1.974e-03, -1.518e-03, 1.183e-03,  ..., 1.633e-03, -1.437e-03, -1.533e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.8.self_attn.q_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-1.281e-03, -1.464e-03, 1.258e-03,  ..., -1.628e-03, 1.236e-03, -1.266e-03],\n",
       "           [-1.239e-03, 1.269e-03, -1.283e-03,  ..., -1.245e-03, -1.257e-03, -1.684e-03],\n",
       "           [-1.424e-03, 1.410e-03, -1.458e-03,  ..., 1.491e-03, 1.505e-03, -1.605e-03],\n",
       "           ...,\n",
       "           [-1.323e-03, 1.638e-03, -1.808e-03,  ..., -2.531e-03, 2.188e-03, -2.157e-03],\n",
       "           [1.358e-03, 1.496e-03, 1.734e-03,  ..., 2.474e-03, 2.232e-03, -2.377e-03],\n",
       "           [-1.580e-03, -1.883e-03, -1.687e-03,  ..., -2.480e-03, -2.235e-03, -1.831e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.8.self_attn.k_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-2.628e-03, -1.984e-03, -1.771e-03,  ..., -1.651e-03, 1.797e-03, -1.553e-03],\n",
       "           [-1.904e-03, 1.391e-03, -2.260e-03,  ..., 1.884e-03, 1.274e-03, -1.549e-03],\n",
       "           [2.371e-03, 1.889e-03, -2.081e-03,  ..., 1.728e-03, -1.572e-03, -1.682e-03],\n",
       "           ...,\n",
       "           [-2.037e-03, 2.026e-03, -2.026e-03,  ..., -3.042e-03, -2.726e-03, 2.304e-03],\n",
       "           [-2.157e-03, 2.480e-03, 2.043e-03,  ..., 2.403e-03, 2.771e-03, -2.144e-03],\n",
       "           [3.096e-03, -2.565e-03, -2.098e-03,  ..., -1.650e-03, -1.730e-03, 2.192e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.8.self_attn.v_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[6.189e-04, -1.019e-03, 1.078e-03,  ..., -1.122e-03, -1.412e-03, -1.748e-03],\n",
       "           [-5.174e-04, 7.701e-04, -1.034e-03,  ..., 1.125e-03, -1.439e-03, -1.798e-03],\n",
       "           [5.817e-04, -7.095e-04, 1.073e-03,  ..., 1.185e-03, 1.288e-03, -1.307e-03],\n",
       "           ...,\n",
       "           [6.876e-04, -7.358e-04, 8.416e-04,  ..., -9.151e-04, -1.208e-03, 1.486e-03],\n",
       "           [-6.561e-04, 7.205e-04, -8.130e-04,  ..., -1.348e-03, 1.116e-03, -9.489e-04],\n",
       "           [5.708e-04, -7.763e-04, -8.602e-04,  ..., 1.309e-03, 1.190e-03, 9.642e-04]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.8.self_attn.o_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[7.736e-03, 3.216e-03, -5.859e-03,  ..., -1.950e-02, 9.529e-03, -5.123e-03],\n",
       "           [6.852e-04, -8.106e-04, -9.322e-04,  ..., 1.904e-03, -2.485e-03, 1.335e-03],\n",
       "           [1.750e-03, 1.063e-03, -1.092e-03,  ..., 2.066e-03, -2.451e-03, -1.978e-03],\n",
       "           ...,\n",
       "           [-1.472e-03, 1.668e-03, 1.803e-03,  ..., -1.364e-03, -9.937e-04, 1.313e-03],\n",
       "           [1.381e-03, -1.502e-03, -1.306e-03,  ..., 1.123e-03, 1.530e-03, -1.028e-03],\n",
       "           [1.348e-03, -1.611e-03, 1.296e-03,  ..., 1.124e-03, -1.029e-03, -1.410e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.8.mlp.gate_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[5.554e-03, 2.495e-03, 1.738e-03,  ..., -1.020e-03, 7.467e-04, -1.195e-03],\n",
       "           [-4.818e-03, 2.592e-03, 1.477e-03,  ..., 1.026e-03, -9.422e-04, -1.109e-03],\n",
       "           [2.125e-03, 1.624e-03, 1.302e-03,  ..., -1.118e-03, 8.135e-04, -8.764e-04],\n",
       "           ...,\n",
       "           [1.778e-03, -1.595e-03, -1.598e-03,  ..., -1.311e-03, 1.517e-03, 1.410e-03],\n",
       "           [-1.778e-03, 1.320e-03, -1.532e-03,  ..., -1.476e-03, 1.689e-03, -1.740e-03],\n",
       "           [1.250e-03, 1.588e-03, -1.846e-03,  ..., 1.807e-03, 1.995e-03, -2.289e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.8.mlp.up_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[5.634e-03, -3.712e-03, 2.451e-03,  ..., -1.345e-03, -1.216e-03, 1.303e-03],\n",
       "           [-4.845e-03, -3.054e-03, 1.950e-03,  ..., 1.171e-03, -1.211e-03, 9.751e-04],\n",
       "           [-2.195e-03, 1.654e-03, 1.180e-03,  ..., -1.316e-03, 1.397e-03, -1.266e-03],\n",
       "           ...,\n",
       "           [1.456e-03, 1.715e-03, -1.375e-03,  ..., -1.510e-03, -1.763e-03, -1.281e-03],\n",
       "           [-1.004e-03, 1.299e-03, 1.364e-03,  ..., 1.613e-03, -1.517e-03, -1.504e-03],\n",
       "           [1.024e-03, 1.483e-03, 1.040e-03,  ..., -1.664e-03, 1.448e-03, 1.807e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.8.mlp.down_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-3.168e-02, 1.151e-02, -1.608e-02,  ..., -6.954e-03, -4.681e-03, -4.730e-03],\n",
       "           [7.603e-03, -3.078e-03, -4.799e-03,  ..., -1.549e-03, -2.701e-03, -1.425e-03],\n",
       "           [-1.470e-02, 4.330e-03, -4.631e-03,  ..., 1.017e-03, 9.084e-04, -1.136e-03],\n",
       "           ...,\n",
       "           [-1.422e-03, 1.919e-03, 1.729e-03,  ..., -1.429e-03, -1.480e-03, -1.927e-03],\n",
       "           [-1.751e-03, 1.755e-03, -1.731e-03,  ..., -1.564e-03, -1.447e-03, 1.959e-03],\n",
       "           [1.554e-03, 2.079e-03, 1.708e-03,  ..., 1.701e-03, 1.920e-03, 1.431e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.9.self_attn.q_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[9.322e-04, -1.135e-03, -1.156e-03,  ..., 1.561e-03, -1.694e-03, 1.252e-03],\n",
       "           [-1.061e-03, -1.503e-03, 9.756e-04,  ..., 1.364e-03, 1.044e-03, -1.405e-03],\n",
       "           [1.021e-03, -1.454e-03, -1.052e-03,  ..., -1.064e-03, -9.155e-04, -1.335e-03],\n",
       "           ...,\n",
       "           [-2.382e-03, -2.058e-03, 2.096e-03,  ..., 2.821e-03, 2.647e-03, -2.089e-03],\n",
       "           [-1.488e-03, -1.504e-03, 1.943e-03,  ..., 2.577e-03, 2.420e-03, -2.361e-03],\n",
       "           [-2.844e-03, -2.113e-03, -2.060e-03,  ..., 3.439e-03, -2.457e-03, 2.014e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.9.self_attn.k_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-2.733e-03, -2.798e-03, 2.439e-03,  ..., -1.639e-03, -1.825e-03, 1.661e-03],\n",
       "           [-1.443e-03, 1.609e-03, -2.115e-03,  ..., -1.869e-03, 1.873e-03, -1.630e-03],\n",
       "           [2.020e-03, 2.495e-03, -1.821e-03,  ..., 1.999e-03, -2.234e-03, 1.968e-03],\n",
       "           ...,\n",
       "           [4.044e-03, -3.365e-03, -3.870e-03,  ..., 3.914e-03, -3.235e-03, 3.191e-03],\n",
       "           [-3.613e-03, -3.813e-03, 2.903e-03,  ..., 3.340e-03, 3.222e-03, 3.765e-03],\n",
       "           [7.935e-03, 3.914e-03, -3.082e-03,  ..., -2.439e-03, -3.290e-03, 2.777e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.9.self_attn.v_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-1.264e-03, 8.659e-04, -1.043e-03,  ..., -1.144e-03, -1.086e-03, -1.456e-03],\n",
       "           [7.105e-04, -1.100e-03, -1.465e-03,  ..., 1.919e-03, -2.071e-03, 1.784e-03],\n",
       "           [8.397e-04, -1.208e-03, 1.178e-03,  ..., 1.882e-03, 1.609e-03, -1.779e-03],\n",
       "           ...,\n",
       "           [8.154e-04, -1.141e-03, 1.260e-03,  ..., -1.705e-03, 1.784e-03, -1.385e-03],\n",
       "           [8.302e-04, 1.441e-03, -1.262e-03,  ..., -1.645e-03, -1.266e-03, 1.514e-03],\n",
       "           [-7.563e-04, -1.348e-03, -1.279e-03,  ..., -1.772e-03, -1.493e-03, 1.374e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.9.self_attn.o_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[4.730e-03, 2.634e-03, 3.929e-03,  ..., 5.871e-03, 4.025e-03, 3.843e-03],\n",
       "           [-1.005e-03, -9.398e-04, -9.832e-04,  ..., -7.672e-04, -6.781e-04, 6.652e-04],\n",
       "           [-1.461e-03, -8.206e-04, 7.648e-04,  ..., 8.345e-04, -9.918e-04, -7.973e-04],\n",
       "           ...,\n",
       "           [-1.771e-03, -1.340e-03, 1.608e-03,  ..., -1.786e-03, -2.073e-03, -1.622e-03],\n",
       "           [1.613e-03, -1.052e-03, -1.987e-03,  ..., 1.646e-03, 1.843e-03, 1.657e-03],\n",
       "           [2.016e-03, 1.316e-03, -1.463e-03,  ..., 1.199e-03, 1.672e-03, -1.674e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.9.mlp.gate_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-4.639e-03, -2.075e-03, 1.453e-03,  ..., -9.179e-04, 9.527e-04, 8.488e-04],\n",
       "           [-3.738e-03, -2.010e-03, 1.227e-03,  ..., -8.650e-04, 9.794e-04, -9.613e-04],\n",
       "           [-6.195e-03, -2.987e-03, 2.741e-03,  ..., -2.760e-03, -2.399e-03, -2.956e-03],\n",
       "           ...,\n",
       "           [1.680e-03, 2.356e-03, 1.714e-03,  ..., 1.878e-03, -2.426e-03, -2.375e-03],\n",
       "           [-1.797e-03, -1.382e-03, -1.554e-03,  ..., -1.582e-03, -1.212e-03, -1.399e-03],\n",
       "           [-1.156e-03, -1.469e-03, -1.339e-03,  ..., 1.519e-03, 1.415e-03, 1.404e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.9.mlp.up_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[5.043e-03, -3.771e-03, 2.800e-03,  ..., -2.213e-03, 1.475e-03, -2.264e-03],\n",
       "           [1.102e-02, -1.338e-03, -1.101e-03,  ..., 1.004e-03, 1.012e-03, -8.879e-04],\n",
       "           [2.684e-03, 1.858e-03, -1.514e-03,  ..., 1.216e-03, 1.465e-03, 1.230e-03],\n",
       "           ...,\n",
       "           [1.523e-03, -1.200e-03, -1.747e-03,  ..., -1.631e-03, -1.126e-03, -1.385e-03],\n",
       "           [-1.084e-03, 1.571e-03, 1.487e-03,  ..., -1.470e-03, 1.566e-03, -1.485e-03],\n",
       "           [1.304e-03, 2.174e-03, -1.421e-03,  ..., 1.660e-03, 1.863e-03, 2.123e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.9.mlp.down_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[3.506e-02, 1.548e-02, 1.511e-02,  ..., -4.524e-03, -6.470e-03, -8.881e-03],\n",
       "           [-4.726e-03, 2.592e-03, -2.028e-03,  ..., 1.391e-03, -8.664e-04, -1.422e-03],\n",
       "           [5.890e-03, -3.075e-03, -3.273e-03,  ..., -9.637e-04, 9.346e-04, -8.559e-04],\n",
       "           ...,\n",
       "           [1.153e-03, 1.970e-03, 1.637e-03,  ..., -1.616e-03, 1.641e-03, 1.551e-03],\n",
       "           [2.069e-03, 1.530e-03, 1.391e-03,  ..., 1.265e-03, 1.578e-03, 1.794e-03],\n",
       "           [1.309e-03, -2.195e-03, -1.756e-03,  ..., -1.558e-03, -1.171e-03, 1.720e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.10.self_attn.q_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-1.031e-03, -1.109e-03, -1.029e-03,  ..., -1.287e-03, -1.349e-03, -1.316e-03],\n",
       "           [-1.473e-03, -1.685e-03, -1.529e-03,  ..., 1.601e-03, -1.892e-03, 1.872e-03],\n",
       "           [1.396e-03, 1.349e-03, -1.410e-03,  ..., -1.539e-03, 1.223e-03, -1.487e-03],\n",
       "           ...,\n",
       "           [1.667e-03, 1.910e-03, 1.891e-03,  ..., 2.707e-03, 2.466e-03, 3.172e-03],\n",
       "           [1.380e-03, -1.622e-03, 1.550e-03,  ..., 1.844e-03, -1.860e-03, -2.132e-03],\n",
       "           [-1.440e-03, -2.310e-03, -2.672e-03,  ..., -2.478e-03, -2.853e-03, 2.623e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.10.self_attn.k_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[2.497e-03, -1.656e-03, -1.645e-03,  ..., 1.532e-03, 1.120e-03, 2.247e-03],\n",
       "           [-2.020e-03, 1.236e-03, 1.498e-03,  ..., 1.225e-03, 1.346e-03, 1.208e-03],\n",
       "           [2.470e-03, 1.923e-03, 1.478e-03,  ..., 1.223e-03, 1.128e-03, -1.153e-03],\n",
       "           ...,\n",
       "           [2.771e-03, -2.838e-03, 2.794e-03,  ..., 2.796e-03, -3.347e-03, -2.817e-03],\n",
       "           [-3.838e-03, -3.937e-03, -2.853e-03,  ..., -3.302e-03, -3.399e-03, 3.351e-03],\n",
       "           [-1.997e-03, -3.044e-03, -3.321e-03,  ..., -3.492e-03, 2.981e-03, -3.231e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.10.self_attn.v_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-1.531e-03, 1.732e-03, -1.450e-03,  ..., -1.057e-03, 1.022e-03, -1.366e-03],\n",
       "           [-2.357e-03, 1.358e-03, 1.117e-03,  ..., -7.610e-04, -7.806e-04, -1.000e-03],\n",
       "           [9.832e-04, -8.440e-04, -1.013e-03,  ..., -1.122e-03, -1.410e-03, 9.880e-04],\n",
       "           ...,\n",
       "           [-5.903e-04, -1.286e-03, 1.157e-03,  ..., -1.730e-03, -1.624e-03, 1.502e-03],\n",
       "           [-1.596e-03, -1.462e-03, 9.398e-04,  ..., 9.966e-04, -1.252e-03, 1.521e-03],\n",
       "           [7.153e-04, 1.075e-03, -1.591e-03,  ..., 1.522e-03, 1.359e-03, -1.361e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.10.self_attn.o_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-2.364e-02, -5.997e-03, -1.854e-02,  ..., 1.337e-02, 6.256e-03, -7.511e-03],\n",
       "           [-1.218e-03, 1.114e-03, -1.466e-03,  ..., 1.202e-03, 9.775e-04, 2.674e-03],\n",
       "           [3.231e-03, 1.004e-03, -2.169e-03,  ..., 1.736e-03, 1.007e-03, 2.495e-03],\n",
       "           ...,\n",
       "           [-1.323e-03, 1.676e-03, -1.450e-03,  ..., -1.591e-03, -1.378e-03, 1.564e-03],\n",
       "           [-1.311e-03, 1.679e-03, 1.606e-03,  ..., -1.554e-03, 1.690e-03, 1.691e-03],\n",
       "           [-1.219e-03, -1.637e-03, 1.600e-03,  ..., 1.464e-03, -2.127e-03, -1.513e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.10.mlp.gate_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-5.047e-03, -2.131e-03, -1.381e-03,  ..., 7.534e-04, -1.053e-03, 7.777e-04],\n",
       "           [5.623e-03, -9.470e-04, 8.116e-04,  ..., 6.456e-04, 5.770e-04, 5.870e-04],\n",
       "           [-4.623e-03, -1.999e-03, 1.367e-03,  ..., 1.132e-03, -1.420e-03, 1.042e-03],\n",
       "           ...,\n",
       "           [1.334e-03, 1.585e-03, 1.493e-03,  ..., -1.485e-03, -1.297e-03, 1.254e-03],\n",
       "           [1.431e-03, 1.492e-03, -1.425e-03,  ..., 1.431e-03, -1.492e-03, -1.943e-03],\n",
       "           [1.454e-03, -1.501e-03, 1.524e-03,  ..., 1.476e-03, 1.597e-03, -1.653e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.10.mlp.up_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[5.581e-03, -3.407e-03, 2.005e-03,  ..., 1.852e-03, 2.428e-03, -1.801e-03],\n",
       "           [-1.065e-02, -3.769e-03, -3.281e-03,  ..., -2.483e-03, 2.579e-03, 2.991e-03],\n",
       "           [-5.150e-03, 2.369e-03, 1.816e-03,  ..., 1.477e-03, 1.530e-03, -1.404e-03],\n",
       "           ...,\n",
       "           [1.038e-03, 1.421e-03, -1.531e-03,  ..., 1.453e-03, -2.293e-03, 1.410e-03],\n",
       "           [1.422e-03, 1.354e-03, 1.477e-03,  ..., 1.338e-03, 1.419e-03, 1.887e-03],\n",
       "           [-1.515e-03, -1.816e-03, -1.331e-03,  ..., 1.239e-03, 1.451e-03, -1.395e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.10.mlp.down_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-3.851e-02, 2.840e-02, 2.165e-02,  ..., 3.817e-03, -7.797e-03, 1.217e-02],\n",
       "           [-3.565e-03, -1.726e-03, 3.435e-03,  ..., 1.575e-03, -1.330e-03, -1.695e-03],\n",
       "           [7.462e-03, 3.651e-03, -2.497e-03,  ..., 8.035e-04, -8.469e-04, -1.610e-03],\n",
       "           ...,\n",
       "           [1.099e-03, 1.932e-03, -2.014e-03,  ..., 2.077e-03, 1.512e-03, 1.500e-03],\n",
       "           [1.731e-03, 2.054e-03, -1.472e-03,  ..., -1.713e-03, 1.345e-03, 1.747e-03],\n",
       "           [2.270e-03, -1.617e-03, -1.850e-03,  ..., -1.526e-03, -2.321e-03, 1.143e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.11.self_attn.q_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[1.175e-03, 1.150e-03, -9.947e-04,  ..., 1.059e-03, -1.020e-03, -9.556e-04],\n",
       "           [1.531e-03, 1.948e-03, 1.514e-03,  ..., 1.157e-03, 1.479e-03, 1.291e-03],\n",
       "           [-2.748e-03, -2.048e-03, -2.256e-03,  ..., -1.387e-03, 1.654e-03, -1.147e-03],\n",
       "           ...,\n",
       "           [1.930e-03, -2.136e-03, -1.859e-03,  ..., -2.344e-03, 2.249e-03, 2.398e-03],\n",
       "           [1.843e-03, -1.587e-03, -1.826e-03,  ..., 2.068e-03, -2.632e-03, 2.981e-03],\n",
       "           [2.096e-03, -2.167e-03, 1.712e-03,  ..., 2.020e-03, -2.409e-03, 3.073e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.11.self_attn.k_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[2.659e-03, -2.203e-03, -2.132e-03,  ..., -2.041e-03, -1.531e-03, 1.596e-03],\n",
       "           [3.107e-03, -2.563e-03, 2.625e-03,  ..., 1.830e-03, 2.621e-03, -2.039e-03],\n",
       "           [1.965e-03, -1.720e-03, -1.739e-03,  ..., -1.241e-03, 1.439e-03, 1.199e-03],\n",
       "           ...,\n",
       "           [-4.112e-03, 3.286e-03, 2.897e-03,  ..., -2.548e-03, 3.536e-03, -3.101e-03],\n",
       "           [3.527e-03, 4.147e-03, -2.510e-03,  ..., 3.651e-03, -2.808e-03, -3.065e-03],\n",
       "           [3.798e-03, -3.901e-03, 3.323e-03,  ..., 3.477e-03, 3.054e-03, -2.789e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.11.self_attn.v_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[9.966e-04, -1.193e-03, -1.375e-03,  ..., -1.154e-03, 1.466e-03, -1.247e-03],\n",
       "           [8.955e-04, -1.281e-03, -1.385e-03,  ..., 1.205e-03, -1.197e-03, 1.412e-03],\n",
       "           [-8.993e-04, -1.528e-03, -1.739e-03,  ..., -1.060e-03, 1.108e-03, 1.317e-03],\n",
       "           ...,\n",
       "           [6.676e-04, -1.242e-03, 1.382e-03,  ..., 1.379e-03, 1.577e-03, -1.684e-03],\n",
       "           [8.521e-04, -1.184e-03, -1.181e-03,  ..., 1.621e-03, 1.432e-03, 1.804e-03],\n",
       "           [-7.162e-04, -1.148e-03, -1.299e-03,  ..., 1.539e-03, -1.945e-03, -1.957e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.11.self_attn.o_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[9.872e-03, -4.742e-03, 8.942e-03,  ..., 6.779e-03, 8.232e-03, 4.208e-03],\n",
       "           [-1.670e-03, 1.289e-03, -1.220e-03,  ..., -1.063e-03, -9.160e-04, -4.945e-04],\n",
       "           [-1.181e-03, -1.046e-03, 2.028e-03,  ..., 8.965e-04, 1.364e-03, 9.928e-04],\n",
       "           ...,\n",
       "           [1.784e-03, -1.138e-03, -1.523e-03,  ..., -1.370e-03, 1.398e-03, -1.801e-03],\n",
       "           [-1.368e-03, 1.287e-03, -1.933e-03,  ..., 1.350e-03, -1.898e-03, -1.539e-03],\n",
       "           [-1.243e-03, -1.461e-03, 2.157e-03,  ..., 1.383e-03, -1.450e-03, 1.573e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.11.mlp.gate_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-4.105e-03, -2.033e-03, 1.330e-03,  ..., -9.441e-04, -9.079e-04, 1.079e-03],\n",
       "           [3.433e-03, 1.692e-03, 1.289e-03,  ..., 8.163e-04, 8.888e-04, -9.384e-04],\n",
       "           [3.565e-03, 2.028e-03, 1.076e-03,  ..., 1.257e-03, -1.062e-03, -1.201e-03],\n",
       "           ...,\n",
       "           [-1.255e-03, -1.359e-03, 1.375e-03,  ..., 1.662e-03, 1.566e-03, 1.639e-03],\n",
       "           [-1.098e-03, 1.935e-03, -1.835e-03,  ..., -2.079e-03, -2.007e-03, -1.600e-03],\n",
       "           [-1.058e-03, 1.145e-03, 1.246e-03,  ..., 1.319e-03, -1.146e-03, 1.401e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.11.mlp.up_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[9.346e-03, 1.605e-03, 1.377e-03,  ..., 1.059e-03, 1.218e-03, 1.193e-03],\n",
       "           [4.852e-03, -2.642e-03, -2.096e-03,  ..., 2.474e-03, 2.062e-03, 2.094e-03],\n",
       "           [-3.279e-03, -1.289e-03, 9.003e-04,  ..., 1.216e-03, -9.270e-04, 9.422e-04],\n",
       "           ...,\n",
       "           [1.293e-03, 1.205e-03, -1.658e-03,  ..., 1.654e-03, 1.537e-03, -1.584e-03],\n",
       "           [1.512e-03, 1.516e-03, 1.743e-03,  ..., -1.475e-03, -1.997e-03, -1.654e-03],\n",
       "           [-1.369e-03, -1.316e-03, 1.197e-03,  ..., 1.316e-03, 1.798e-03, 1.571e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.11.mlp.down_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-3.427e-02, 1.831e-02, -2.260e-02,  ..., -4.414e-03, 8.308e-03, -5.409e-03],\n",
       "           [2.579e-03, 1.815e-03, 3.410e-03,  ..., 1.578e-03, 1.303e-03, 1.043e-03],\n",
       "           [-3.460e-03, -4.906e-03, 3.115e-03,  ..., 7.730e-04, -1.412e-03, 8.512e-04],\n",
       "           ...,\n",
       "           [1.094e-03, -1.366e-03, -1.675e-03,  ..., -1.385e-03, 1.896e-03, -1.774e-03],\n",
       "           [1.340e-03, 1.440e-03, 2.918e-03,  ..., -1.525e-03, -1.431e-03, 1.374e-03],\n",
       "           [-1.461e-03, 2.071e-03, -1.815e-03,  ..., -1.318e-03, -1.458e-03, -1.668e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.12.self_attn.q_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-1.356e-03, -1.140e-03, -1.048e-03,  ..., -1.236e-03, 1.645e-03, -1.395e-03],\n",
       "           [2.081e-03, -1.097e-03, -1.731e-03,  ..., -1.940e-03, -1.751e-03, 1.473e-03],\n",
       "           [-1.600e-03, -1.289e-03, -1.242e-03,  ..., 1.602e-03, 1.258e-03, 1.735e-03],\n",
       "           ...,\n",
       "           [-1.816e-03, -1.106e-03, -1.359e-03,  ..., 1.670e-03, 2.010e-03, 1.923e-03],\n",
       "           [-2.211e-03, -2.131e-03, 2.249e-03,  ..., 1.868e-03, 1.950e-03, 1.372e-03],\n",
       "           [2.333e-03, -1.556e-03, 1.610e-03,  ..., -1.834e-03, 1.456e-03, -1.626e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.12.self_attn.k_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-2.499e-03, 1.909e-03, 1.293e-03,  ..., 1.474e-03, -1.245e-03, -1.657e-03],\n",
       "           [1.176e-03, 1.226e-03, -1.189e-03,  ..., 1.189e-03, -1.250e-03, 1.662e-03],\n",
       "           [-1.903e-03, 1.906e-03, 1.966e-03,  ..., -1.499e-03, -1.491e-03, 1.254e-03],\n",
       "           ...,\n",
       "           [-1.008e-02, 4.189e-03, 3.323e-03,  ..., -2.861e-03, -2.996e-03, -2.895e-03],\n",
       "           [3.870e-03, -3.633e-03, -3.633e-03,  ..., 4.818e-03, -3.208e-03, 4.719e-03],\n",
       "           [-8.446e-03, 4.219e-03, 2.924e-03,  ..., -3.288e-03, -3.242e-03, -2.954e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.12.self_attn.v_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[4.842e-04, -6.599e-04, 1.531e-03,  ..., 1.058e-03, 1.266e-03, -1.131e-03],\n",
       "           [5.989e-04, -8.917e-04, -8.802e-04,  ..., -1.464e-03, -1.318e-03, -1.534e-03],\n",
       "           [-6.113e-04, -7.281e-04, -8.144e-04,  ..., -1.401e-03, 1.580e-03, 1.547e-03],\n",
       "           ...,\n",
       "           [-6.204e-04, -1.048e-03, 1.128e-03,  ..., -1.384e-03, -1.972e-03, -1.736e-03],\n",
       "           [9.823e-04, 1.238e-03, -1.201e-03,  ..., 1.423e-03, -1.589e-03, -1.361e-03],\n",
       "           [7.672e-04, -1.102e-03, 1.204e-03,  ..., 1.619e-03, -1.509e-03, -1.677e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.12.self_attn.o_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[3.944e-03, 3.714e-03, -4.620e-03,  ..., 1.018e-02, 2.810e-03, -5.119e-03],\n",
       "           [8.459e-04, -8.402e-04, 9.027e-04,  ..., -9.031e-04, -6.475e-04, -9.975e-04],\n",
       "           [9.241e-04, 6.986e-04, 7.753e-04,  ..., 1.414e-03, -7.386e-04, 1.036e-03],\n",
       "           ...,\n",
       "           [-1.435e-03, -1.696e-03, 1.235e-03,  ..., -1.099e-03, -1.211e-03, 1.650e-03],\n",
       "           [-1.309e-03, 1.360e-03, -1.164e-03,  ..., -1.263e-03, -1.427e-03, 1.588e-03],\n",
       "           [1.670e-03, -1.580e-03, -1.367e-03,  ..., 1.094e-03, 1.309e-03, -1.432e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.12.mlp.gate_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[4.921e-03, -2.663e-03, 1.850e-03,  ..., -1.388e-03, -1.642e-03, 1.388e-03],\n",
       "           [-8.202e-03, 5.184e-03, -4.185e-03,  ..., -2.274e-03, -2.731e-03, -2.678e-03],\n",
       "           [6.836e-03, -3.874e-03, -2.983e-03,  ..., 1.740e-03, 1.383e-03, -1.735e-03],\n",
       "           ...,\n",
       "           [1.401e-03, -1.372e-03, 1.710e-03,  ..., -1.375e-03, -1.306e-03, 1.463e-03],\n",
       "           [-1.644e-03, -1.478e-03, -1.843e-03,  ..., 1.564e-03, 1.892e-03, -2.090e-03],\n",
       "           [-1.127e-03, -1.849e-03, -1.666e-03,  ..., -1.474e-03, -1.598e-03, -1.623e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.12.mlp.up_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[7.843e-03, -1.423e-03, -1.275e-03,  ..., -1.006e-03, -1.187e-03, 1.152e-03],\n",
       "           [-3.489e-03, 2.342e-03, 1.691e-03,  ..., 1.375e-03, 1.357e-03, -9.584e-04],\n",
       "           [3.664e-03, -2.281e-03, -1.658e-03,  ..., 7.882e-04, 7.529e-04, 6.609e-04],\n",
       "           ...,\n",
       "           [-1.517e-03, -1.492e-03, -1.507e-03,  ..., -2.104e-03, -1.923e-03, -1.585e-03],\n",
       "           [-1.034e-03, -1.336e-03, -1.390e-03,  ..., 2.146e-03, -1.561e-03, 1.655e-03],\n",
       "           [-1.288e-03, 1.362e-03, 1.709e-03,  ..., -1.602e-03, 1.726e-03, 1.479e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.12.mlp.down_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-3.690e-02, 1.426e-02, -1.633e-02,  ..., 6.962e-03, 5.741e-03, 7.614e-03],\n",
       "           [2.310e-03, 2.033e-03, -2.228e-03,  ..., 1.189e-03, -1.099e-03, 1.151e-03],\n",
       "           [-3.944e-03, 3.519e-03, -2.171e-03,  ..., -7.772e-04, 7.243e-04, 1.241e-03],\n",
       "           ...,\n",
       "           [-1.581e-03, -1.710e-03, 1.978e-03,  ..., 1.779e-03, -1.694e-03, 2.066e-03],\n",
       "           [-1.262e-03, -1.608e-03, -1.656e-03,  ..., 1.704e-03, 1.659e-03, -1.568e-03],\n",
       "           [-1.279e-03, -1.184e-03, -1.636e-03,  ..., -1.185e-03, -1.601e-03, -1.500e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.13.self_attn.q_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[1.137e-03, -1.207e-03, 1.130e-03,  ..., -1.178e-03, -1.382e-03, 1.350e-03],\n",
       "           [-1.173e-03, -1.582e-03, 1.170e-03,  ..., 1.314e-03, -1.071e-03, 1.320e-03],\n",
       "           [-2.726e-03, 1.709e-03, -1.414e-03,  ..., -1.558e-03, -1.392e-03, -1.116e-03],\n",
       "           ...,\n",
       "           [3.128e-03, 2.043e-03, -2.647e-03,  ..., -2.789e-03, 2.176e-03, -1.974e-03],\n",
       "           [2.274e-03, -2.192e-03, 1.957e-03,  ..., -3.138e-03, 2.632e-03, 2.501e-03],\n",
       "           [2.867e-03, -3.164e-03, 2.033e-03,  ..., -2.411e-03, -2.615e-03, 2.337e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.13.self_attn.k_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-2.836e-03, -1.752e-03, -1.486e-03,  ..., -1.124e-03, 1.009e-03, 1.228e-03],\n",
       "           [2.645e-03, 1.575e-03, -1.404e-03,  ..., 1.117e-03, -1.410e-03, 1.358e-03],\n",
       "           [-3.225e-03, -1.533e-03, -1.489e-03,  ..., -1.158e-03, 1.143e-03, 1.064e-03],\n",
       "           ...,\n",
       "           [6.699e-03, 4.208e-03, -5.001e-03,  ..., 3.424e-03, 3.527e-03, -2.569e-03],\n",
       "           [-5.295e-03, -4.913e-03, -4.025e-03,  ..., 3.641e-03, -3.002e-03, -2.817e-03],\n",
       "           [-4.139e-03, 4.486e-03, -4.448e-03,  ..., 3.199e-03, 3.242e-03, -3.418e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.13.self_attn.v_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-1.513e-03, 7.949e-04, -9.832e-04,  ..., 1.250e-03, 7.501e-04, 8.650e-04],\n",
       "           [-7.024e-04, 6.213e-04, -8.497e-04,  ..., 8.988e-04, -7.954e-04, 8.526e-04],\n",
       "           [-6.571e-04, -7.205e-04, 8.016e-04,  ..., -1.017e-03, -1.070e-03, -9.174e-04],\n",
       "           ...,\n",
       "           [-1.151e-03, 1.451e-03, 1.278e-03,  ..., -1.529e-03, 1.650e-03, 1.457e-03],\n",
       "           [2.069e-03, -1.193e-03, 1.149e-03,  ..., -1.596e-03, -1.498e-03, 1.232e-03],\n",
       "           [-8.545e-04, 1.198e-03, -1.271e-03,  ..., 1.750e-03, 1.337e-03, 1.658e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.13.self_attn.o_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[6.992e-03, -5.108e-03, -4.925e-03,  ..., 5.901e-03, 6.241e-03, 7.183e-03],\n",
       "           [-9.584e-04, -1.143e-03, 1.413e-03,  ..., 2.399e-03, 1.624e-03, 9.470e-04],\n",
       "           [-1.455e-03, 1.617e-03, -1.001e-03,  ..., 1.564e-03, 1.811e-03, 1.226e-03],\n",
       "           ...,\n",
       "           [-1.273e-03, -1.531e-03, -1.460e-03,  ..., 2.304e-03, 1.256e-03, -1.245e-03],\n",
       "           [1.622e-03, -1.514e-03, 1.685e-03,  ..., 2.064e-03, 1.680e-03, -1.203e-03],\n",
       "           [-1.047e-03, 1.483e-03, -1.549e-03,  ..., 1.774e-03, -1.355e-03, 1.230e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.13.mlp.gate_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[5.276e-03, -2.607e-03, -1.694e-03,  ..., -9.823e-04, -1.512e-03, 1.006e-03],\n",
       "           [4.433e-03, -1.881e-03, -1.444e-03,  ..., 8.693e-04, -9.832e-04, -9.737e-04],\n",
       "           [-6.919e-04, 1.010e-03, 1.189e-03,  ..., 1.021e-03, 1.202e-03, 8.240e-04],\n",
       "           ...,\n",
       "           [-1.808e-03, 2.110e-03, -1.508e-03,  ..., -1.503e-03, 1.694e-03, -1.738e-03],\n",
       "           [-1.376e-03, -1.033e-03, -1.304e-03,  ..., -1.609e-03, -1.608e-03, -1.361e-03],\n",
       "           [-1.241e-03, 1.311e-03, -1.469e-03,  ..., -1.642e-03, -1.493e-03, -1.624e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.13.mlp.up_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[9.323e-03, 1.184e-03, 1.204e-03,  ..., -9.546e-04, -1.302e-03, 1.027e-03],\n",
       "           [6.077e-03, -3.738e-03, -2.409e-03,  ..., -1.554e-03, -1.907e-03, -1.821e-03],\n",
       "           [3.286e-03, -3.180e-03, -1.916e-03,  ..., -1.419e-03, -1.825e-03, 1.393e-03],\n",
       "           ...,\n",
       "           [-1.454e-03, 1.554e-03, 1.503e-03,  ..., 1.528e-03, 1.476e-03, 1.384e-03],\n",
       "           [1.227e-03, -1.400e-03, 1.328e-03,  ..., -1.568e-03, -1.959e-03, -1.455e-03],\n",
       "           [-1.480e-03, 1.902e-03, -2.026e-03,  ..., 1.406e-03, 1.237e-03, -1.476e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.13.mlp.down_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-3.198e-02, -1.141e-02, -1.263e-02,  ..., 6.664e-03, 6.191e-03, 5.795e-03],\n",
       "           [-1.404e-03, -1.665e-03, 3.052e-03,  ..., 9.937e-04, 9.108e-04, -8.793e-04],\n",
       "           [3.479e-03, 4.467e-03, -2.361e-03,  ..., 1.037e-03, 1.081e-03, -1.139e-03],\n",
       "           ...,\n",
       "           [1.694e-03, 1.447e-03, 2.104e-03,  ..., -1.787e-03, -1.610e-03, 1.513e-03],\n",
       "           [-1.344e-03, 1.724e-03, -2.373e-03,  ..., -1.698e-03, 1.646e-03, 1.663e-03],\n",
       "           [-1.691e-03, -2.148e-03, -1.966e-03,  ..., -1.617e-03, -1.617e-03, -2.163e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.14.self_attn.q_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[1.535e-03, -2.113e-03, 1.684e-03,  ..., 1.367e-03, -1.729e-03, 1.635e-03],\n",
       "           [-1.596e-03, -2.474e-03, -2.066e-03,  ..., -1.811e-03, 1.902e-03, -1.445e-03],\n",
       "           [-1.535e-03, 2.085e-03, 1.970e-03,  ..., -1.601e-03, -1.574e-03, 2.020e-03],\n",
       "           ...,\n",
       "           [-1.475e-03, -1.987e-03, 2.151e-03,  ..., 2.842e-03, 2.710e-03, 2.779e-03],\n",
       "           [-1.545e-03, -1.982e-03, -2.590e-03,  ..., 2.787e-03, 2.876e-03, -3.021e-03],\n",
       "           [-2.712e-03, -1.773e-03, -2.226e-03,  ..., 2.853e-03, -2.304e-03, -2.735e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.14.self_attn.k_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-1.929e-03, 1.740e-03, -2.058e-03,  ..., 1.745e-03, 1.817e-03, -1.606e-03],\n",
       "           [-1.871e-03, 1.729e-03, -2.007e-03,  ..., 1.497e-03, -1.759e-03, -1.540e-03],\n",
       "           [2.996e-03, 2.119e-03, 2.230e-03,  ..., 1.617e-03, -1.689e-03, -1.571e-03],\n",
       "           ...,\n",
       "           [6.390e-03, 4.578e-03, -4.486e-03,  ..., 3.645e-03, -3.660e-03, 3.559e-03],\n",
       "           [6.325e-03, 5.173e-03, 3.653e-03,  ..., 3.345e-03, 3.704e-03, -3.536e-03],\n",
       "           [8.049e-03, 6.252e-03, -3.313e-03,  ..., 2.865e-03, 2.932e-03, -3.006e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.14.self_attn.v_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-7.272e-04, -9.751e-04, -1.133e-03,  ..., 9.742e-04, -1.020e-03, -1.229e-03],\n",
       "           [9.713e-04, -1.267e-03, 1.225e-03,  ..., 1.272e-03, -1.182e-03, -1.347e-03],\n",
       "           [1.361e-03, 1.399e-03, -1.412e-03,  ..., -1.402e-03, -1.298e-03, 1.264e-03],\n",
       "           ...,\n",
       "           [-6.967e-04, 7.629e-04, -1.176e-03,  ..., -1.064e-03, 1.437e-03, 1.545e-03],\n",
       "           [-2.308e-03, -1.115e-03, -1.045e-03,  ..., 1.777e-03, 1.282e-03, -1.414e-03],\n",
       "           [1.854e-03, -1.180e-03, -1.108e-03,  ..., 1.143e-03, -1.290e-03, 9.871e-04]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.14.self_attn.o_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-2.960e-03, 3.038e-03, 4.353e-03,  ..., -3.082e-03, 2.476e-03, -2.977e-03],\n",
       "           [8.330e-04, -9.842e-04, -7.730e-04,  ..., 9.189e-04, -9.375e-04, 8.163e-04],\n",
       "           [-1.007e-03, 9.050e-04, 1.934e-03,  ..., -1.213e-03, -1.050e-03, -1.133e-03],\n",
       "           ...,\n",
       "           [1.687e-03, 1.094e-03, 1.224e-03,  ..., 1.235e-03, -1.493e-03, 1.119e-03],\n",
       "           [1.450e-03, 1.305e-03, -1.312e-03,  ..., -1.549e-03, -1.376e-03, -1.331e-03],\n",
       "           [1.423e-03, -1.229e-03, 1.571e-03,  ..., -2.005e-03, 1.288e-03, -1.307e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.14.mlp.gate_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[3.632e-03, 1.636e-03, -1.051e-03,  ..., 7.644e-04, 8.078e-04, 1.070e-03],\n",
       "           [4.269e-03, -2.344e-03, 1.300e-03,  ..., -1.091e-03, -8.473e-04, -1.081e-03],\n",
       "           [-2.029e-03, 1.141e-03, 7.653e-04,  ..., 6.289e-04, -5.803e-04, -6.137e-04],\n",
       "           ...,\n",
       "           [1.184e-03, -1.999e-03, -1.930e-03,  ..., 1.888e-03, 1.855e-03, 1.643e-03],\n",
       "           [1.256e-03, -1.489e-03, 1.495e-03,  ..., -1.416e-03, 1.291e-03, 1.710e-03],\n",
       "           [1.161e-03, -1.293e-03, 1.127e-03,  ..., 1.744e-03, -1.736e-03, 1.914e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.14.mlp.up_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-6.817e-03, 1.424e-03, -9.708e-04,  ..., 1.239e-03, 1.143e-03, 1.262e-03],\n",
       "           [-6.573e-03, 3.582e-03, 2.449e-03,  ..., -2.165e-03, -2.186e-03, -2.508e-03],\n",
       "           [-2.192e-03, -1.388e-03, 1.459e-03,  ..., -1.580e-03, -1.232e-03, -1.481e-03],\n",
       "           ...,\n",
       "           [-1.052e-03, -1.255e-03, 1.511e-03,  ..., -2.102e-03, 1.540e-03, 1.737e-03],\n",
       "           [-1.557e-03, -1.607e-03, 1.482e-03,  ..., 1.552e-03, -1.539e-03, 1.485e-03],\n",
       "           [-1.341e-03, -1.554e-03, -1.816e-03,  ..., -2.625e-03, 2.522e-03, -1.502e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.14.mlp.down_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[3.256e-02, 1.203e-02, -1.263e-02,  ..., -6.340e-03, -7.523e-03, 5.981e-03],\n",
       "           [-2.285e-03, -2.014e-03, -1.963e-03,  ..., -1.120e-03, -1.281e-03, -1.904e-03],\n",
       "           [2.890e-03, 2.707e-03, -2.230e-03,  ..., -9.832e-04, -1.025e-03, 8.106e-04],\n",
       "           ...,\n",
       "           [-1.173e-03, 2.052e-03, 1.905e-03,  ..., -1.494e-03, -1.647e-03, -1.916e-03],\n",
       "           [1.340e-03, 1.913e-03, -1.710e-03,  ..., 1.944e-03, -1.710e-03, 1.873e-03],\n",
       "           [1.498e-03, -1.947e-03, -1.328e-03,  ..., 1.792e-03, -2.003e-03, -1.673e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.15.self_attn.q_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[2.310e-03, 1.961e-03, -1.667e-03,  ..., 1.365e-03, -1.580e-03, -1.351e-03],\n",
       "           [2.831e-03, 2.714e-03, 2.253e-03,  ..., -1.754e-03, 1.525e-03, 1.795e-03],\n",
       "           [-2.075e-03, 1.714e-03, 1.639e-03,  ..., 1.256e-03, 1.134e-03, 1.176e-03],\n",
       "           ...,\n",
       "           [1.391e-03, -2.367e-03, -2.094e-03,  ..., 3.128e-03, -2.443e-03, -2.356e-03],\n",
       "           [2.981e-03, -2.323e-03, 2.048e-03,  ..., -2.329e-03, -2.869e-03, 2.352e-03],\n",
       "           [2.247e-03, -2.441e-03, -2.329e-03,  ..., -2.556e-03, -3.115e-03, 2.981e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.15.self_attn.k_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[1.420e-03, -1.631e-03, 1.709e-03,  ..., -1.389e-03, -1.439e-03, -1.530e-03],\n",
       "           [2.283e-03, 1.541e-03, 1.694e-03,  ..., 1.069e-03, 1.170e-03, -1.469e-03],\n",
       "           [-1.536e-03, 1.660e-03, -1.342e-03,  ..., -1.405e-03, 1.279e-03, -1.703e-03],\n",
       "           ...,\n",
       "           [-3.899e-03, -3.859e-03, -3.155e-03,  ..., -3.593e-03, -4.261e-03, 3.693e-03],\n",
       "           [1.043e-02, 3.500e-03, 2.743e-03,  ..., 2.777e-03, -2.758e-03, 2.131e-03],\n",
       "           [-5.775e-03, -5.630e-03, 3.841e-03,  ..., 3.019e-03, -2.934e-03, 3.586e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.15.self_attn.v_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[1.359e-03, -1.122e-03, 1.418e-03,  ..., 1.472e-03, -1.331e-03, 1.372e-03],\n",
       "           [-9.599e-04, -1.410e-03, 1.561e-03,  ..., 1.770e-03, 2.239e-03, 1.377e-03],\n",
       "           [-1.466e-03, -1.744e-03, -1.427e-03,  ..., -1.446e-03, 1.595e-03, 1.770e-03],\n",
       "           ...,\n",
       "           [-8.097e-04, 1.260e-03, -1.359e-03,  ..., 1.810e-03, 1.507e-03, -1.779e-03],\n",
       "           [1.511e-03, 1.265e-03, -1.615e-03,  ..., 1.603e-03, -1.534e-03, -1.885e-03],\n",
       "           [-7.296e-04, 1.574e-03, 1.766e-03,  ..., 1.604e-03, 2.159e-03, -1.648e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.15.self_attn.o_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[4.929e-03, 4.757e-03, -4.597e-03,  ..., 3.244e-03, -1.467e-03, 5.322e-03],\n",
       "           [-9.918e-04, 1.228e-03, 1.241e-03,  ..., 8.841e-04, 7.858e-04, 8.082e-04],\n",
       "           [-1.074e-03, -1.552e-03, 1.248e-03,  ..., 1.049e-03, -9.437e-04, 9.747e-04],\n",
       "           ...,\n",
       "           [1.304e-03, -1.285e-03, 1.253e-03,  ..., -2.283e-03, 1.521e-03, -1.480e-03],\n",
       "           [1.496e-03, 1.700e-03, -1.737e-03,  ..., -1.558e-03, -1.747e-03, 1.544e-03],\n",
       "           [1.598e-03, -1.653e-03, -1.381e-03,  ..., 1.283e-03, -1.408e-03, -1.217e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.15.mlp.gate_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[4.272e-03, 2.426e-03, 1.497e-03,  ..., -9.928e-04, 1.149e-03, -1.206e-03],\n",
       "           [-8.369e-03, 4.906e-03, 3.511e-03,  ..., 1.789e-03, -1.545e-03, -1.720e-03],\n",
       "           [-4.601e-03, 1.292e-02, -1.471e-02,  ..., 2.544e-03, -2.796e-03, -2.119e-03],\n",
       "           ...,\n",
       "           [1.209e-03, -1.178e-03, -1.074e-03,  ..., 2.045e-03, -1.742e-03, -1.441e-03],\n",
       "           [1.076e-03, 1.217e-03, 1.318e-03,  ..., 1.494e-03, 1.459e-03, -1.928e-03],\n",
       "           [1.231e-03, 1.245e-03, -1.489e-03,  ..., 1.296e-03, 1.409e-03, -1.609e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.15.mlp.up_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-6.840e-03, -1.008e-03, 9.871e-04,  ..., 1.217e-03, -1.124e-03, -1.285e-03],\n",
       "           [-3.187e-03, -1.904e-03, -1.326e-03,  ..., 7.582e-04, 7.639e-04, -6.967e-04],\n",
       "           [2.094e-03, 2.676e-03, -2.943e-03,  ..., -9.456e-04, -8.035e-04, -8.168e-04],\n",
       "           ...,\n",
       "           [1.174e-03, -1.483e-03, 1.597e-03,  ..., -2.159e-03, 1.563e-03, 2.241e-03],\n",
       "           [-1.328e-03, 1.451e-03, 1.252e-03,  ..., 1.665e-03, 1.704e-03, -1.715e-03],\n",
       "           [9.160e-04, -1.503e-03, 1.453e-03,  ..., 1.825e-03, -1.380e-03, -2.069e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.15.mlp.down_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[3.430e-02, 7.828e-03, -7.820e-03,  ..., -5.302e-03, -4.276e-03, -7.965e-03],\n",
       "           [3.174e-03, 2.264e-03, -3.090e-03,  ..., -1.076e-03, 1.135e-03, 1.250e-03],\n",
       "           [-3.475e-03, 2.987e-03, -2.800e-03,  ..., 1.256e-03, 1.929e-03, -1.484e-03],\n",
       "           ...,\n",
       "           [1.052e-03, 1.183e-03, -1.174e-03,  ..., -1.766e-03, 1.443e-03, 2.014e-03],\n",
       "           [-1.322e-03, -1.348e-03, 2.243e-03,  ..., -2.024e-03, 1.531e-03, 1.949e-03],\n",
       "           [-1.328e-03, -1.209e-03, -2.100e-03,  ..., -2.113e-03, 1.735e-03, 1.384e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.16.self_attn.q_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[3.307e-03, 3.874e-03, -3.258e-03,  ..., -2.768e-03, 2.682e-03, 2.485e-03],\n",
       "           [-3.035e-03, 1.491e-03, 1.805e-03,  ..., -1.578e-03, 1.588e-03, 1.341e-03],\n",
       "           [2.373e-03, -1.900e-03, -1.374e-03,  ..., -1.431e-03, 1.877e-03, 1.651e-03],\n",
       "           ...,\n",
       "           [3.668e-03, 2.590e-03, -1.902e-03,  ..., 2.604e-03, -2.756e-03, -2.954e-03],\n",
       "           [-2.325e-03, 1.966e-03, -1.493e-03,  ..., -2.316e-03, 2.560e-03, -2.964e-03],\n",
       "           [1.649e-03, 2.110e-03, -1.555e-03,  ..., 2.459e-03, -3.017e-03, 2.275e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.16.self_attn.k_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-1.718e-03, 1.207e-03, -1.779e-03,  ..., -1.251e-03, 1.135e-03, -9.022e-04],\n",
       "           [-2.720e-03, -1.828e-03, 1.776e-03,  ..., -2.026e-03, 1.953e-03, -1.757e-03],\n",
       "           [2.106e-03, -1.705e-03, 1.326e-03,  ..., 1.404e-03, 1.405e-03, -1.352e-03],\n",
       "           ...,\n",
       "           [1.112e-02, -4.856e-03, -3.519e-03,  ..., 3.006e-03, -2.834e-03, 2.783e-03],\n",
       "           [-7.507e-03, 5.962e-03, -4.955e-03,  ..., -3.893e-03, -4.543e-03, 3.435e-03],\n",
       "           [-4.230e-03, 4.734e-03, 5.386e-03,  ..., -4.337e-03, -4.242e-03, -5.379e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.16.self_attn.v_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[7.081e-04, -1.016e-03, 8.702e-04,  ..., 1.694e-03, -1.155e-03, 1.345e-03],\n",
       "           [8.779e-04, 9.975e-04, 1.118e-03,  ..., -1.120e-03, -1.192e-03, -1.133e-03],\n",
       "           [8.116e-04, 1.060e-03, -1.477e-03,  ..., 1.250e-03, -1.150e-03, 1.066e-03],\n",
       "           ...,\n",
       "           [-1.895e-03, -8.712e-04, 1.044e-03,  ..., 1.086e-03, -1.525e-03, 1.106e-03],\n",
       "           [7.997e-04, 9.804e-04, 1.138e-03,  ..., 1.657e-03, -1.470e-03, -1.189e-03],\n",
       "           [-1.436e-03, 9.527e-04, -8.388e-04,  ..., -1.216e-03, 1.342e-03, 1.339e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.16.self_attn.o_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[1.846e-02, -9.186e-03, 3.569e-03,  ..., -2.892e-03, 4.005e-03, 3.223e-03],\n",
       "           [-1.788e-03, -9.069e-04, 1.082e-03,  ..., -9.470e-04, 7.486e-04, 8.616e-04],\n",
       "           [-1.401e-03, 8.330e-04, 1.068e-03,  ..., 9.251e-04, -7.067e-04, 9.604e-04],\n",
       "           ...,\n",
       "           [-1.706e-03, -1.523e-03, 1.947e-03,  ..., -1.297e-03, 1.397e-03, -1.527e-03],\n",
       "           [-1.615e-03, -1.674e-03, -1.803e-03,  ..., 1.392e-03, 1.126e-03, -1.387e-03],\n",
       "           [-1.555e-03, 1.561e-03, 1.613e-03,  ..., -1.578e-03, 1.366e-03, 1.181e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.16.mlp.gate_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[4.906e-03, 2.674e-03, -1.913e-03,  ..., -1.552e-03, 1.613e-03, 1.745e-03],\n",
       "           [6.168e-03, -3.326e-03, -3.506e-03,  ..., 2.842e-03, 2.304e-03, 2.346e-03],\n",
       "           [-4.406e-03, 6.721e-03, -5.402e-03,  ..., 3.723e-03, -3.551e-03, -3.387e-03],\n",
       "           ...,\n",
       "           [1.268e-03, 1.349e-03, 1.565e-03,  ..., -1.485e-03, -1.458e-03, -1.652e-03],\n",
       "           [1.676e-03, 1.501e-03, 1.348e-03,  ..., 1.520e-03, -1.774e-03, -1.374e-03],\n",
       "           [1.059e-03, -1.006e-03, -1.429e-03,  ..., 1.726e-03, -1.624e-03, 1.386e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.16.mlp.up_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-7.446e-03, 1.683e-03, -1.625e-03,  ..., -1.794e-03, -1.685e-03, -1.596e-03],\n",
       "           [2.787e-03, -1.842e-03, -1.454e-03,  ..., -8.798e-04, -6.442e-04, -9.060e-04],\n",
       "           [-2.180e-03, -1.708e-03, 1.327e-03,  ..., -7.548e-04, -9.699e-04, 9.727e-04],\n",
       "           ...,\n",
       "           [1.238e-03, 1.438e-03, -1.477e-03,  ..., 1.656e-03, -1.571e-03, 1.733e-03],\n",
       "           [1.867e-03, -1.611e-03, -1.396e-03,  ..., -1.439e-03, 1.495e-03, 1.863e-03],\n",
       "           [1.044e-03, -1.443e-03, 1.554e-03,  ..., -2.731e-03, -2.037e-03, 1.751e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.16.mlp.down_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[3.534e-02, 1.796e-02, -2.304e-02,  ..., -1.478e-02, -3.664e-03, -5.268e-03],\n",
       "           [-3.628e-03, -2.243e-03, -3.195e-03,  ..., 1.530e-03, 1.383e-03, -9.985e-04],\n",
       "           [3.872e-03, -4.498e-03, 5.043e-03,  ..., 1.205e-03, 1.419e-03, -1.158e-03],\n",
       "           ...,\n",
       "           [1.043e-03, 1.173e-03, -1.368e-03,  ..., 1.708e-03, 1.566e-03, 1.717e-03],\n",
       "           [-9.518e-04, 1.271e-03, -1.420e-03,  ..., 1.816e-03, 1.410e-03, 1.916e-03],\n",
       "           [1.023e-03, -1.141e-03, 1.558e-03,  ..., -1.348e-03, 1.706e-03, -1.886e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.17.self_attn.q_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-1.080e-03, -1.313e-03, -1.493e-03,  ..., -1.586e-03, 1.409e-03, 1.771e-03],\n",
       "           [-1.204e-03, 1.633e-03, 1.347e-03,  ..., -1.306e-03, -1.586e-03, 1.718e-03],\n",
       "           [1.501e-03, -1.686e-03, -1.499e-03,  ..., -1.230e-03, -1.640e-03, 1.540e-03],\n",
       "           ...,\n",
       "           [2.333e-03, 1.733e-03, -1.631e-03,  ..., 2.628e-03, 3.334e-03, 2.695e-03],\n",
       "           [-4.669e-03, -2.056e-03, 2.172e-03,  ..., -2.182e-03, 1.941e-03, 2.735e-03],\n",
       "           [-2.342e-03, -1.731e-03, 2.289e-03,  ..., 2.401e-03, -1.814e-03, -1.991e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.17.self_attn.k_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-1.886e-03, -1.639e-03, -1.699e-03,  ..., 1.348e-03, -1.169e-03, 1.118e-03],\n",
       "           [-2.079e-03, 1.351e-03, -1.429e-03,  ..., -1.289e-03, -1.242e-03, -1.092e-03],\n",
       "           [-1.349e-03, -1.211e-03, -1.379e-03,  ..., 1.259e-03, 1.453e-03, 1.348e-03],\n",
       "           ...,\n",
       "           [6.321e-03, 4.677e-03, 4.307e-03,  ..., -3.338e-03, -3.351e-03, -3.588e-03],\n",
       "           [-1.339e-02, 4.620e-03, -3.281e-03,  ..., 2.487e-03, -3.260e-03, -2.914e-03],\n",
       "           [4.959e-03, 4.330e-03, -4.608e-03,  ..., 3.759e-03, -3.635e-03, -3.515e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.17.self_attn.v_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[1.106e-03, -1.629e-03, 2.199e-03,  ..., 1.827e-03, 1.936e-03, 1.980e-03],\n",
       "           [1.420e-03, 2.132e-03, 1.441e-03,  ..., -1.525e-03, -1.746e-03, 1.848e-03],\n",
       "           [1.583e-03, 1.763e-03, -1.876e-03,  ..., 2.357e-03, -1.826e-03, -1.800e-03],\n",
       "           ...,\n",
       "           [-1.354e-03, 1.686e-03, 1.450e-03,  ..., 1.311e-03, 1.539e-03, 1.270e-03],\n",
       "           [-1.066e-03, -2.245e-03, -2.024e-03,  ..., 1.480e-03, -1.555e-03, -1.300e-03],\n",
       "           [-8.779e-04, -9.880e-04, -1.218e-03,  ..., 1.167e-03, -1.149e-03, -1.372e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.17.self_attn.o_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[1.437e-02, 1.004e-02, 4.864e-03,  ..., 3.202e-03, 2.842e-03, -5.161e-03],\n",
       "           [1.527e-03, 1.818e-03, -1.976e-03,  ..., -1.281e-03, -1.185e-03, 1.237e-03],\n",
       "           [-1.375e-03, 2.260e-03, 1.629e-03,  ..., 1.966e-03, 1.701e-03, 1.309e-03],\n",
       "           ...,\n",
       "           [-1.710e-03, -1.873e-03, -1.706e-03,  ..., -1.583e-03, 1.104e-03, -1.379e-03],\n",
       "           [1.926e-03, 1.478e-03, 1.702e-03,  ..., 1.719e-03, -1.260e-03, -1.596e-03],\n",
       "           [-1.499e-03, -1.646e-03, 1.782e-03,  ..., 1.733e-03, 9.422e-04, -1.266e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.17.mlp.gate_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[6.130e-03, 3.405e-03, 2.712e-03,  ..., 1.786e-03, 1.706e-03, -1.569e-03],\n",
       "           [2.243e-03, -1.551e-03, 1.945e-03,  ..., -9.322e-04, -7.739e-04, -9.675e-04],\n",
       "           [6.077e-03, 3.307e-03, -3.174e-03,  ..., 2.142e-03, -2.569e-03, -2.024e-03],\n",
       "           ...,\n",
       "           [1.794e-03, 1.247e-03, 1.257e-03,  ..., -1.470e-03, 1.347e-03, 1.206e-03],\n",
       "           [-1.628e-03, 1.247e-03, 1.124e-03,  ..., 1.268e-03, 1.082e-03, -1.187e-03],\n",
       "           [-1.086e-03, -1.610e-03, -1.616e-03,  ..., -1.399e-03, -1.713e-03, -1.819e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.17.mlp.up_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[7.359e-03, -1.403e-03, -1.530e-03,  ..., 1.440e-03, 1.791e-03, 1.611e-03],\n",
       "           [-3.540e-03, 3.160e-03, -2.863e-03,  ..., 4.435e-04, 5.016e-04, -4.785e-04],\n",
       "           [3.883e-03, -1.414e-03, -1.091e-03,  ..., 8.044e-04, -8.435e-04, -9.084e-04],\n",
       "           ...,\n",
       "           [-1.026e-03, 1.340e-03, 1.353e-03,  ..., 1.959e-03, -1.375e-03, 1.341e-03],\n",
       "           [-1.440e-03, 1.204e-03, -1.575e-03,  ..., 1.435e-03, 1.443e-03, 1.636e-03],\n",
       "           [1.244e-03, -1.192e-03, -1.516e-03,  ..., -1.521e-03, -1.552e-03, 1.554e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.17.mlp.down_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-3.925e-02, 1.504e-02, -1.232e-02,  ..., -3.426e-03, -3.880e-03, 3.469e-03],\n",
       "           [4.490e-03, 2.947e-03, -1.767e-03,  ..., 1.268e-03, -1.830e-03, -1.815e-03],\n",
       "           [3.906e-03, 2.443e-03, 3.199e-03,  ..., 1.186e-03, 1.261e-03, -1.475e-03],\n",
       "           ...,\n",
       "           [1.384e-03, 1.391e-03, -1.322e-03,  ..., -1.480e-03, -1.634e-03, 1.213e-03],\n",
       "           [1.381e-03, 1.488e-03, 1.287e-03,  ..., 1.570e-03, 1.699e-03, 1.453e-03],\n",
       "           [-1.721e-03, 1.443e-03, -1.376e-03,  ..., -2.104e-03, -1.592e-03, -1.555e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.18.self_attn.q_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-1.917e-03, 1.410e-03, -1.433e-03,  ..., -1.558e-03, -1.173e-03, 1.293e-03],\n",
       "           [1.798e-03, 1.422e-03, -1.328e-03,  ..., -1.625e-03, -1.385e-03, 1.449e-03],\n",
       "           [-1.553e-03, 2.033e-03, -1.752e-03,  ..., -2.739e-03, 2.085e-03, 1.987e-03],\n",
       "           ...,\n",
       "           [-2.283e-03, -1.430e-03, 1.738e-03,  ..., 2.018e-03, -1.925e-03, -2.832e-03],\n",
       "           [-1.925e-03, 2.577e-03, 1.823e-03,  ..., 3.010e-03, -3.004e-03, -2.075e-03],\n",
       "           [-3.391e-03, -2.106e-03, 1.639e-03,  ..., -2.100e-03, 2.607e-03, -2.192e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.18.self_attn.k_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[2.773e-03, -2.153e-03, -2.310e-03,  ..., 1.780e-03, -1.714e-03, -1.773e-03],\n",
       "           [-2.541e-03, 2.186e-03, 1.884e-03,  ..., -1.937e-03, 1.356e-03, -1.863e-03],\n",
       "           [2.928e-03, 2.556e-03, -1.746e-03,  ..., 1.737e-03, -1.503e-03, -2.100e-03],\n",
       "           ...,\n",
       "           [6.195e-03, 3.994e-03, 2.758e-03,  ..., 3.204e-03, -2.619e-03, -3.010e-03],\n",
       "           [-3.468e-03, 4.494e-03, 3.218e-03,  ..., -3.868e-03, 4.387e-03, -3.222e-03],\n",
       "           [5.566e-03, -3.164e-03, -3.149e-03,  ..., -2.087e-03, 2.100e-03, -2.378e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.18.self_attn.v_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-6.375e-04, 1.083e-03, 1.313e-03,  ..., -1.264e-03, 1.410e-03, 1.218e-03],\n",
       "           [-5.455e-04, 9.546e-04, -1.300e-03,  ..., -1.549e-03, -1.502e-03, -1.278e-03],\n",
       "           [-7.958e-04, 9.804e-04, 1.526e-03,  ..., 1.260e-03, 9.794e-04, 1.462e-03],\n",
       "           ...,\n",
       "           [-1.396e-03, -1.451e-03, -1.593e-03,  ..., -1.266e-03, 1.822e-03, 1.769e-03],\n",
       "           [-1.581e-03, -1.427e-03, -1.269e-03,  ..., 1.508e-03, -1.458e-03, -1.455e-03],\n",
       "           [9.565e-04, -1.356e-03, -1.214e-03,  ..., 1.625e-03, 2.033e-03, -2.033e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.18.self_attn.o_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-5.665e-03, -7.820e-03, 3.220e-03,  ..., -1.055e-02, 2.619e-03, -7.156e-03],\n",
       "           [-2.153e-03, 1.020e-03, -1.054e-03,  ..., 9.604e-04, 9.751e-04, -1.406e-03],\n",
       "           [9.098e-04, -8.669e-04, 1.330e-03,  ..., 1.404e-03, -9.470e-04, 1.355e-03],\n",
       "           ...,\n",
       "           [-1.382e-03, 1.351e-03, 1.449e-03,  ..., -1.547e-03, 1.410e-03, 1.719e-03],\n",
       "           [1.192e-03, 1.147e-03, -1.684e-03,  ..., -1.423e-03, 1.802e-03, -1.608e-03],\n",
       "           [1.330e-03, -1.534e-03, -1.312e-03,  ..., 1.715e-03, -1.712e-03, 1.786e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.18.mlp.gate_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-6.058e-03, 3.063e-03, 1.853e-03,  ..., -1.159e-03, -1.354e-03, -1.147e-03],\n",
       "           [-2.045e-03, 1.395e-03, -8.311e-04,  ..., -8.421e-04, 9.675e-04, -6.604e-04],\n",
       "           [2.293e-03, 1.447e-03, 9.375e-04,  ..., 6.824e-04, -7.358e-04, -6.089e-04],\n",
       "           ...,\n",
       "           [1.554e-03, -1.768e-03, -1.463e-03,  ..., -1.028e-03, 1.282e-03, -1.307e-03],\n",
       "           [1.472e-03, 1.589e-03, 1.567e-03,  ..., 1.320e-03, 1.285e-03, -1.516e-03],\n",
       "           [1.959e-03, 1.510e-03, -1.628e-03,  ..., 1.823e-03, -2.087e-03, 1.389e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.18.mlp.up_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-6.229e-03, -1.842e-03, -1.688e-03,  ..., 1.172e-03, 1.150e-03, -1.113e-03],\n",
       "           [-2.514e-03, 1.172e-03, -1.116e-03,  ..., -1.288e-03, -1.348e-03, -1.331e-03],\n",
       "           [-1.542e-03, 1.095e-03, 1.199e-03,  ..., 1.218e-03, 1.469e-03, -9.494e-04],\n",
       "           ...,\n",
       "           [-1.328e-03, -1.649e-03, -1.822e-03,  ..., -1.571e-03, -1.624e-03, 1.554e-03],\n",
       "           [1.569e-03, -1.320e-03, 1.444e-03,  ..., 1.483e-03, -1.957e-03, -1.540e-03],\n",
       "           [1.616e-03, 1.501e-03, -1.490e-03,  ..., -1.601e-03, -1.904e-03, 1.606e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.18.mlp.down_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[3.928e-02, -1.252e-02, -6.580e-03,  ..., 3.639e-03, -3.321e-03, 4.593e-03],\n",
       "           [-3.828e-03, 2.743e-03, 1.210e-03,  ..., 1.369e-03, 1.557e-03, 1.351e-03],\n",
       "           [3.725e-03, 1.587e-03, 2.399e-03,  ..., 1.184e-03, 1.364e-03, -1.107e-03],\n",
       "           ...,\n",
       "           [1.136e-03, 1.605e-03, 1.347e-03,  ..., 1.450e-03, 2.142e-03, 1.455e-03],\n",
       "           [-1.325e-03, -1.266e-03, 1.440e-03,  ..., 1.537e-03, 1.508e-03, 1.762e-03],\n",
       "           [1.058e-03, -1.348e-03, 1.493e-03,  ..., 1.273e-03, 1.591e-03, -1.599e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.19.self_attn.q_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-2.209e-03, 2.064e-03, -1.584e-03,  ..., -1.431e-03, 1.543e-03, -1.848e-03],\n",
       "           [-1.982e-03, -1.730e-03, -1.493e-03,  ..., -1.691e-03, -1.493e-03, 1.512e-03],\n",
       "           [1.598e-03, -1.326e-03, -1.226e-03,  ..., -1.450e-03, -1.715e-03, 2.226e-03],\n",
       "           ...,\n",
       "           [-1.439e-03, 1.236e-03, -1.628e-03,  ..., 1.755e-03, -2.216e-03, -2.373e-03],\n",
       "           [-1.708e-03, 1.756e-03, 1.775e-03,  ..., 2.134e-03, 1.773e-03, -1.639e-03],\n",
       "           [-2.481e-03, 1.644e-03, -1.391e-03,  ..., -1.696e-03, -2.428e-03, -2.422e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.19.self_attn.k_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[3.527e-03, 3.092e-03, 2.325e-03,  ..., -1.661e-03, 1.336e-03, 1.751e-03],\n",
       "           [-1.694e-03, -1.539e-03, -1.840e-03,  ..., -1.281e-03, -1.539e-03, -1.576e-03],\n",
       "           [2.386e-03, 1.982e-03, -1.853e-03,  ..., 1.705e-03, 1.272e-03, -1.661e-03],\n",
       "           ...,\n",
       "           [-5.775e-03, -5.959e-03, 5.295e-03,  ..., 4.017e-03, 4.684e-03, -4.345e-03],\n",
       "           [6.382e-03, -5.394e-03, -4.501e-03,  ..., 4.585e-03, 2.790e-03, -3.887e-03],\n",
       "           [7.538e-03, -5.707e-03, -4.608e-03,  ..., -4.101e-03, -4.997e-03, -3.473e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.19.self_attn.v_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[1.019e-03, 1.206e-03, -1.503e-03,  ..., -1.364e-03, -1.435e-03, 1.520e-03],\n",
       "           [-7.944e-04, -8.893e-04, 1.546e-03,  ..., -1.637e-03, -1.707e-03, -1.401e-03],\n",
       "           [-1.056e-03, 8.774e-04, -1.408e-03,  ..., -1.953e-03, 1.521e-03, -1.591e-03],\n",
       "           ...,\n",
       "           [8.378e-04, 1.190e-03, 1.273e-03,  ..., 1.510e-03, -1.365e-03, -1.437e-03],\n",
       "           [1.302e-03, -1.201e-03, -1.258e-03,  ..., 1.270e-03, -1.485e-03, -1.779e-03],\n",
       "           [1.593e-03, 1.047e-03, -1.188e-03,  ..., 1.375e-03, 1.621e-03, -1.702e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.19.self_attn.o_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-2.783e-03, 3.088e-03, 4.292e-03,  ..., -4.528e-03, 4.410e-03, -1.237e-02],\n",
       "           [-1.177e-03, -1.441e-03, 1.493e-03,  ..., -1.331e-03, 1.335e-03, -1.167e-03],\n",
       "           [1.536e-03, -1.266e-03, 1.333e-03,  ..., -1.260e-03, -1.356e-03, -1.291e-03],\n",
       "           ...,\n",
       "           [-1.604e-03, -1.871e-03, 1.524e-03,  ..., 1.524e-03, 1.582e-03, -1.319e-03],\n",
       "           [-1.575e-03, -1.742e-03, -1.449e-03,  ..., -1.431e-03, -1.555e-03, -1.527e-03],\n",
       "           [1.370e-03, -1.514e-03, 1.538e-03,  ..., 1.203e-03, 1.168e-03, -1.219e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.19.mlp.gate_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[5.089e-03, 1.523e-03, 1.401e-03,  ..., 1.077e-03, -1.076e-03, 8.712e-04],\n",
       "           [-3.359e-03, 1.007e-02, 6.844e-03,  ..., 2.953e-03, -2.190e-03, -2.256e-03],\n",
       "           [-3.204e-03, 3.620e-03, 5.066e-03,  ..., -2.895e-03, 2.542e-03, 2.724e-03],\n",
       "           ...,\n",
       "           [-1.598e-03, -1.644e-03, 1.383e-03,  ..., -1.595e-03, -1.304e-03, 1.934e-03],\n",
       "           [1.740e-03, -2.178e-03, -1.704e-03,  ..., -2.207e-03, 2.148e-03, 1.740e-03],\n",
       "           [-1.465e-03, 1.919e-03, -1.521e-03,  ..., 1.980e-03, 1.673e-03, 1.756e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.19.mlp.up_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-6.237e-03, -1.116e-03, 1.276e-03,  ..., -1.459e-03, 1.360e-03, 1.135e-03],\n",
       "           [1.803e-03, 2.417e-03, 2.214e-03,  ..., -7.796e-04, 7.710e-04, -9.522e-04],\n",
       "           [-2.079e-03, -2.209e-03, 1.766e-03,  ..., 9.646e-04, 7.949e-04, -1.211e-03],\n",
       "           ...,\n",
       "           [-1.546e-03, 1.404e-03, 1.662e-03,  ..., -1.723e-03, 1.685e-03, -1.822e-03],\n",
       "           [1.164e-03, -1.183e-03, -1.467e-03,  ..., -1.512e-03, -1.810e-03, -1.419e-03],\n",
       "           [-1.454e-03, 1.477e-03, -1.501e-03,  ..., 1.518e-03, -1.451e-03, 1.489e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.19.mlp.down_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[3.885e-02, 1.215e-02, -1.076e-02,  ..., 4.284e-03, -3.365e-03, -3.067e-03],\n",
       "           [-3.340e-03, -2.104e-03, 2.729e-03,  ..., 1.330e-03, 1.529e-03, -1.753e-03],\n",
       "           [4.223e-03, -2.851e-03, 2.977e-03,  ..., 1.052e-03, 1.183e-03, -1.506e-03],\n",
       "           ...,\n",
       "           [1.406e-03, -1.446e-03, -1.941e-03,  ..., 1.607e-03, -1.713e-03, 1.835e-03],\n",
       "           [1.307e-03, 1.493e-03, -1.654e-03,  ..., 1.209e-03, 1.785e-03, 1.859e-03],\n",
       "           [1.442e-03, 1.493e-03, 1.319e-03,  ..., 2.188e-03, -1.565e-03, -1.356e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.20.self_attn.q_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[1.995e-03, -1.844e-03, 1.478e-03,  ..., -1.558e-03, -1.617e-03, -1.600e-03],\n",
       "           [-1.705e-03, -1.707e-03, 1.861e-03,  ..., -2.539e-03, 2.098e-03, -2.150e-03],\n",
       "           [1.924e-03, 1.914e-03, -1.556e-03,  ..., -1.509e-03, 1.952e-03, 1.543e-03],\n",
       "           ...,\n",
       "           [-1.594e-03, 1.595e-03, 1.931e-03,  ..., -2.304e-03, 3.277e-03, -2.510e-03],\n",
       "           [-1.693e-03, -1.807e-03, -2.602e-03,  ..., 2.100e-03, -2.409e-03, -2.726e-03],\n",
       "           [2.050e-03, -1.926e-03, -2.054e-03,  ..., 2.298e-03, -2.176e-03, -2.096e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.20.self_attn.k_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-2.613e-03, 3.107e-03, 2.462e-03,  ..., 1.781e-03, 1.754e-03, -1.664e-03],\n",
       "           [2.342e-03, 1.872e-03, 2.354e-03,  ..., -1.870e-03, 2.298e-03, 2.840e-03],\n",
       "           [-2.392e-03, -2.260e-03, 1.965e-03,  ..., -3.078e-03, 2.697e-03, 2.357e-03],\n",
       "           ...,\n",
       "           [3.176e-03, 2.714e-03, 3.584e-03,  ..., 3.304e-03, -3.441e-03, 3.485e-03],\n",
       "           [-3.162e-03, 3.479e-03, 3.384e-03,  ..., -3.750e-03, -4.990e-03, -3.439e-03],\n",
       "           [-4.337e-03, -3.328e-03, 2.878e-03,  ..., -3.647e-03, -3.736e-03, 2.975e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.20.self_attn.v_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[6.299e-04, -1.115e-03, 1.243e-03,  ..., -1.353e-03, -1.813e-03, 1.531e-03],\n",
       "           [9.027e-04, -1.056e-03, 1.219e-03,  ..., 1.617e-03, -1.241e-03, -1.733e-03],\n",
       "           [1.047e-03, -1.114e-03, 1.227e-03,  ..., 1.511e-03, 1.587e-03, -1.383e-03],\n",
       "           ...,\n",
       "           [1.060e-03, 1.246e-03, -1.635e-03,  ..., -2.136e-03, -2.249e-03, -2.201e-03],\n",
       "           [-1.074e-03, 1.470e-03, 1.767e-03,  ..., 2.350e-03, 2.245e-03, -2.344e-03],\n",
       "           [1.483e-03, 1.337e-03, -1.598e-03,  ..., -2.035e-03, -2.018e-03, 2.193e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.20.self_attn.o_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-3.439e-03, -6.680e-03, -6.115e-03,  ..., 4.665e-03, 5.554e-03, -9.308e-03],\n",
       "           [-1.326e-03, 2.052e-03, -1.361e-03,  ..., -1.838e-03, 1.773e-03, 1.703e-03],\n",
       "           [1.128e-03, -1.366e-03, -1.156e-03,  ..., -1.939e-03, 1.267e-03, -1.946e-03],\n",
       "           ...,\n",
       "           [-1.382e-03, 1.424e-03, 1.749e-03,  ..., 1.634e-03, -1.945e-03, -2.033e-03],\n",
       "           [1.331e-03, -1.551e-03, -1.552e-03,  ..., 1.587e-03, -1.552e-03, 1.566e-03],\n",
       "           [1.278e-03, 1.414e-03, -1.444e-03,  ..., 1.686e-03, 1.425e-03, 1.928e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.20.mlp.gate_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[4.780e-03, -1.578e-03, 1.425e-03,  ..., -1.042e-03, 1.071e-03, -1.356e-03],\n",
       "           [2.155e-03, -1.638e-03, 1.345e-03,  ..., -1.032e-03, 1.039e-03, 1.031e-03],\n",
       "           [-3.042e-03, 2.003e-03, -1.916e-03,  ..., 1.718e-03, -2.171e-03, 2.045e-03],\n",
       "           ...,\n",
       "           [-2.144e-03, 1.726e-03, 2.335e-03,  ..., 2.256e-03, 1.517e-03, 1.679e-03],\n",
       "           [1.418e-03, -1.428e-03, 1.955e-03,  ..., -1.499e-03, -1.431e-03, -1.480e-03],\n",
       "           [-1.423e-03, 1.616e-03, 1.717e-03,  ..., 1.558e-03, -1.895e-03, 1.864e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.20.mlp.up_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-4.532e-03, -2.014e-03, -1.454e-03,  ..., -1.374e-03, 1.333e-03, -1.198e-03],\n",
       "           [-5.657e-03, -1.228e-03, 1.445e-03,  ..., -1.303e-03, 2.085e-03, -1.428e-03],\n",
       "           [-2.268e-03, -1.734e-03, -2.151e-03,  ..., -1.487e-03, -1.650e-03, -1.806e-03],\n",
       "           ...,\n",
       "           [1.270e-03, 1.403e-03, 1.442e-03,  ..., -1.518e-03, -1.584e-03, -1.632e-03],\n",
       "           [1.353e-03, -1.648e-03, -1.402e-03,  ..., -1.312e-03, 1.579e-03, 1.349e-03],\n",
       "           [1.253e-03, 1.559e-03, 1.576e-03,  ..., 1.722e-03, -2.260e-03, 1.430e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.20.mlp.down_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[3.937e-02, 1.557e-02, 7.988e-03,  ..., -4.002e-03, 4.913e-03, -3.275e-03],\n",
       "           [-2.977e-03, -3.019e-03, 1.681e-03,  ..., 1.364e-03, -1.381e-03, -1.826e-03],\n",
       "           [-3.897e-03, 8.896e-03, -3.719e-03,  ..., -2.043e-03, 1.512e-03, -1.211e-03],\n",
       "           ...,\n",
       "           [-1.431e-03, 1.379e-03, -1.564e-03,  ..., 1.616e-03, -1.701e-03, 1.683e-03],\n",
       "           [1.735e-03, -1.473e-03, -1.372e-03,  ..., -1.632e-03, 2.083e-03, 1.343e-03],\n",
       "           [-9.742e-04, 1.451e-03, 1.262e-03,  ..., 1.772e-03, 1.647e-03, 1.976e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.21.self_attn.q_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-1.020e-03, -1.021e-03, -1.140e-03,  ..., 1.548e-03, 1.884e-03, 1.575e-03],\n",
       "           [1.002e-03, 1.226e-03, -1.707e-03,  ..., -2.180e-03, -1.944e-03, -1.536e-03],\n",
       "           [8.607e-04, -1.078e-03, 1.507e-03,  ..., -1.449e-03, 2.081e-03, 1.541e-03],\n",
       "           ...,\n",
       "           [2.073e-03, 2.100e-03, -2.104e-03,  ..., 2.283e-03, -3.225e-03, 1.945e-03],\n",
       "           [1.813e-03, -1.815e-03, 2.651e-03,  ..., -2.466e-03, 2.441e-03, 2.270e-03],\n",
       "           [-2.174e-03, 2.214e-03, 1.841e-03,  ..., 2.193e-03, -2.262e-03, -2.121e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.21.self_attn.k_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-1.295e-03, -1.091e-03, -1.217e-03,  ..., 1.559e-03, -1.357e-03, -1.434e-03],\n",
       "           [-1.642e-03, -2.073e-03, -1.451e-03,  ..., -1.901e-03, -1.811e-03, -1.535e-03],\n",
       "           [-1.116e-03, 1.216e-03, -1.274e-03,  ..., -1.405e-03, 1.881e-03, -1.342e-03],\n",
       "           ...,\n",
       "           [3.546e-03, -4.131e-03, -2.977e-03,  ..., 2.493e-03, 3.317e-03, -2.769e-03],\n",
       "           [-3.849e-03, -4.711e-03, 3.265e-03,  ..., -2.668e-03, -2.708e-03, 2.823e-03],\n",
       "           [-4.482e-03, -2.995e-03, 2.905e-03,  ..., 2.680e-03, -3.046e-03, -2.573e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.21.self_attn.v_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-1.863e-03, 2.092e-03, -1.698e-03,  ..., -1.666e-03, 1.466e-03, 1.880e-03],\n",
       "           [-1.288e-03, -1.674e-03, 1.986e-03,  ..., 1.838e-03, -1.522e-03, -1.614e-03],\n",
       "           [1.436e-03, -1.600e-03, -1.915e-03,  ..., 2.014e-03, 1.708e-03, 1.348e-03],\n",
       "           ...,\n",
       "           [8.321e-04, -1.770e-03, -1.680e-03,  ..., 1.883e-03, 1.710e-03, -2.012e-03],\n",
       "           [1.036e-03, 1.292e-03, 1.266e-03,  ..., -1.799e-03, -1.288e-03, 1.670e-03],\n",
       "           [-9.985e-04, -1.348e-03, 1.658e-03,  ..., 1.545e-03, 1.219e-03, 1.737e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.21.self_attn.o_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[1.051e-02, 5.405e-03, 4.528e-03,  ..., -8.400e-03, 9.987e-03, -5.234e-03],\n",
       "           [-1.902e-03, -1.837e-03, 2.415e-03,  ..., 1.550e-03, 1.639e-03, -1.300e-03],\n",
       "           [1.894e-03, 1.577e-03, -2.253e-03,  ..., -1.512e-03, 1.716e-03, 2.821e-03],\n",
       "           ...,\n",
       "           [1.989e-03, 1.083e-03, 1.598e-03,  ..., -1.486e-03, -1.183e-03, -1.319e-03],\n",
       "           [-1.790e-03, 1.417e-03, -1.346e-03,  ..., 1.814e-03, 1.348e-03, -1.143e-03],\n",
       "           [-1.982e-03, 1.308e-03, -1.666e-03,  ..., 1.682e-03, 1.239e-03, -1.479e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.21.mlp.gate_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-6.424e-03, -1.669e-03, 1.546e-03,  ..., -1.206e-03, -1.027e-03, 1.146e-03],\n",
       "           [5.451e-03, 2.741e-03, -1.835e-03,  ..., 9.580e-04, 1.088e-03, 8.249e-04],\n",
       "           [-3.052e-03, -1.730e-03, -9.561e-04,  ..., -5.698e-04, -6.890e-04, -6.456e-04],\n",
       "           ...,\n",
       "           [1.699e-03, -1.429e-03, 1.933e-03,  ..., 1.340e-03, 1.767e-03, 1.802e-03],\n",
       "           [-1.225e-03, -1.437e-03, -1.650e-03,  ..., -1.845e-03, -1.348e-03, 1.912e-03],\n",
       "           [1.446e-03, -1.803e-03, -1.499e-03,  ..., -1.470e-03, -1.621e-03, 1.646e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.21.mlp.up_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[6.176e-03, 1.794e-03, 1.360e-03,  ..., -1.166e-03, 1.337e-03, -1.146e-03],\n",
       "           [-4.375e-03, 2.375e-03, -2.430e-03,  ..., -1.576e-03, -1.099e-03, 1.211e-03],\n",
       "           [2.031e-03, -1.174e-03, -1.198e-03,  ..., 1.672e-03, 1.536e-03, 1.497e-03],\n",
       "           ...,\n",
       "           [1.389e-03, -1.472e-03, -1.574e-03,  ..., -1.608e-03, 1.995e-03, 1.720e-03],\n",
       "           [1.248e-03, 1.640e-03, -1.792e-03,  ..., -1.598e-03, -1.676e-03, -1.580e-03],\n",
       "           [-1.120e-03, -1.253e-03, 1.783e-03,  ..., -1.822e-03, -1.896e-03, -1.963e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.21.mlp.down_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-4.047e-02, 1.482e-02, 1.663e-02,  ..., 3.952e-03, -3.590e-03, -4.433e-03],\n",
       "           [-3.454e-03, -3.456e-03, 3.351e-03,  ..., -1.488e-03, 1.758e-03, -1.256e-03],\n",
       "           [3.326e-03, -5.337e-03, 1.799e-03,  ..., -1.431e-03, -1.452e-03, -1.231e-03],\n",
       "           ...,\n",
       "           [-9.665e-04, -1.454e-03, -1.316e-03,  ..., -1.287e-03, 1.670e-03, -1.364e-03],\n",
       "           [1.311e-03, -1.122e-03, -1.526e-03,  ..., 1.431e-03, 2.102e-03, -1.611e-03],\n",
       "           [-1.220e-03, 1.381e-03, 1.367e-03,  ..., 2.083e-03, -1.704e-03, 1.492e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.22.self_attn.q_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[1.593e-03, -1.667e-03, -1.518e-03,  ..., 2.140e-03, -1.724e-03, 1.628e-03],\n",
       "           [9.351e-04, -1.104e-03, -1.068e-03,  ..., -1.241e-03, 1.280e-03, 9.608e-04],\n",
       "           [7.119e-04, 1.010e-03, -1.090e-03,  ..., 1.225e-03, 1.114e-03, 1.234e-03],\n",
       "           ...,\n",
       "           [-1.429e-03, 2.647e-03, 2.012e-03,  ..., -3.016e-03, -2.439e-03, -2.264e-03],\n",
       "           [2.934e-03, 1.662e-03, -1.879e-03,  ..., -1.773e-03, 1.862e-03, 2.020e-03],\n",
       "           [2.693e-03, -1.732e-03, 1.587e-03,  ..., 2.140e-03, -2.035e-03, 2.298e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.22.self_attn.k_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[3.271e-03, 2.211e-03, 1.731e-03,  ..., 1.189e-03, -1.240e-03, -1.021e-03],\n",
       "           [-3.124e-03, 2.258e-03, -2.203e-03,  ..., -1.657e-03, 1.755e-03, -1.462e-03],\n",
       "           [2.234e-03, -2.163e-03, -2.426e-03,  ..., -2.251e-03, -2.241e-03, -2.289e-03],\n",
       "           ...,\n",
       "           [6.954e-03, -4.868e-03, 3.525e-03,  ..., -4.349e-03, 3.290e-03, 3.569e-03],\n",
       "           [-7.759e-03, 3.843e-03, -3.883e-03,  ..., 2.844e-03, -2.756e-03, -3.551e-03],\n",
       "           [-7.706e-03, -3.914e-03, 4.181e-03,  ..., -2.954e-03, 2.512e-03, -3.214e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.22.self_attn.v_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-7.110e-04, -1.145e-03, -1.210e-03,  ..., -1.863e-03, -1.529e-03, 1.454e-03],\n",
       "           [9.646e-04, -1.036e-03, 1.276e-03,  ..., 1.161e-03, -1.410e-03, -1.473e-03],\n",
       "           [9.122e-04, -1.311e-03, -1.410e-03,  ..., -2.003e-03, -1.919e-03, 1.533e-03],\n",
       "           ...,\n",
       "           [1.529e-03, -1.615e-03, -1.493e-03,  ..., 1.929e-03, 1.525e-03, 1.752e-03],\n",
       "           [-1.220e-03, 1.387e-03, 1.349e-03,  ..., -1.857e-03, 1.716e-03, -2.066e-03],\n",
       "           [1.418e-03, 1.455e-03, 1.892e-03,  ..., 1.939e-03, -1.986e-03, 2.111e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.22.self_attn.o_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-8.743e-03, -6.050e-03, 5.360e-03,  ..., 4.696e-03, -5.642e-03, -1.924e-02],\n",
       "           [-1.219e-03, -1.245e-03, -1.745e-03,  ..., 9.551e-04, 1.380e-03, 2.483e-03],\n",
       "           [1.051e-03, 1.126e-03, -1.113e-03,  ..., -1.007e-03, 1.810e-03, 1.601e-03],\n",
       "           ...,\n",
       "           [9.732e-04, -1.396e-03, -1.622e-03,  ..., -1.884e-03, 1.760e-03, -1.657e-03],\n",
       "           [1.215e-03, -1.556e-03, -1.924e-03,  ..., 2.447e-03, 1.611e-03, -1.918e-03],\n",
       "           [-1.233e-03, 1.469e-03, 1.618e-03,  ..., 1.720e-03, 2.104e-03, 1.540e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.22.mlp.gate_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[3.401e-03, -2.867e-03, -1.362e-03,  ..., -8.802e-04, 8.554e-04, 8.683e-04],\n",
       "           [3.555e-03, 2.718e-03, -1.863e-03,  ..., 9.947e-04, 6.380e-04, -6.475e-04],\n",
       "           [-2.853e-03, -1.791e-03, -1.750e-03,  ..., -2.859e-03, 1.691e-03, 2.234e-03],\n",
       "           ...,\n",
       "           [-1.458e-03, 1.640e-03, 1.540e-03,  ..., 1.337e-03, -1.614e-03, 1.668e-03],\n",
       "           [1.617e-03, -1.878e-03, 1.735e-03,  ..., 1.428e-03, 1.611e-03, 1.721e-03],\n",
       "           [-1.783e-03, 1.410e-03, 1.555e-03,  ..., 1.303e-03, -1.482e-03, -1.619e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.22.mlp.up_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-5.138e-03, -1.141e-03, -9.718e-04,  ..., 1.403e-03, 1.500e-03, -1.466e-03],\n",
       "           [5.463e-03, -4.215e-03, 3.683e-03,  ..., 2.106e-03, -2.172e-03, -2.058e-03],\n",
       "           [1.703e-03, -1.489e-03, 1.677e-03,  ..., -1.579e-03, -1.640e-03, 1.671e-03],\n",
       "           ...,\n",
       "           [-1.426e-03, -1.923e-03, -1.586e-03,  ..., -1.367e-03, 1.637e-03, -1.841e-03],\n",
       "           [1.400e-03, -1.483e-03, 1.689e-03,  ..., -2.089e-03, -2.241e-03, 1.431e-03],\n",
       "           [1.243e-03, -1.471e-03, 1.822e-03,  ..., -1.535e-03, -1.885e-03, 1.480e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.22.mlp.down_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[3.763e-02, -2.184e-02, 1.549e-02,  ..., 4.631e-03, 3.445e-03, -5.089e-03],\n",
       "           [-4.318e-03, 3.328e-03, 1.560e-03,  ..., -1.500e-03, -1.735e-03, -1.116e-03],\n",
       "           [-2.691e-03, -2.699e-03, 2.096e-03,  ..., 1.264e-03, -1.063e-03, 1.315e-03],\n",
       "           ...,\n",
       "           [-1.476e-03, -1.403e-03, 1.226e-03,  ..., -1.850e-03, -1.409e-03, 1.842e-03],\n",
       "           [1.205e-03, -1.467e-03, 1.592e-03,  ..., 1.547e-03, 1.235e-03, 1.431e-03],\n",
       "           [-1.126e-03, -1.361e-03, -1.421e-03,  ..., 1.771e-03, -1.502e-03, 1.601e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.23.self_attn.q_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-1.019e-03, -1.315e-03, -1.459e-03,  ..., 1.526e-03, 1.485e-03, 1.594e-03],\n",
       "           [9.422e-04, 1.064e-03, 9.108e-04,  ..., -1.666e-03, 1.564e-03, -1.632e-03],\n",
       "           [-1.018e-03, -8.707e-04, 9.832e-04,  ..., 1.628e-03, 1.536e-03, 1.538e-03],\n",
       "           ...,\n",
       "           [-2.745e-03, 3.096e-03, -3.256e-03,  ..., 3.290e-03, 2.888e-03, -2.758e-03],\n",
       "           [-3.910e-03, 3.174e-03, 3.244e-03,  ..., -2.369e-03, -2.926e-03, 3.458e-03],\n",
       "           [-2.871e-03, -2.914e-03, 2.954e-03,  ..., -3.633e-03, 4.143e-03, 4.555e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.23.self_attn.k_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-2.897e-03, -2.043e-03, -1.927e-03,  ..., 1.942e-03, -2.014e-03, -2.340e-03],\n",
       "           [2.468e-03, -1.828e-03, 2.048e-03,  ..., -1.416e-03, 1.822e-03, -1.533e-03],\n",
       "           [-2.127e-03, -1.926e-03, -1.656e-03,  ..., 1.691e-03, -1.583e-03, 1.846e-03],\n",
       "           ...,\n",
       "           [5.901e-03, -4.585e-03, -4.524e-03,  ..., -3.647e-03, -4.189e-03, 3.698e-03],\n",
       "           [-7.416e-03, -5.257e-03, 3.504e-03,  ..., 3.784e-03, -3.256e-03, -2.914e-03],\n",
       "           [4.147e-03, -5.669e-03, 3.952e-03,  ..., -4.181e-03, 3.231e-03, -3.122e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.23.self_attn.v_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-7.348e-04, 1.101e-03, 1.216e-03,  ..., -1.738e-03, 1.632e-03, 1.600e-03],\n",
       "           [8.912e-04, 1.297e-03, -1.223e-03,  ..., -2.026e-03, 1.440e-03, 1.685e-03],\n",
       "           [1.148e-03, 1.287e-03, 1.551e-03,  ..., -1.480e-03, 1.605e-03, 1.363e-03],\n",
       "           ...,\n",
       "           [1.280e-03, 1.758e-03, 2.636e-03,  ..., 1.937e-03, 1.615e-03, -1.544e-03],\n",
       "           [9.775e-04, 1.799e-03, 2.321e-03,  ..., -2.222e-03, -1.281e-03, -1.415e-03],\n",
       "           [1.116e-03, 1.749e-03, -1.439e-03,  ..., 1.465e-03, 1.718e-03, -2.008e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.23.self_attn.o_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-6.393e-03, 5.836e-03, -8.980e-03,  ..., -8.362e-03, -6.504e-03, 7.874e-03],\n",
       "           [-2.279e-03, -2.136e-03, -1.590e-03,  ..., 1.760e-03, 1.218e-03, -2.028e-03],\n",
       "           [-1.816e-03, 1.616e-03, 2.115e-03,  ..., 1.911e-03, 1.500e-03, 1.645e-03],\n",
       "           ...,\n",
       "           [1.560e-03, -1.220e-03, -1.462e-03,  ..., 1.657e-03, 2.058e-03, 1.420e-03],\n",
       "           [-1.354e-03, -1.047e-03, -1.569e-03,  ..., -1.917e-03, 1.618e-03, -1.300e-03],\n",
       "           [-1.934e-03, -1.201e-03, -1.418e-03,  ..., -1.668e-03, 1.284e-03, -1.207e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.23.mlp.gate_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[1.350e-02, -2.666e-03, -1.778e-03,  ..., 8.540e-04, -1.003e-03, -1.053e-03],\n",
       "           [-3.242e-03, -1.661e-03, 1.135e-03,  ..., 8.159e-04, -8.545e-04, -7.186e-04],\n",
       "           [-3.387e-03, 1.864e-03, 1.909e-03,  ..., -1.656e-03, -1.731e-03, 1.959e-03],\n",
       "           ...,\n",
       "           [-1.446e-03, 1.426e-03, 1.662e-03,  ..., -1.713e-03, -1.802e-03, -1.641e-03],\n",
       "           [-1.467e-03, 1.601e-03, 1.492e-03,  ..., 1.739e-03, -1.646e-03, -1.772e-03],\n",
       "           [1.638e-03, -1.492e-03, -1.601e-03,  ..., 1.277e-03, 1.753e-03, -1.554e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.23.mlp.up_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-6.424e-03, -1.681e-03, 1.453e-03,  ..., 1.160e-03, -1.366e-03, -1.270e-03],\n",
       "           [-1.377e-03, -1.150e-03, -1.245e-03,  ..., -1.325e-03, -1.152e-03, 1.271e-03],\n",
       "           [-1.269e-03, 1.660e-03, -1.502e-03,  ..., -1.637e-03, 2.045e-03, -1.738e-03],\n",
       "           ...,\n",
       "           [1.294e-03, 1.176e-03, 1.665e-03,  ..., -1.795e-03, 1.468e-03, -1.787e-03],\n",
       "           [1.167e-03, -1.497e-03, 1.765e-03,  ..., 1.777e-03, 1.598e-03, -1.508e-03],\n",
       "           [1.194e-03, -1.628e-03, -2.184e-03,  ..., -1.744e-03, 1.365e-03, -1.506e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.23.mlp.down_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[4.266e-02, -7.942e-03, -7.572e-03,  ..., -3.687e-03, -3.002e-03, 3.027e-03],\n",
       "           [-3.922e-03, -1.349e-03, -4.784e-03,  ..., 1.407e-03, 1.486e-03, -1.245e-03],\n",
       "           [3.006e-03, 1.836e-03, 2.512e-03,  ..., -1.002e-03, 1.338e-03, -1.117e-03],\n",
       "           ...,\n",
       "           [-1.176e-03, -1.482e-03, -1.762e-03,  ..., 1.816e-03, -1.470e-03, 1.490e-03],\n",
       "           [-1.247e-03, 1.650e-03, -1.436e-03,  ..., 1.650e-03, -1.373e-03, 1.643e-03],\n",
       "           [-1.181e-03, -1.694e-03, -1.636e-03,  ..., 2.123e-03, 1.493e-03, 1.540e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.24.self_attn.q_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[1.029e-03, -1.157e-03, 9.804e-04,  ..., -1.302e-03, 1.376e-03, -1.015e-03],\n",
       "           [1.036e-03, 1.076e-03, -1.543e-03,  ..., 1.572e-03, -1.399e-03, -1.811e-03],\n",
       "           [-1.047e-03, -1.413e-03, 1.409e-03,  ..., -1.273e-03, -1.121e-03, 1.042e-03],\n",
       "           ...,\n",
       "           [-1.993e-03, -2.243e-03, -2.016e-03,  ..., 3.178e-03, -2.117e-03, 2.390e-03],\n",
       "           [-1.443e-03, -1.899e-03, 2.253e-03,  ..., -2.501e-03, -2.840e-03, 1.856e-03],\n",
       "           [2.594e-03, 1.700e-03, 1.749e-03,  ..., -2.213e-03, 2.132e-03, 2.100e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.24.self_attn.k_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-2.804e-03, 2.203e-03, 1.932e-03,  ..., -1.691e-03, 1.550e-03, 1.381e-03],\n",
       "           [-1.838e-03, 2.455e-03, -1.805e-03,  ..., 2.283e-03, -1.845e-03, -2.172e-03],\n",
       "           [1.930e-03, 1.870e-03, 1.842e-03,  ..., 2.075e-03, 1.616e-03, 1.644e-03],\n",
       "           ...,\n",
       "           [-2.857e-03, -4.295e-03, -5.043e-03,  ..., 3.672e-03, 2.918e-03, -3.353e-03],\n",
       "           [-3.586e-03, -3.740e-03, 4.360e-03,  ..., 3.275e-03, -3.668e-03, -2.832e-03],\n",
       "           [3.389e-03, -4.452e-03, -4.082e-03,  ..., 3.092e-03, 3.847e-03, -3.538e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.24.self_attn.v_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-9.212e-04, 1.057e-03, 1.306e-03,  ..., 1.470e-03, -2.243e-03, -2.197e-03],\n",
       "           [1.207e-03, 1.197e-03, 1.965e-03,  ..., 1.760e-03, 1.748e-03, -1.863e-03],\n",
       "           [9.456e-04, -1.171e-03, 1.647e-03,  ..., -1.526e-03, 1.634e-03, 1.886e-03],\n",
       "           ...,\n",
       "           [2.041e-03, 1.651e-03, 2.024e-03,  ..., 1.698e-03, 1.801e-03, 1.571e-03],\n",
       "           [1.140e-03, 1.581e-03, 1.543e-03,  ..., 2.073e-03, 2.111e-03, 1.758e-03],\n",
       "           [-1.567e-03, 1.747e-03, -1.510e-03,  ..., -2.281e-03, -1.778e-03, -1.644e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.24.self_attn.o_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-5.363e-03, -3.675e-03, -4.772e-03,  ..., -1.257e-02, -3.177e-02, -2.492e-02],\n",
       "           [1.469e-03, -1.495e-03, 1.922e-03,  ..., 2.020e-03, 2.056e-03, 1.663e-03],\n",
       "           [1.993e-03, -1.543e-03, 1.269e-03,  ..., -1.499e-03, -4.528e-03, 1.653e-03],\n",
       "           ...,\n",
       "           [-1.461e-03, 1.550e-03, -1.596e-03,  ..., 1.898e-03, -1.680e-03, 1.935e-03],\n",
       "           [1.654e-03, -1.725e-03, -1.525e-03,  ..., 1.663e-03, -1.868e-03, -1.560e-03],\n",
       "           [-1.422e-03, -1.822e-03, 1.491e-03,  ..., -2.207e-03, 1.222e-03, -1.423e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.24.mlp.gate_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[4.829e-03, 1.934e-03, -2.104e-03,  ..., 1.203e-03, 9.785e-04, -1.104e-03],\n",
       "           [5.074e-03, -6.210e-03, -9.415e-03,  ..., 1.905e-03, 1.520e-03, 1.854e-03],\n",
       "           [-3.716e-03, 4.189e-03, -6.798e-03,  ..., 2.031e-03, -1.959e-03, 2.201e-03],\n",
       "           ...,\n",
       "           [-1.691e-03, -1.599e-03, 1.627e-03,  ..., -1.793e-03, 1.714e-03, -1.821e-03],\n",
       "           [1.688e-03, 1.761e-03, -1.444e-03,  ..., -1.388e-03, 1.575e-03, 1.524e-03],\n",
       "           [-1.708e-03, -1.449e-03, 1.809e-03,  ..., 1.616e-03, 1.591e-03, 1.631e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.24.mlp.up_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[5.764e-03, 1.445e-03, 1.290e-03,  ..., -1.067e-03, 1.516e-03, 1.101e-03],\n",
       "           [2.834e-03, 2.230e-03, -2.338e-03,  ..., 5.865e-04, -9.279e-04, -6.499e-04],\n",
       "           [2.699e-03, -2.251e-03, -1.926e-03,  ..., -7.081e-04, -6.638e-04, 8.073e-04],\n",
       "           ...,\n",
       "           [1.650e-03, -1.291e-03, -1.687e-03,  ..., 1.464e-03, -1.889e-03, 1.634e-03],\n",
       "           [1.262e-03, -1.444e-03, 1.647e-03,  ..., -1.682e-03, 2.035e-03, 1.666e-03],\n",
       "           [1.804e-03, 1.534e-03, 1.457e-03,  ..., 2.010e-03, 1.898e-03, -1.554e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.24.mlp.down_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-3.583e-02, -1.852e-02, 1.048e-02,  ..., 2.428e-03, -2.331e-03, 2.525e-03],\n",
       "           [3.296e-03, 4.436e-03, -3.155e-03,  ..., 1.319e-03, -1.254e-03, 1.545e-03],\n",
       "           [3.853e-03, 1.720e-03, 3.035e-03,  ..., -1.217e-03, 1.431e-03, -1.113e-03],\n",
       "           ...,\n",
       "           [-1.111e-03, 1.324e-03, -1.641e-03,  ..., -1.516e-03, 1.486e-03, 2.571e-03],\n",
       "           [1.173e-03, 1.298e-03, -1.137e-03,  ..., 1.585e-03, -1.667e-03, 1.586e-03],\n",
       "           [1.190e-03, -1.232e-03, 1.166e-03,  ..., -1.719e-03, -1.432e-03, -1.283e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.25.self_attn.q_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[1.903e-03, -1.726e-03, -1.632e-03,  ..., 1.714e-03, 2.062e-03, 1.716e-03],\n",
       "           [-1.252e-03, -1.541e-03, -2.050e-03,  ..., -2.382e-03, -2.529e-03, 2.310e-03],\n",
       "           [-1.404e-03, 2.054e-03, -1.827e-03,  ..., -2.056e-03, -2.232e-03, 1.874e-03],\n",
       "           ...,\n",
       "           [1.142e-03, -1.464e-03, -1.208e-03,  ..., -1.195e-03, 1.393e-03, -1.121e-03],\n",
       "           [-8.035e-04, -1.392e-03, 9.851e-04,  ..., 1.390e-03, 1.566e-03, 1.403e-03],\n",
       "           [-9.680e-04, -9.151e-04, 8.674e-04,  ..., -1.441e-03, 1.362e-03, 1.328e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.25.self_attn.k_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-4.478e-03, 4.093e-03, -2.701e-03,  ..., 1.934e-03, 2.459e-03, -3.067e-03],\n",
       "           [-8.293e-03, 3.096e-03, 3.325e-03,  ..., 3.065e-03, -2.586e-03, -2.703e-03],\n",
       "           [-7.637e-03, -3.815e-03, -2.930e-03,  ..., -2.882e-03, 3.263e-03, -2.354e-03],\n",
       "           ...,\n",
       "           [6.626e-03, -4.795e-03, 5.428e-03,  ..., 2.769e-03, 3.584e-03, 2.422e-03],\n",
       "           [-7.576e-03, -4.520e-03, 5.146e-03,  ..., -2.628e-03, -3.763e-03, -2.750e-03],\n",
       "           [-7.648e-03, -5.222e-03, -4.807e-03,  ..., -2.457e-03, -3.122e-03, 2.411e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.25.self_attn.v_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-1.447e-03, 1.364e-03, 1.697e-03,  ..., -2.129e-03, 1.712e-03, 1.685e-03],\n",
       "           [-1.193e-03, -1.624e-03, 1.791e-03,  ..., 1.339e-03, -1.574e-03, 2.026e-03],\n",
       "           [8.426e-04, -1.307e-03, -2.020e-03,  ..., -1.656e-03, 1.554e-03, -1.708e-03],\n",
       "           ...,\n",
       "           [1.196e-03, 1.856e-03, -2.235e-03,  ..., -9.251e-04, 9.379e-04, -9.842e-04],\n",
       "           [-1.481e-03, -1.677e-03, -2.588e-03,  ..., 1.328e-03, 9.880e-04, -1.235e-03],\n",
       "           [1.341e-03, -2.014e-03, -1.957e-03,  ..., -1.245e-03, 1.046e-03, -1.077e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.25.self_attn.o_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-7.141e-03, -1.124e-02, -9.041e-03,  ..., -4.623e-03, 3.685e-03, -6.248e-03],\n",
       "           [1.696e-03, -1.312e-03, 3.464e-03,  ..., -1.921e-03, 2.689e-03, -1.727e-03],\n",
       "           [1.688e-03, 1.755e-03, 1.487e-03,  ..., -1.562e-03, -1.661e-03, -2.407e-03],\n",
       "           ...,\n",
       "           [1.734e-03, 1.591e-03, 1.777e-03,  ..., -1.121e-03, -1.175e-03, -1.412e-03],\n",
       "           [1.659e-03, -1.272e-03, -1.586e-03,  ..., 1.405e-03, -1.119e-03, 1.317e-03],\n",
       "           [-1.351e-03, 1.376e-03, -1.516e-03,  ..., 1.380e-03, 1.123e-03, 1.442e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.25.mlp.gate_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[4.402e-03, -2.417e-03, 1.523e-03,  ..., -1.200e-03, 8.383e-04, -1.021e-03],\n",
       "           [-3.809e-03, -2.090e-03, 1.210e-03,  ..., 5.598e-04, -6.785e-04, 5.198e-04],\n",
       "           [-4.059e-03, 3.675e-03, -3.998e-03,  ..., -5.989e-04, -4.978e-04, -6.390e-04],\n",
       "           ...,\n",
       "           [1.770e-03, 1.779e-03, -2.003e-03,  ..., 1.866e-03, -1.832e-03, -1.873e-03],\n",
       "           [1.482e-03, -1.870e-03, 1.338e-03,  ..., -1.663e-03, 2.039e-03, -1.694e-03],\n",
       "           [1.339e-03, 1.553e-03, -1.516e-03,  ..., -1.730e-03, -1.863e-03, 1.765e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.25.mlp.up_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-5.798e-03, -1.822e-03, -2.254e-03,  ..., -9.255e-04, 9.108e-04, 9.646e-04],\n",
       "           [1.893e-03, 1.370e-03, -1.781e-03,  ..., 9.241e-04, -9.303e-04, 1.127e-03],\n",
       "           [4.509e-03, -3.424e-03, -2.333e-03,  ..., -9.656e-04, 1.195e-03, 1.149e-03],\n",
       "           ...,\n",
       "           [-1.465e-03, -1.508e-03, 1.609e-03,  ..., -1.340e-03, -1.513e-03, -1.739e-03],\n",
       "           [1.235e-03, 1.652e-03, 1.856e-03,  ..., 1.903e-03, -1.467e-03, -1.424e-03],\n",
       "           [1.295e-03, -1.290e-03, 1.354e-03,  ..., -1.454e-03, -1.459e-03, -1.946e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.25.mlp.down_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[3.934e-02, -1.556e-02, -8.934e-03,  ..., 2.420e-03, -2.296e-03, -3.195e-03],\n",
       "           [-4.200e-03, -5.787e-03, 4.719e-03,  ..., -1.313e-03, -1.134e-03, 1.389e-03],\n",
       "           [-2.777e-03, -2.876e-03, 2.024e-03,  ..., -1.861e-03, -1.885e-03, -1.451e-03],\n",
       "           ...,\n",
       "           [1.244e-03, -1.569e-03, 1.278e-03,  ..., -1.442e-03, 1.719e-03, -1.948e-03],\n",
       "           [1.197e-03, 1.480e-03, 1.306e-03,  ..., 1.682e-03, 1.690e-03, -2.119e-03],\n",
       "           [-1.104e-03, -1.620e-03, 1.388e-03,  ..., -1.776e-03, 1.833e-03, 1.905e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.26.self_attn.q_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[1.124e-03, 1.334e-03, 1.493e-03,  ..., -1.829e-03, 1.816e-03, -2.274e-03],\n",
       "           [-1.275e-03, -1.621e-03, 1.540e-03,  ..., -1.972e-03, -1.844e-03, -1.877e-03],\n",
       "           [-1.522e-03, 1.282e-03, -1.567e-03,  ..., -2.226e-03, -2.340e-03, 2.279e-03],\n",
       "           ...,\n",
       "           [1.851e-03, 1.669e-03, 1.572e-03,  ..., 2.007e-03, -2.043e-03, -2.281e-03],\n",
       "           [-2.262e-03, -2.140e-03, -1.824e-03,  ..., 1.830e-03, -2.033e-03, 2.012e-03],\n",
       "           [1.747e-03, 1.718e-03, 1.997e-03,  ..., 1.566e-03, -2.146e-03, 2.625e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.26.self_attn.k_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[1.849e-03, -1.552e-03, -1.997e-03,  ..., -2.089e-03, -2.617e-03, 1.818e-03],\n",
       "           [2.132e-03, 2.052e-03, -2.043e-03,  ..., -1.848e-03, 2.150e-03, -1.501e-03],\n",
       "           [1.716e-03, 2.653e-03, 2.363e-03,  ..., 2.106e-03, 2.363e-03, -1.743e-03],\n",
       "           ...,\n",
       "           [4.131e-03, 4.604e-03, 3.334e-03,  ..., 2.907e-03, -3.534e-03, 5.039e-03],\n",
       "           [-4.559e-03, 3.687e-03, -3.479e-03,  ..., 2.394e-03, -3.393e-03, -2.647e-03],\n",
       "           [5.089e-03, 4.074e-03, 4.318e-03,  ..., -3.426e-03, -3.576e-03, 3.069e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.26.self_attn.v_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-1.045e-03, -1.610e-03, 1.982e-03,  ..., 2.087e-03, -1.867e-03, -2.556e-03],\n",
       "           [-1.060e-03, -1.222e-03, -1.755e-03,  ..., -2.022e-03, -2.296e-03, 3.002e-03],\n",
       "           [-1.132e-03, -1.511e-03, -1.916e-03,  ..., -2.024e-03, -2.262e-03, 1.991e-03],\n",
       "           ...,\n",
       "           [-1.025e-03, -1.366e-03, -1.402e-03,  ..., -2.129e-03, -1.936e-03, 1.787e-03],\n",
       "           [-9.871e-04, 1.262e-03, 1.301e-03,  ..., -2.182e-03, 2.134e-03, 2.251e-03],\n",
       "           [1.009e-03, 1.539e-03, -1.472e-03,  ..., -1.863e-03, 1.869e-03, 1.744e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.26.self_attn.o_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[5.432e-03, 5.981e-03, 5.291e-03,  ..., 6.710e-03, -1.317e-02, -1.327e-02],\n",
       "           [-2.951e-03, -1.744e-03, 1.945e-03,  ..., 1.617e-03, 3.204e-03, -1.916e-03],\n",
       "           [-1.700e-03, 1.879e-03, -1.636e-03,  ..., 9.775e-04, -2.127e-03, 2.237e-03],\n",
       "           ...,\n",
       "           [1.413e-03, -1.869e-03, -1.413e-03,  ..., -1.689e-03, 2.197e-03, -1.413e-03],\n",
       "           [1.335e-03, 1.624e-03, 1.891e-03,  ..., 1.431e-03, 1.717e-03, 1.937e-03],\n",
       "           [-1.768e-03, -1.705e-03, -1.955e-03,  ..., -1.410e-03, -1.571e-03, 1.785e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.26.mlp.gate_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[7.252e-03, -1.349e-03, -1.301e-03,  ..., -7.439e-04, 1.063e-03, -8.531e-04],\n",
       "           [4.906e-03, 2.272e-03, 2.304e-03,  ..., -8.016e-04, 7.634e-04, -6.623e-04],\n",
       "           [3.822e-03, -3.893e-03, 2.874e-03,  ..., -1.357e-03, -1.822e-03, 1.769e-03],\n",
       "           ...,\n",
       "           [-1.850e-03, -1.688e-03, -2.285e-03,  ..., 1.852e-03, -1.688e-03, 1.622e-03],\n",
       "           [-1.923e-03, 1.569e-03, 1.692e-03,  ..., -2.024e-03, 1.602e-03, 1.715e-03],\n",
       "           [-1.672e-03, 1.356e-03, -1.550e-03,  ..., 1.581e-03, -1.852e-03, -2.010e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.26.mlp.up_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[2.424e-03, -1.587e-03, 1.909e-03,  ..., -9.379e-04, -1.245e-03, -1.079e-03],\n",
       "           [-4.536e-03, 2.048e-03, 1.801e-03,  ..., 9.718e-04, 9.995e-04, 1.135e-03],\n",
       "           [-1.857e-03, 1.993e-03, 1.361e-03,  ..., 1.011e-03, -1.313e-03, -1.352e-03],\n",
       "           ...,\n",
       "           [-1.245e-03, -1.965e-03, -1.787e-03,  ..., 1.677e-03, -1.651e-03, 1.461e-03],\n",
       "           [1.170e-03, -1.476e-03, -1.568e-03,  ..., -1.548e-03, -1.438e-03, 1.611e-03],\n",
       "           [-1.486e-03, 1.616e-03, -1.399e-03,  ..., -2.098e-03, 1.766e-03, 1.433e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.26.mlp.down_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[3.564e-02, -1.254e-02, 1.276e-02,  ..., -2.449e-03, 5.074e-03, -2.264e-03],\n",
       "           [-9.399e-03, 3.376e-03, -2.907e-03,  ..., -1.468e-03, -1.023e-03, -1.451e-03],\n",
       "           [-5.741e-03, 2.312e-03, -1.488e-03,  ..., -1.350e-03, -1.472e-03, -1.197e-03],\n",
       "           ...,\n",
       "           [-1.094e-03, -1.163e-03, 1.325e-03,  ..., -1.666e-03, -1.641e-03, 1.637e-03],\n",
       "           [1.084e-03, -1.386e-03, -1.307e-03,  ..., 1.976e-03, -1.640e-03, -2.018e-03],\n",
       "           [-9.336e-04, 1.325e-03, 1.502e-03,  ..., 1.466e-03, 1.533e-03, 1.880e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.27.self_attn.q_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-1.827e-03, -2.466e-03, 1.928e-03,  ..., -2.842e-03, -2.531e-03, 1.723e-03],\n",
       "           [1.830e-03, -2.007e-03, -2.192e-03,  ..., 2.153e-03, -2.453e-03, 2.104e-03],\n",
       "           [2.337e-03, 1.925e-03, 1.856e-03,  ..., 2.390e-03, -2.069e-03, 2.371e-03],\n",
       "           ...,\n",
       "           [1.765e-03, 1.687e-03, 1.898e-03,  ..., 2.197e-03, 2.644e-03, -2.487e-03],\n",
       "           [-2.609e-03, 1.902e-03, -2.167e-03,  ..., 2.323e-03, 2.474e-03, 2.270e-03],\n",
       "           [1.472e-03, -2.018e-03, -2.192e-03,  ..., 2.571e-03, 2.447e-03, 2.649e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.27.self_attn.k_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-2.720e-03, 2.272e-03, -2.901e-03,  ..., 2.068e-03, 2.909e-03, 2.602e-03],\n",
       "           [-2.827e-03, -2.625e-03, -2.333e-03,  ..., 2.186e-03, 2.028e-03, 2.342e-03],\n",
       "           [3.138e-03, 2.686e-03, -2.325e-03,  ..., 2.121e-03, 2.134e-03, 2.943e-03],\n",
       "           ...,\n",
       "           [-3.056e-03, 3.147e-03, 5.955e-03,  ..., 3.723e-03, -3.012e-03, -3.677e-03],\n",
       "           [-3.326e-03, 3.269e-03, -5.695e-03,  ..., 3.525e-03, -3.876e-03, 3.571e-03],\n",
       "           [-3.717e-03, 3.468e-03, -3.490e-03,  ..., -3.857e-03, 4.086e-03, 3.197e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.27.self_attn.v_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[1.191e-03, -1.479e-03, 1.520e-03,  ..., 1.620e-03, 1.388e-03, -1.229e-03],\n",
       "           [1.360e-03, 1.471e-03, 1.468e-03,  ..., -1.452e-03, 1.334e-03, 1.191e-03],\n",
       "           [1.561e-03, -1.567e-03, 1.626e-03,  ..., -1.663e-03, 1.079e-03, -1.372e-03],\n",
       "           ...,\n",
       "           [-1.167e-03, -1.280e-03, -1.446e-03,  ..., -2.506e-03, -2.422e-03, 1.767e-03],\n",
       "           [-1.605e-03, -1.719e-03, -1.621e-03,  ..., -2.636e-03, -1.748e-03, 1.878e-03],\n",
       "           [-1.292e-03, 1.577e-03, -2.005e-03,  ..., -1.864e-03, -2.382e-03, -1.938e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.27.self_attn.o_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[2.405e-02, 3.214e-02, -1.611e-02,  ..., 1.340e-02, 4.364e-02, 1.900e-02],\n",
       "           [6.844e-03, 1.789e-03, 2.964e-03,  ..., -2.377e-03, -4.242e-03, -1.897e-03],\n",
       "           [1.394e-03, -1.297e-03, 2.035e-03,  ..., -1.579e-03, 1.266e-03, -1.455e-03],\n",
       "           ...,\n",
       "           [-1.544e-03, -1.284e-03, -1.514e-03,  ..., -2.062e-03, 1.743e-03, -1.704e-03],\n",
       "           [-1.347e-03, 1.836e-03, 1.787e-03,  ..., 1.942e-03, -1.652e-03, -1.742e-03],\n",
       "           [-1.974e-03, 1.501e-03, 2.220e-03,  ..., 1.641e-03, 1.963e-03, -1.863e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.27.mlp.gate_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[5.939e-03, 2.735e-03, 2.577e-03,  ..., -9.680e-04, 1.089e-03, -9.937e-04],\n",
       "           [1.035e-02, -1.879e-03, 1.332e-03,  ..., 8.278e-04, 1.112e-03, -8.249e-04],\n",
       "           [4.940e-03, 1.434e-02, -9.918e-03,  ..., 1.164e-03, -2.008e-03, 1.415e-03],\n",
       "           ...,\n",
       "           [-1.530e-03, -1.214e-03, 1.455e-03,  ..., -2.029e-03, 1.961e-03, 1.653e-03],\n",
       "           [-1.999e-03, -1.406e-03, -1.687e-03,  ..., 1.935e-03, 1.641e-03, -1.744e-03],\n",
       "           [-1.399e-03, -1.578e-03, -1.793e-03,  ..., 1.733e-03, -1.828e-03, -1.403e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.27.mlp.up_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[3.878e-03, -2.068e-03, -1.824e-03,  ..., -9.308e-04, -1.060e-03, -1.215e-03],\n",
       "           [5.287e-03, 2.161e-03, 1.513e-03,  ..., -1.244e-03, 1.048e-03, -1.054e-03],\n",
       "           [2.306e-03, 4.536e-03, 2.752e-03,  ..., 6.671e-04, 5.531e-04, -5.646e-04],\n",
       "           ...,\n",
       "           [1.741e-03, 1.377e-03, -1.655e-03,  ..., -1.658e-03, 1.593e-03, 1.757e-03],\n",
       "           [-1.274e-03, 1.502e-03, 1.799e-03,  ..., 1.856e-03, 1.638e-03, -1.313e-03],\n",
       "           [1.145e-03, -1.264e-03, 1.292e-03,  ..., -2.264e-03, 1.461e-03, 1.916e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.27.mlp.down_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-3.186e-02, -6.989e-03, -5.924e-03,  ..., -3.162e-03, -2.018e-03, 2.909e-03],\n",
       "           [4.658e-03, 5.497e-03, -2.295e-03,  ..., -1.387e-03, -1.616e-03, -1.264e-03],\n",
       "           [3.050e-03, 2.287e-03, 2.220e-03,  ..., 1.375e-03, 1.284e-03, 1.364e-03],\n",
       "           ...,\n",
       "           [-1.612e-03, -1.053e-03, -1.533e-03,  ..., 2.012e-03, 1.471e-03, -1.467e-03],\n",
       "           [-1.237e-03, 1.562e-03, -1.503e-03,  ..., -1.378e-03, -2.037e-03, 1.719e-03],\n",
       "           [9.265e-04, -1.715e-03, 1.309e-03,  ..., 2.602e-03, 1.874e-03, 1.834e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.28.self_attn.q_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[2.531e-03, 2.028e-03, 2.113e-03,  ..., 2.373e-03, -2.230e-03, -1.616e-03],\n",
       "           [-1.545e-03, -1.920e-03, -1.705e-03,  ..., 2.522e-03, 2.245e-03, 1.858e-03],\n",
       "           [-2.161e-03, -1.493e-03, 1.605e-03,  ..., -2.644e-03, -1.924e-03, -1.855e-03],\n",
       "           ...,\n",
       "           [3.925e-03, 3.017e-03, 2.220e-03,  ..., -2.220e-03, -2.417e-03, -1.687e-03],\n",
       "           [-2.157e-03, 1.833e-03, 2.047e-03,  ..., 2.483e-03, 2.621e-03, -3.027e-03],\n",
       "           [1.921e-03, 2.165e-03, -2.407e-03,  ..., -2.598e-03, -2.371e-03, -2.577e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.28.self_attn.k_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[5.306e-03, -4.059e-03, 3.214e-03,  ..., 1.894e-03, 2.298e-03, 2.197e-03],\n",
       "           [2.352e-03, -2.167e-03, -2.161e-03,  ..., -2.483e-03, -2.598e-03, 2.472e-03],\n",
       "           [-2.275e-03, -2.192e-03, 2.087e-03,  ..., -3.071e-03, -2.022e-03, -2.129e-03],\n",
       "           ...,\n",
       "           [5.703e-03, 3.002e-03, 3.151e-03,  ..., -2.438e-03, -3.017e-03, 2.720e-03],\n",
       "           [2.850e-03, 2.728e-03, 2.804e-03,  ..., -3.670e-03, 3.145e-03, 3.349e-03],\n",
       "           [-3.765e-03, -4.169e-03, -2.792e-03,  ..., 3.345e-03, -3.559e-03, -3.313e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.28.self_attn.v_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-1.316e-03, -1.487e-03, 1.742e-03,  ..., -2.085e-03, 2.718e-03, -2.275e-03],\n",
       "           [-2.241e-03, 1.428e-03, 1.497e-03,  ..., 1.862e-03, 1.695e-03, 1.963e-03],\n",
       "           [1.381e-03, 1.770e-03, -1.270e-03,  ..., 1.953e-03, 2.081e-03, 2.413e-03],\n",
       "           ...,\n",
       "           [-1.013e-03, 1.452e-03, 1.450e-03,  ..., 1.722e-03, -1.594e-03, -1.595e-03],\n",
       "           [8.597e-04, -1.078e-03, 1.408e-03,  ..., -1.679e-03, 1.755e-03, 1.511e-03],\n",
       "           [-1.545e-03, -1.740e-03, 1.744e-03,  ..., 1.590e-03, 1.189e-03, 1.394e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.28.self_attn.o_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[6.920e-03, -4.692e-03, -1.328e-02,  ..., 8.369e-03, -3.561e-03, 4.707e-03],\n",
       "           [-2.249e-03, -1.842e-03, 1.980e-03,  ..., -1.641e-03, -1.610e-03, 1.584e-03],\n",
       "           [-1.534e-03, -1.837e-03, 1.619e-03,  ..., 1.217e-03, -2.106e-03, 1.760e-03],\n",
       "           ...,\n",
       "           [1.571e-03, 1.562e-03, -1.563e-03,  ..., -1.466e-03, 1.459e-03, 1.084e-03],\n",
       "           [1.491e-03, -2.235e-03, 1.666e-03,  ..., -1.881e-03, -1.207e-03, -1.177e-03],\n",
       "           [-1.842e-03, 1.908e-03, 1.732e-03,  ..., -1.299e-03, 1.609e-03, -1.526e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.28.mlp.gate_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[4.539e-03, 2.018e-03, 2.075e-03,  ..., -9.127e-04, -7.043e-04, -8.097e-04],\n",
       "           [3.269e-03, 1.750e-03, 1.514e-03,  ..., 8.545e-04, -7.114e-04, 6.199e-04],\n",
       "           [-3.090e-03, 2.008e-03, -1.433e-03,  ..., 1.022e-03, -1.105e-03, 7.644e-04],\n",
       "           ...,\n",
       "           [-1.900e-03, -1.511e-03, 1.805e-03,  ..., 1.684e-03, 2.033e-03, 1.995e-03],\n",
       "           [1.231e-03, 1.667e-03, 2.468e-03,  ..., -1.732e-03, 1.884e-03, -1.759e-03],\n",
       "           [1.399e-03, -1.978e-03, -1.900e-03,  ..., 1.731e-03, 2.134e-03, 2.069e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.28.mlp.up_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[5.806e-03, 1.541e-03, -1.945e-03,  ..., 7.691e-04, -1.186e-03, 7.305e-04],\n",
       "           [7.236e-03, -6.046e-03, -5.680e-03,  ..., -2.115e-03, -1.793e-03, 1.891e-03],\n",
       "           [3.231e-03, -1.945e-03, 1.515e-03,  ..., -1.221e-03, 1.193e-03, -1.698e-03],\n",
       "           ...,\n",
       "           [1.279e-03, -1.292e-03, 1.431e-03,  ..., 1.698e-03, -1.935e-03, 1.357e-03],\n",
       "           [1.348e-03, -1.580e-03, 1.352e-03,  ..., -2.060e-03, -1.385e-03, -1.647e-03],\n",
       "           [-1.057e-03, 1.352e-03, 1.720e-03,  ..., -1.740e-03, -1.699e-03, 1.642e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.28.mlp.down_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-4.816e-02, 7.607e-03, 8.354e-03,  ..., 2.285e-03, 2.344e-03, 2.890e-03],\n",
       "           [9.338e-03, 3.071e-03, -4.028e-03,  ..., 1.547e-03, -1.331e-03, -1.620e-03],\n",
       "           [-3.012e-03, 1.966e-03, 1.422e-03,  ..., -1.295e-03, -1.440e-03, -1.226e-03],\n",
       "           ...,\n",
       "           [-1.492e-03, 1.483e-03, -1.866e-03,  ..., 1.860e-03, -2.201e-03, 2.226e-03],\n",
       "           [-9.480e-04, -1.322e-03, 1.343e-03,  ..., -1.494e-03, 2.476e-03, -1.980e-03],\n",
       "           [-1.112e-03, 1.248e-03, 1.703e-03,  ..., 1.536e-03, 1.540e-03, -1.721e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.29.self_attn.q_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-1.431e-03, -1.725e-03, 1.591e-03,  ..., -2.468e-03, 1.572e-03, -1.955e-03],\n",
       "           [-1.611e-03, 1.753e-03, 1.596e-03,  ..., 2.548e-03, -2.436e-03, 2.254e-03],\n",
       "           [1.402e-03, 1.349e-03, -1.282e-03,  ..., -1.913e-03, 1.745e-03, -1.874e-03],\n",
       "           ...,\n",
       "           [-3.729e-03, 2.836e-03, -3.002e-03,  ..., -1.727e-03, -1.388e-03, 1.615e-03],\n",
       "           [-4.772e-03, 2.243e-03, 2.972e-03,  ..., -1.607e-03, 1.583e-03, 1.299e-03],\n",
       "           [4.269e-03, 3.351e-03, -2.506e-03,  ..., 2.235e-03, -1.625e-03, -2.113e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.29.self_attn.k_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-2.964e-03, -2.590e-03, 2.380e-03,  ..., -1.681e-03, -1.997e-03, -1.882e-03],\n",
       "           [-2.914e-03, 1.632e-03, -2.455e-03,  ..., 2.748e-03, 2.298e-03, -1.937e-03],\n",
       "           [2.102e-03, -2.092e-03, 2.222e-03,  ..., -2.171e-03, 2.348e-03, 2.396e-03],\n",
       "           ...,\n",
       "           [-7.450e-03, -5.409e-03, -3.887e-03,  ..., -1.358e-03, 1.311e-03, -1.513e-03],\n",
       "           [-8.553e-03, 4.585e-03, -3.883e-03,  ..., 1.398e-03, 2.005e-03, -1.584e-03],\n",
       "           [-8.072e-03, -4.841e-03, -3.483e-03,  ..., -1.190e-03, 1.425e-03, -1.375e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.29.self_attn.v_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-1.234e-03, 1.290e-03, 1.731e-03,  ..., -1.714e-03, -2.098e-03, 1.819e-03],\n",
       "           [-1.321e-03, 1.352e-03, -1.348e-03,  ..., 1.765e-03, -2.367e-03, 1.862e-03],\n",
       "           [1.055e-03, -1.375e-03, 1.645e-03,  ..., 1.989e-03, -1.874e-03, 2.203e-03],\n",
       "           ...,\n",
       "           [1.405e-03, -1.756e-03, 2.588e-03,  ..., -2.699e-03, -2.527e-03, 1.837e-03],\n",
       "           [1.081e-03, 1.881e-03, -2.123e-03,  ..., -2.058e-03, 2.241e-03, 1.781e-03],\n",
       "           [-1.114e-03, 1.677e-03, 2.497e-03,  ..., 2.348e-03, 2.483e-03, 2.077e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.29.self_attn.o_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[1.012e-02, 3.189e-03, -7.568e-03,  ..., -3.773e-03, -2.029e-03, -2.165e-03],\n",
       "           [2.108e-03, 1.434e-03, -2.260e-03,  ..., 1.686e-03, -1.664e-03, -1.754e-03],\n",
       "           [-1.663e-03, -1.989e-03, 1.759e-03,  ..., 1.523e-03, -1.294e-03, -2.035e-03],\n",
       "           ...,\n",
       "           [-1.256e-03, 2.068e-03, 1.799e-03,  ..., -2.090e-03, -1.976e-03, -1.819e-03],\n",
       "           [-1.963e-03, 2.052e-03, -1.535e-03,  ..., 1.797e-03, -1.842e-03, -2.062e-03],\n",
       "           [1.451e-03, 1.902e-03, 1.481e-03,  ..., 2.361e-03, 2.153e-03, -1.773e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.29.mlp.gate_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-5.684e-03, 1.366e-03, -1.561e-03,  ..., 8.035e-04, 5.989e-04, -8.512e-04],\n",
       "           [-2.699e-03, -1.511e-03, 1.294e-03,  ..., 8.450e-04, -8.597e-04, -7.391e-04],\n",
       "           [-2.338e-03, 1.736e-03, -1.220e-03,  ..., 7.124e-04, -5.965e-04, -6.967e-04],\n",
       "           ...,\n",
       "           [-1.755e-03, -1.472e-03, -1.641e-03,  ..., -1.687e-03, -1.933e-03, -1.681e-03],\n",
       "           [1.494e-03, 1.432e-03, 1.145e-03,  ..., -1.813e-03, 1.961e-03, -2.182e-03],\n",
       "           [-1.991e-03, 1.619e-03, -1.738e-03,  ..., 2.337e-03, -1.545e-03, 1.573e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.29.mlp.up_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[3.191e-03, -2.085e-03, -1.732e-03,  ..., 9.899e-04, 1.112e-03, -1.052e-03],\n",
       "           [3.139e-03, -1.685e-03, 1.752e-03,  ..., 1.000e-03, -1.005e-03, -1.040e-03],\n",
       "           [-4.543e-03, 1.505e-03, -8.464e-04,  ..., 6.380e-04, -4.640e-04, 5.817e-04],\n",
       "           ...,\n",
       "           [-1.307e-03, -1.556e-03, -1.800e-03,  ..., 1.609e-03, -1.242e-03, 1.760e-03],\n",
       "           [1.094e-03, -1.380e-03, -1.273e-03,  ..., 2.134e-03, 1.797e-03, -1.916e-03],\n",
       "           [1.229e-03, -1.466e-03, 1.357e-03,  ..., -1.662e-03, -1.872e-03, 1.781e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.29.mlp.down_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-2.855e-02, -1.553e-02, -6.947e-03,  ..., -2.026e-03, -3.151e-03, 2.836e-03],\n",
       "           [5.138e-03, -8.141e-03, 3.025e-03,  ..., 1.365e-03, 1.402e-03, 1.940e-03],\n",
       "           [-2.592e-03, 2.342e-03, -2.388e-03,  ..., -1.475e-03, 1.205e-03, -1.304e-03],\n",
       "           ...,\n",
       "           [-1.317e-03, 1.312e-03, 1.335e-03,  ..., 1.740e-03, -1.568e-03, 1.667e-03],\n",
       "           [1.562e-03, 1.629e-03, -1.317e-03,  ..., -1.451e-03, 2.090e-03, 1.750e-03],\n",
       "           [-1.153e-03, -1.210e-03, -1.592e-03,  ..., 1.484e-03, -1.520e-03, 2.195e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.30.self_attn.q_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[2.050e-03, 1.859e-03, 1.559e-03,  ..., -1.431e-03, -1.471e-03, 1.559e-03],\n",
       "           [-1.927e-03, 2.563e-03, -2.228e-03,  ..., 1.735e-03, 2.279e-03, 2.001e-03],\n",
       "           [-1.833e-03, 1.619e-03, -1.544e-03,  ..., -2.031e-03, -1.889e-03, -1.741e-03],\n",
       "           ...,\n",
       "           [-2.468e-03, -2.497e-03, 2.842e-03,  ..., 2.127e-03, -2.304e-03, 3.109e-03],\n",
       "           [-2.611e-03, -2.611e-03, 2.455e-03,  ..., 2.209e-03, -2.293e-03, -3.042e-03],\n",
       "           [2.377e-03, 2.317e-03, -1.777e-03,  ..., 2.304e-03, -2.317e-03, 2.766e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.30.self_attn.k_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[1.961e-03, 2.287e-03, 2.331e-03,  ..., -1.471e-03, -1.592e-03, 1.836e-03],\n",
       "           [-1.959e-03, 1.511e-03, 2.007e-03,  ..., 1.740e-03, -1.617e-03, 1.570e-03],\n",
       "           [-2.958e-03, 2.432e-03, 2.199e-03,  ..., 2.329e-03, -1.957e-03, -2.777e-03],\n",
       "           ...,\n",
       "           [-4.108e-03, 3.496e-03, 3.126e-03,  ..., 4.417e-03, -3.248e-03, -4.070e-03],\n",
       "           [-2.848e-03, -3.225e-03, 3.086e-03,  ..., -3.014e-03, -3.464e-03, -3.639e-03],\n",
       "           [-2.726e-03, -3.124e-03, 3.033e-03,  ..., 3.805e-03, -2.825e-03, 3.353e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.30.self_attn.v_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-1.789e-03, -2.954e-03, -2.241e-03,  ..., 1.823e-03, -1.532e-03, 1.567e-03],\n",
       "           [1.306e-03, -2.487e-03, -2.064e-03,  ..., 2.268e-03, -1.414e-03, 2.502e-03],\n",
       "           [-1.463e-03, -1.735e-03, -2.117e-03,  ..., -2.188e-03, 1.807e-03, 1.894e-03],\n",
       "           ...,\n",
       "           [1.523e-03, -1.627e-03, 1.723e-03,  ..., -1.689e-03, 1.864e-03, -1.920e-03],\n",
       "           [1.278e-03, 1.577e-03, 1.532e-03,  ..., 2.172e-03, -1.758e-03, -1.768e-03],\n",
       "           [-1.219e-03, -1.918e-03, -1.266e-03,  ..., -1.781e-03, -1.913e-03, 2.045e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.30.self_attn.o_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[1.106e-02, 1.198e-02, 1.166e-02,  ..., -5.058e-03, 6.382e-03, 7.160e-03],\n",
       "           [-2.445e-03, 2.525e-03, -2.590e-03,  ..., 2.651e-03, 2.634e-03, 2.295e-03],\n",
       "           [-1.546e-03, 1.974e-03, -2.081e-03,  ..., 2.447e-03, -1.826e-03, 2.441e-03],\n",
       "           ...,\n",
       "           [1.137e-03, -1.376e-03, 1.315e-03,  ..., -1.083e-03, -1.689e-03, -1.919e-03],\n",
       "           [-1.450e-03, 1.390e-03, 1.352e-03,  ..., -1.475e-03, -1.704e-03, 1.987e-03],\n",
       "           [2.073e-03, 1.242e-03, 1.390e-03,  ..., -1.249e-03, -1.795e-03, -2.056e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.30.mlp.gate_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-9.293e-03, -2.361e-03, -1.620e-03,  ..., 1.099e-03, -1.046e-03, -9.637e-04],\n",
       "           [2.542e-03, 1.410e-03, 1.472e-03,  ..., -9.880e-04, -1.005e-03, -9.227e-04],\n",
       "           [-4.356e-03, 1.648e-03, 1.709e-03,  ..., 1.788e-03, 2.314e-03, -1.585e-03],\n",
       "           ...,\n",
       "           [-1.556e-03, 1.884e-03, -1.749e-03,  ..., -1.705e-03, 1.457e-03, -1.890e-03],\n",
       "           [-1.271e-03, 1.854e-03, -1.844e-03,  ..., 1.441e-03, 1.408e-03, -1.899e-03],\n",
       "           [1.532e-03, 1.484e-03, 1.676e-03,  ..., -2.186e-03, -2.096e-03, 1.658e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.30.mlp.up_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-4.871e-03, -2.296e-03, -1.775e-03,  ..., 1.162e-03, 1.115e-03, 1.038e-03],\n",
       "           [3.975e-03, 2.344e-03, 1.519e-03,  ..., 9.995e-04, 8.292e-04, -1.017e-03],\n",
       "           [-1.167e-03, 1.507e-03, 1.322e-03,  ..., 1.902e-03, 1.370e-03, -1.660e-03],\n",
       "           ...,\n",
       "           [-1.554e-03, 1.369e-03, 1.270e-03,  ..., -1.718e-03, -1.483e-03, -2.041e-03],\n",
       "           [-1.273e-03, -1.761e-03, -1.730e-03,  ..., -1.813e-03, -1.422e-03, 1.678e-03],\n",
       "           [-1.621e-03, -1.260e-03, 1.246e-03,  ..., 1.556e-03, -1.728e-03, -1.718e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.30.mlp.down_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[4.321e-02, -6.817e-03, -7.225e-03,  ..., -3.242e-03, -2.495e-03, -2.842e-03],\n",
       "           [-9.605e-03, 3.788e-03, 4.780e-03,  ..., 1.355e-03, 1.212e-03, -1.114e-03],\n",
       "           [3.246e-03, 3.586e-03, 1.787e-03,  ..., -1.143e-03, -1.106e-03, 1.301e-03],\n",
       "           ...,\n",
       "           [1.417e-03, -1.443e-03, 1.853e-03,  ..., -1.706e-03, 1.936e-03, -1.505e-03],\n",
       "           [-1.236e-03, -1.555e-03, -1.627e-03,  ..., 1.521e-03, 1.332e-03, -1.900e-03],\n",
       "           [-1.288e-03, -1.297e-03, -1.416e-03,  ..., -1.674e-03, 1.961e-03, 1.472e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.31.self_attn.q_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[1.953e-03, 1.650e-03, 2.142e-03,  ..., -1.394e-03, 1.818e-03, 1.293e-03],\n",
       "           [8.864e-04, 1.058e-03, 9.470e-04,  ..., -1.024e-03, -1.080e-03, -9.918e-04],\n",
       "           [-1.126e-03, -1.115e-03, -1.404e-03,  ..., 1.549e-03, -1.297e-03, 1.481e-03],\n",
       "           ...,\n",
       "           [-3.937e-03, -3.016e-03, 2.390e-03,  ..., 3.378e-03, -2.783e-03, -2.256e-03],\n",
       "           [-2.647e-03, 3.363e-03, 2.907e-03,  ..., -2.571e-03, 2.974e-03, -2.987e-03],\n",
       "           [4.257e-03, -3.187e-03, 2.960e-03,  ..., -2.720e-03, -2.705e-03, -3.363e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.31.self_attn.k_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[2.073e-03, 1.854e-03, 1.922e-03,  ..., -1.549e-03, -2.035e-03, -1.593e-03],\n",
       "           [1.881e-03, 1.986e-03, 1.955e-03,  ..., -1.760e-03, 1.968e-03, -1.779e-03],\n",
       "           [3.359e-03, 1.829e-03, -1.560e-03,  ..., 2.106e-03, 2.085e-03, -2.079e-03],\n",
       "           ...,\n",
       "           [5.577e-03, -6.733e-03, -5.451e-03,  ..., -1.716e-03, 2.026e-03, -1.750e-03],\n",
       "           [5.909e-03, 5.707e-03, 5.943e-03,  ..., 1.531e-03, -1.936e-03, 1.644e-03],\n",
       "           [-7.874e-03, 4.459e-03, -3.851e-03,  ..., 1.720e-03, 2.115e-03, 2.251e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.31.self_attn.v_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-1.136e-03, 1.456e-03, -1.999e-03,  ..., 2.098e-03, -2.462e-03, 2.209e-03],\n",
       "           [1.400e-03, -1.491e-03, -1.683e-03,  ..., 1.859e-03, 2.098e-03, 2.216e-03],\n",
       "           [-1.054e-03, -1.353e-03, 1.841e-03,  ..., 1.851e-03, -2.476e-03, 2.323e-03],\n",
       "           ...,\n",
       "           [-1.366e-03, 1.490e-03, 1.534e-03,  ..., -1.369e-03, 1.766e-03, 1.258e-03],\n",
       "           [1.011e-03, -1.425e-03, -1.781e-03,  ..., -1.628e-03, 1.629e-03, -1.579e-03],\n",
       "           [1.425e-03, 1.794e-03, 1.698e-03,  ..., -1.466e-03, -1.359e-03, 1.809e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.31.self_attn.o_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[2.570e-02, -9.911e-03, 1.595e-02,  ..., -1.089e-02, 5.775e-03, -6.817e-03],\n",
       "           [-3.105e-03, 1.646e-03, 2.316e-03,  ..., 2.680e-03, -3.334e-03, 2.003e-03],\n",
       "           [-1.404e-03, -1.391e-03, -1.826e-03,  ..., 2.203e-03, 2.146e-03, 2.695e-03],\n",
       "           ...,\n",
       "           [-1.399e-03, 1.610e-03, 1.561e-03,  ..., 9.108e-04, -1.525e-03, -1.709e-03],\n",
       "           [1.567e-03, 1.523e-03, -1.624e-03,  ..., 1.109e-03, 1.647e-03, -1.875e-03],\n",
       "           [-1.838e-03, -1.539e-03, -2.237e-03,  ..., -1.068e-03, -1.413e-03, 1.854e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.31.mlp.gate_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-7.298e-03, 2.827e-03, 2.020e-03,  ..., -1.466e-03, 1.225e-03, 1.044e-03],\n",
       "           [-4.185e-03, 2.357e-03, -2.380e-03,  ..., -1.532e-03, -1.687e-03, 1.246e-03],\n",
       "           [-9.560e-03, 3.681e-03, -3.111e-03,  ..., 1.836e-03, -1.626e-03, 1.524e-03],\n",
       "           ...,\n",
       "           [-1.309e-03, 1.877e-03, -1.790e-03,  ..., -2.012e-03, 1.679e-03, -1.528e-03],\n",
       "           [-1.303e-03, 1.376e-03, 1.933e-03,  ..., 1.804e-03, 2.604e-03, -1.731e-03],\n",
       "           [1.533e-03, -1.686e-03, -1.864e-03,  ..., -1.710e-03, -1.603e-03, -1.777e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.31.mlp.up_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-5.577e-03, 2.192e-03, 1.950e-03,  ..., 8.407e-04, 1.394e-03, 1.266e-03],\n",
       "           [2.806e-03, 1.519e-03, 1.626e-03,  ..., -1.066e-03, 1.074e-03, 1.366e-03],\n",
       "           [4.108e-03, 2.621e-03, -2.195e-03,  ..., 1.315e-03, 1.060e-03, -1.177e-03],\n",
       "           ...,\n",
       "           [1.184e-03, -1.399e-03, -1.767e-03,  ..., -1.498e-03, 1.735e-03, 1.827e-03],\n",
       "           [-1.399e-03, 1.739e-03, 1.348e-03,  ..., -2.064e-03, -1.767e-03, -1.848e-03],\n",
       "           [-1.415e-03, -1.523e-03, -1.483e-03,  ..., -2.123e-03, -1.553e-03, 1.758e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.31.mlp.down_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[4.254e-02, -1.168e-02, 5.978e-03,  ..., 3.046e-03, 2.491e-03, 2.289e-03],\n",
       "           [-9.003e-03, -4.295e-03, -2.947e-03,  ..., -1.466e-03, 1.289e-03, -1.277e-03],\n",
       "           [-3.294e-03, 2.924e-03, -1.456e-03,  ..., 1.298e-03, 1.725e-03, -1.422e-03],\n",
       "           ...,\n",
       "           [1.340e-03, -1.112e-03, -1.417e-03,  ..., 1.850e-03, -1.741e-03, -1.860e-03],\n",
       "           [1.365e-03, 1.554e-03, -1.423e-03,  ..., 1.686e-03, 1.690e-03, -1.874e-03],\n",
       "           [-9.503e-04, 1.526e-03, 1.657e-03,  ..., 1.900e-03, 1.532e-03, -1.561e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.32.self_attn.q_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[6.633e-04, -5.803e-04, -6.781e-04,  ..., -8.101e-04, 7.529e-04, -9.089e-04],\n",
       "           [-6.185e-04, 4.787e-04, -5.760e-04,  ..., 7.463e-04, -6.719e-04, -8.259e-04],\n",
       "           [-1.114e-03, -9.246e-04, -8.636e-04,  ..., -1.369e-03, -9.727e-04, 1.204e-03],\n",
       "           ...,\n",
       "           [-2.071e-03, 2.092e-03, -2.386e-03,  ..., 2.451e-03, 2.594e-03, 2.491e-03],\n",
       "           [-6.130e-03, -2.756e-03, -2.804e-03,  ..., 2.794e-03, -3.542e-03, 2.806e-03],\n",
       "           [-2.287e-03, -1.842e-03, -1.961e-03,  ..., 3.071e-03, -3.098e-03, 2.247e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.32.self_attn.k_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[2.176e-03, 1.637e-03, 1.527e-03,  ..., 1.711e-03, -2.060e-03, -2.266e-03],\n",
       "           [-2.354e-03, -2.005e-03, -1.982e-03,  ..., 1.627e-03, -2.132e-03, 2.008e-03],\n",
       "           [1.716e-03, -1.692e-03, 1.957e-03,  ..., 2.096e-03, -1.749e-03, 1.789e-03],\n",
       "           ...,\n",
       "           [-3.378e-03, 3.775e-03, 4.810e-03,  ..., -2.281e-03, 2.409e-03, 2.949e-03],\n",
       "           [-8.446e-03, 4.368e-03, 3.437e-03,  ..., 2.460e-03, -1.892e-03, -1.715e-03],\n",
       "           [3.426e-03, -4.063e-03, 4.177e-03,  ..., -2.602e-03, -2.762e-03, -3.132e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.32.self_attn.v_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[1.288e-03, -1.498e-03, -1.499e-03,  ..., -1.783e-03, 2.634e-03, -1.727e-03],\n",
       "           [1.628e-03, -1.339e-03, -1.270e-03,  ..., 2.327e-03, 1.997e-03, -2.039e-03],\n",
       "           [-1.129e-03, 1.598e-03, 1.657e-03,  ..., 2.266e-03, -2.403e-03, -1.952e-03],\n",
       "           ...,\n",
       "           [-1.143e-03, -1.974e-03, -2.842e-03,  ..., -1.577e-03, 1.518e-03, 1.599e-03],\n",
       "           [1.635e-03, 2.138e-03, -2.193e-03,  ..., 1.686e-03, -1.473e-03, 1.825e-03],\n",
       "           [1.454e-03, -1.952e-03, -2.905e-03,  ..., 1.144e-03, 1.531e-03, 1.607e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.32.self_attn.o_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[5.684e-03, -5.100e-03, -5.836e-03,  ..., -4.524e-03, -7.805e-03, -8.720e-03],\n",
       "           [-2.007e-03, -2.861e-03, 2.775e-03,  ..., 1.764e-03, -2.743e-03, 3.675e-03],\n",
       "           [1.482e-03, -1.731e-03, 1.798e-03,  ..., 2.094e-03, 2.304e-03, -3.126e-03],\n",
       "           ...,\n",
       "           [-1.515e-03, 1.698e-03, 1.974e-03,  ..., -2.705e-03, 1.785e-03, 1.001e-03],\n",
       "           [1.449e-03, 1.993e-03, -2.029e-03,  ..., -1.650e-03, -1.681e-03, 1.317e-03],\n",
       "           [1.825e-03, -1.837e-03, 1.821e-03,  ..., -2.562e-03, 1.692e-03, -1.219e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.32.mlp.gate_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-7.645e-03, 2.890e-03, 2.289e-03,  ..., 1.569e-03, 1.450e-03, -1.978e-03],\n",
       "           [3.023e-03, 2.052e-03, 1.309e-03,  ..., -9.274e-04, -6.571e-04, 1.018e-03],\n",
       "           [2.457e-03, -1.725e-03, 1.489e-03,  ..., -1.210e-03, -1.345e-03, -1.247e-03],\n",
       "           ...,\n",
       "           [-1.741e-03, -1.662e-03, -1.703e-03,  ..., 1.852e-03, -1.904e-03, -1.798e-03],\n",
       "           [2.476e-03, 2.880e-03, 2.728e-03,  ..., -2.207e-03, -2.369e-03, -2.197e-03],\n",
       "           [1.727e-03, -2.138e-03, -1.567e-03,  ..., 1.536e-03, -1.409e-03, 1.775e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.32.mlp.up_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-6.050e-03, 2.096e-03, -2.249e-03,  ..., 1.071e-03, 1.384e-03, 1.048e-03],\n",
       "           [-2.985e-03, -1.139e-03, 1.513e-03,  ..., -1.287e-03, -1.159e-03, 9.384e-04],\n",
       "           [6.767e-03, 1.498e-03, 1.707e-03,  ..., 8.512e-04, 8.750e-04, -7.558e-04],\n",
       "           ...,\n",
       "           [1.282e-03, 1.555e-03, 2.090e-03,  ..., -1.523e-03, -1.553e-03, 1.559e-03],\n",
       "           [-1.509e-03, -1.435e-03, 1.542e-03,  ..., 1.690e-03, 2.117e-03, -2.060e-03],\n",
       "           [1.847e-03, 1.548e-03, 2.029e-03,  ..., -1.428e-03, -1.484e-03, -1.817e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.32.mlp.down_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[3.540e-02, 9.216e-03, 8.125e-03,  ..., 4.513e-03, 3.355e-03, 5.032e-03],\n",
       "           [-1.539e-02, -4.574e-03, 5.440e-03,  ..., -1.942e-03, 1.455e-03, -1.533e-03],\n",
       "           [-3.382e-03, -2.699e-03, 1.669e-03,  ..., -1.315e-03, -1.777e-03, 1.524e-03],\n",
       "           ...,\n",
       "           [1.121e-03, -1.513e-03, 1.507e-03,  ..., -1.760e-03, -1.866e-03, 1.641e-03],\n",
       "           [1.250e-03, -1.266e-03, -1.759e-03,  ..., 1.938e-03, 1.480e-03, 2.060e-03],\n",
       "           [-1.040e-03, -1.248e-03, 1.332e-03,  ..., 1.686e-03, -1.638e-03, 1.782e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.33.self_attn.q_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[3.149e-03, 3.639e-03, -2.926e-03,  ..., -2.113e-03, -1.746e-03, -2.146e-03],\n",
       "           [2.264e-03, 1.984e-03, -2.758e-03,  ..., 1.862e-03, 1.824e-03, 1.937e-03],\n",
       "           [-2.207e-03, 2.623e-03, -2.487e-03,  ..., 2.249e-03, -1.970e-03, -2.174e-03],\n",
       "           ...,\n",
       "           [5.260e-03, 2.834e-03, 3.025e-03,  ..., 1.798e-03, -2.048e-03, -1.368e-03],\n",
       "           [3.830e-03, 4.074e-03, -2.489e-03,  ..., -1.584e-03, 1.436e-03, -1.390e-03],\n",
       "           [-3.971e-03, 3.201e-03, 2.611e-03,  ..., -1.443e-03, 1.490e-03, -1.778e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.33.self_attn.k_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-3.191e-03, -2.274e-03, 1.842e-03,  ..., -1.877e-03, 1.624e-03, 1.612e-03],\n",
       "           [-3.038e-03, 2.922e-03, -3.073e-03,  ..., -2.148e-03, 2.651e-03, -2.308e-03],\n",
       "           [-1.868e-03, -2.129e-03, -1.697e-03,  ..., 2.420e-03, -1.584e-03, -2.056e-03],\n",
       "           ...,\n",
       "           [9.560e-03, 5.833e-03, 6.767e-03,  ..., -1.787e-03, -2.300e-03, -2.146e-03],\n",
       "           [6.298e-03, 7.614e-03, -5.356e-03,  ..., 2.359e-03, 1.854e-03, 1.925e-03],\n",
       "           [-7.755e-03, 5.783e-03, -4.841e-03,  ..., -2.537e-03, 1.760e-03, 2.581e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.33.self_attn.v_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-1.655e-03, -1.574e-03, 1.561e-03,  ..., -2.756e-03, 1.718e-03, -2.258e-03],\n",
       "           [-1.416e-03, -1.844e-03, 1.868e-03,  ..., 2.876e-03, 2.260e-03, 3.305e-03],\n",
       "           [1.184e-03, -1.737e-03, -1.677e-03,  ..., 2.705e-03, -2.670e-03, 2.876e-03],\n",
       "           ...,\n",
       "           [-1.065e-03, -1.421e-03, 1.989e-03,  ..., -2.718e-03, 2.756e-03, 2.155e-03],\n",
       "           [-1.327e-03, 1.598e-03, 1.925e-03,  ..., 2.605e-03, -2.527e-03, 2.720e-03],\n",
       "           [1.107e-03, -1.961e-03, 1.657e-03,  ..., -2.529e-03, -2.554e-03, -2.432e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.33.self_attn.o_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-1.415e-02, -1.437e-02, -1.389e-02,  ..., -6.325e-03, -2.591e-02, -1.184e-02],\n",
       "           [5.234e-03, -5.127e-03, 4.555e-03,  ..., -3.891e-03, 5.817e-03, 4.044e-03],\n",
       "           [2.018e-03, -1.995e-03, -1.873e-03,  ..., -1.931e-03, 1.884e-03, 1.891e-03],\n",
       "           ...,\n",
       "           [-2.087e-03, 1.493e-03, -2.205e-03,  ..., -1.434e-03, 1.523e-03, 1.447e-03],\n",
       "           [1.804e-03, 1.410e-03, -2.296e-03,  ..., -1.912e-03, -1.895e-03, -1.882e-03],\n",
       "           [-1.603e-03, 1.254e-03, 2.275e-03,  ..., -1.802e-03, -2.188e-03, 1.629e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.33.mlp.gate_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[5.310e-03, 2.329e-03, -2.777e-03,  ..., 1.472e-03, 1.492e-03, -1.794e-03],\n",
       "           [2.602e-03, 1.515e-03, 1.595e-03,  ..., -8.898e-04, -1.040e-03, 9.456e-04],\n",
       "           [-3.769e-03, 2.192e-03, 1.526e-03,  ..., 1.197e-03, 1.229e-03, 1.134e-03],\n",
       "           ...,\n",
       "           [1.572e-03, 1.790e-03, 1.696e-03,  ..., -1.993e-03, 1.552e-03, -1.507e-03],\n",
       "           [1.472e-03, -1.675e-03, -1.581e-03,  ..., -1.212e-03, -1.813e-03, 1.552e-03],\n",
       "           [1.444e-03, -1.530e-03, -1.387e-03,  ..., -2.306e-03, -1.870e-03, 1.789e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.33.mlp.up_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-3.580e-03, 2.167e-03, -1.976e-03,  ..., -9.322e-04, 1.316e-03, -1.060e-03],\n",
       "           [3.241e-03, 1.863e-03, -1.488e-03,  ..., -8.640e-04, -9.065e-04, -1.076e-03],\n",
       "           [-3.075e-03, -1.328e-03, -1.440e-03,  ..., -9.761e-04, 1.032e-03, -9.689e-04],\n",
       "           ...,\n",
       "           [-1.311e-03, 1.645e-03, 1.522e-03,  ..., -1.611e-03, 1.665e-03, -2.228e-03],\n",
       "           [1.566e-03, 1.666e-03, 1.784e-03,  ..., -1.490e-03, -1.836e-03, 1.619e-03],\n",
       "           [1.520e-03, 1.596e-03, -1.661e-03,  ..., 1.670e-03, 1.896e-03, -1.788e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.33.mlp.down_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-2.652e-02, 6.615e-03, 4.818e-03,  ..., 2.455e-03, -3.729e-03, 3.414e-03],\n",
       "           [1.927e-02, -6.233e-03, 3.649e-03,  ..., 1.323e-03, 1.757e-03, 1.352e-03],\n",
       "           [4.028e-03, -4.158e-03, 1.822e-03,  ..., 1.373e-03, -1.671e-03, 2.075e-03],\n",
       "           ...,\n",
       "           [1.021e-03, 1.758e-03, 1.267e-03,  ..., 2.174e-03, 1.863e-03, 1.790e-03],\n",
       "           [1.098e-03, 1.061e-03, -1.085e-03,  ..., -2.134e-03, 1.764e-03, -1.813e-03],\n",
       "           [1.160e-03, -1.838e-03, -1.595e-03,  ..., -2.161e-03, 2.106e-03, -1.745e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.34.self_attn.q_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-2.577e-03, -2.020e-03, -2.266e-03,  ..., 1.595e-03, -2.151e-03, 2.140e-03],\n",
       "           [2.371e-03, -2.230e-03, -1.587e-03,  ..., 2.596e-03, 1.802e-03, -1.695e-03],\n",
       "           [-2.089e-03, 1.839e-03, -1.682e-03,  ..., 1.239e-03, 1.386e-03, 1.456e-03],\n",
       "           ...,\n",
       "           [1.450e-03, -1.822e-03, 1.837e-03,  ..., -2.752e-03, -2.712e-03, -3.296e-03],\n",
       "           [-2.298e-03, 2.253e-03, 1.949e-03,  ..., -2.413e-03, 2.636e-03, -3.164e-03],\n",
       "           [2.544e-03, -1.608e-03, -1.972e-03,  ..., -2.323e-03, -2.058e-03, -1.910e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.34.self_attn.k_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-1.876e-03, -2.190e-03, 1.869e-03,  ..., 1.648e-03, 1.799e-03, 1.951e-03],\n",
       "           [-2.163e-03, -1.716e-03, 2.319e-03,  ..., -2.172e-03, 2.069e-03, -2.171e-03],\n",
       "           [2.195e-03, -2.096e-03, -1.690e-03,  ..., 1.731e-03, -1.707e-03, -2.001e-03],\n",
       "           ...,\n",
       "           [-4.002e-03, -3.189e-03, 4.433e-03,  ..., -3.315e-03, -3.426e-03, 3.489e-03],\n",
       "           [-4.230e-03, 4.097e-03, -4.074e-03,  ..., 3.025e-03, -3.315e-03, 3.323e-03],\n",
       "           [-4.425e-03, -3.710e-03, 4.162e-03,  ..., 2.975e-03, -3.014e-03, 3.653e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.34.self_attn.v_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-1.583e-03, -1.627e-03, -1.704e-03,  ..., 2.096e-03, 2.571e-03, 2.569e-03],\n",
       "           [1.629e-03, -2.172e-03, 2.266e-03,  ..., -2.495e-03, 1.740e-03, 2.214e-03],\n",
       "           [-2.157e-03, -1.553e-03, 1.410e-03,  ..., 2.325e-03, -2.213e-03, 1.808e-03],\n",
       "           ...,\n",
       "           [-1.509e-03, -1.848e-03, 2.033e-03,  ..., 2.729e-03, 2.686e-03, 2.514e-03],\n",
       "           [-1.884e-03, 2.371e-03, 2.386e-03,  ..., 2.638e-03, 3.073e-03, 2.293e-03],\n",
       "           [2.136e-03, 2.239e-03, -2.327e-03,  ..., -2.972e-03, -2.697e-03, 2.392e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.34.self_attn.o_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-7.935e-03, -1.027e-02, 1.176e-02,  ..., 1.585e-02, -1.846e-02, -1.572e-02],\n",
       "           [5.970e-03, 6.523e-03, 6.580e-03,  ..., -9.209e-03, -1.257e-02, -9.888e-03],\n",
       "           [-2.998e-03, -3.201e-03, 2.913e-03,  ..., -2.581e-03, -2.516e-03, -2.739e-03],\n",
       "           ...,\n",
       "           [-1.946e-03, 1.569e-03, -1.137e-03,  ..., 2.134e-03, 2.274e-03, -2.007e-03],\n",
       "           [-1.952e-03, 2.182e-03, -1.464e-03,  ..., 2.069e-03, 1.706e-03, -1.890e-03],\n",
       "           [-1.959e-03, 2.121e-03, -1.517e-03,  ..., -1.880e-03, -2.008e-03, -2.281e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.34.mlp.gate_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-2.289e-03, 1.509e-03, -1.503e-03,  ..., -9.899e-04, -1.019e-03, 1.034e-03],\n",
       "           [2.892e-03, 1.682e-03, -1.526e-03,  ..., -9.737e-04, -9.794e-04, 1.170e-03],\n",
       "           [3.534e-03, -1.763e-03, 1.604e-03,  ..., -1.246e-03, 1.211e-03, -1.122e-03],\n",
       "           ...,\n",
       "           [1.323e-03, 1.559e-03, 1.597e-03,  ..., 1.494e-03, -1.534e-03, 1.569e-03],\n",
       "           [-1.307e-03, 1.697e-03, 1.822e-03,  ..., -1.891e-03, -1.765e-03, -2.052e-03],\n",
       "           [-1.411e-03, -2.100e-03, -1.569e-03,  ..., 1.719e-03, 1.653e-03, 1.834e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.34.mlp.up_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[2.293e-03, -1.782e-03, -1.627e-03,  ..., 1.289e-03, -1.304e-03, -1.259e-03],\n",
       "           [-3.267e-03, -1.804e-03, 1.434e-03,  ..., 1.577e-03, 1.454e-03, 1.530e-03],\n",
       "           [-1.826e-03, -1.634e-03, -1.609e-03,  ..., 1.445e-03, 1.391e-03, 1.572e-03],\n",
       "           ...,\n",
       "           [-1.423e-03, -1.751e-03, -2.399e-03,  ..., -1.831e-03, 2.090e-03, -1.771e-03],\n",
       "           [2.169e-03, -1.803e-03, -1.713e-03,  ..., 1.508e-03, 1.504e-03, -1.801e-03],\n",
       "           [-2.254e-03, 2.243e-03, -1.564e-03,  ..., -1.777e-03, -1.739e-03, 2.178e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.34.mlp.down_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-1.265e-02, 6.489e-03, 1.599e-02,  ..., 4.066e-03, -1.704e-03, 3.054e-03],\n",
       "           [-1.212e-02, 9.613e-03, -9.041e-03,  ..., -1.498e-03, 1.694e-03, -1.692e-03],\n",
       "           [-1.019e-02, 4.204e-03, 3.677e-03,  ..., -1.511e-03, -1.654e-03, 1.117e-03],\n",
       "           ...,\n",
       "           [1.244e-03, 1.663e-03, -1.151e-03,  ..., 1.534e-03, -1.888e-03, -1.801e-03],\n",
       "           [-1.051e-03, -1.232e-03, 1.403e-03,  ..., 2.085e-03, 1.937e-03, -1.532e-03],\n",
       "           [9.880e-04, -2.569e-03, -1.450e-03,  ..., -1.535e-03, 1.560e-03, -1.637e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.35.self_attn.q_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[1.854e-03, -1.707e-03, -1.751e-03,  ..., 1.912e-03, 1.786e-03, -1.785e-03],\n",
       "           [2.796e-03, 2.817e-03, 1.847e-03,  ..., 1.480e-03, 1.730e-03, -1.659e-03],\n",
       "           [-1.793e-03, -2.007e-03, -2.281e-03,  ..., -1.403e-03, 1.493e-03, -1.350e-03],\n",
       "           ...,\n",
       "           [2.134e-03, -1.402e-03, 1.105e-03,  ..., -1.764e-03, -1.595e-03, 1.556e-03],\n",
       "           [-2.352e-03, -2.241e-03, 1.760e-03,  ..., 1.629e-03, -1.184e-03, -1.507e-03],\n",
       "           [1.616e-03, 1.614e-03, 1.221e-03,  ..., -1.784e-03, -1.354e-03, 1.746e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.35.self_attn.k_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[2.489e-03, 2.941e-03, 1.828e-03,  ..., -2.394e-03, 1.940e-03, 1.690e-03],\n",
       "           [2.619e-03, 2.214e-03, -1.569e-03,  ..., 1.554e-03, 1.677e-03, -2.039e-03],\n",
       "           [2.443e-03, -2.316e-03, -2.079e-03,  ..., -2.531e-03, -2.317e-03, -1.951e-03],\n",
       "           ...,\n",
       "           [-8.514e-03, 7.050e-03, -6.222e-03,  ..., -2.970e-03, -2.991e-03, -2.125e-03],\n",
       "           [1.286e-02, -6.428e-03, 4.501e-03,  ..., 1.999e-03, -1.940e-03, -2.792e-03],\n",
       "           [-8.087e-03, -6.390e-03, 6.836e-03,  ..., -1.925e-03, -2.275e-03, -2.371e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.35.self_attn.v_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[2.178e-03, 2.157e-03, 2.245e-03,  ..., -1.805e-03, -1.490e-03, -1.436e-03],\n",
       "           [-1.079e-02, 2.810e-03, -2.300e-03,  ..., 1.991e-03, -1.823e-03, -1.544e-03],\n",
       "           [-1.694e-03, -2.762e-03, -2.110e-03,  ..., -1.604e-03, -1.409e-03, 1.659e-03],\n",
       "           ...,\n",
       "           [-1.468e-03, 1.980e-03, 2.014e-03,  ..., -2.037e-03, 2.155e-03, 1.833e-03],\n",
       "           [-1.697e-03, -2.384e-03, 2.895e-03,  ..., 1.986e-03, 1.887e-03, -2.041e-03],\n",
       "           [-2.405e-03, 2.502e-03, -2.752e-03,  ..., 2.165e-03, -2.447e-03, -2.625e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.35.self_attn.o_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-4.437e-02, 3.174e-02, -3.195e-02,  ..., -2.041e-03, -2.884e-03, -1.904e-03],\n",
       "           [-1.089e-02, 7.507e-03, -5.093e-03,  ..., -2.392e-03, -2.657e-03, 4.169e-03],\n",
       "           [-3.604e-02, 1.974e-02, -1.197e-02,  ..., -2.052e-03, -3.828e-03, -2.069e-03],\n",
       "           ...,\n",
       "           [1.534e-03, 1.341e-03, -1.125e-03,  ..., -1.826e-03, 1.491e-03, 1.678e-03],\n",
       "           [-1.088e-03, 1.319e-03, 1.135e-03,  ..., -1.597e-03, 1.688e-03, -1.684e-03],\n",
       "           [1.009e-03, -1.682e-03, 1.331e-03,  ..., -1.335e-03, -2.935e-03, 1.611e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.35.mlp.gate_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-2.064e-03, 1.683e-03, 1.820e-03,  ..., 1.715e-03, 1.681e-03, 1.569e-03],\n",
       "           [-2.316e-03, -1.208e-03, 1.128e-03,  ..., 1.474e-03, 1.684e-03, 1.811e-03],\n",
       "           [3.754e-03, 2.291e-03, 2.552e-03,  ..., -1.704e-03, -1.895e-03, 2.722e-03],\n",
       "           ...,\n",
       "           [2.151e-03, 2.062e-03, 2.062e-03,  ..., 2.829e-03, 2.645e-03, -2.485e-03],\n",
       "           [-2.569e-03, 1.938e-03, 2.579e-03,  ..., 2.296e-03, 2.346e-03, 3.035e-03],\n",
       "           [-2.111e-03, -2.316e-03, 2.136e-03,  ..., -2.424e-03, 2.335e-03, -2.615e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.35.mlp.up_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[1.698e-03, -1.245e-03, -1.252e-03,  ..., 1.392e-03, -1.535e-03, -1.359e-03],\n",
       "           [1.844e-03, 1.842e-03, -1.564e-03,  ..., 1.538e-03, 1.275e-03, 1.523e-03],\n",
       "           [2.178e-03, -1.208e-03, 1.176e-03,  ..., -1.184e-03, -1.160e-03, -1.649e-03],\n",
       "           ...,\n",
       "           [-1.566e-03, 1.970e-03, 1.454e-03,  ..., -2.136e-03, -2.028e-03, -1.740e-03],\n",
       "           [1.405e-03, -1.867e-03, 2.171e-03,  ..., -1.806e-03, 1.483e-03, 2.287e-03],\n",
       "           [1.377e-03, -1.915e-03, -2.468e-03,  ..., -2.380e-03, 2.115e-03, -2.638e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'model.layers.35.mlp.down_proj': {'data_type': 'int',\n",
       "   'bits': 4,\n",
       "   'group_size': 128,\n",
       "   'sym': True,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int',\n",
       "   'scale': tensor([[-4.910e-03, 8.095e-03, -6.718e-03,  ..., 1.525e-03, -1.916e-03, -3.128e-03],\n",
       "           [8.545e-03, -1.055e-02, 1.077e-02,  ..., -2.022e-03, -2.384e-03, -3.376e-03],\n",
       "           [-4.250e-03, -4.509e-03, -5.325e-03,  ..., 1.577e-03, 1.785e-03, -4.112e-03],\n",
       "           ...,\n",
       "           [-1.438e-03, 1.058e-03, 1.160e-03,  ..., -1.629e-03, 1.832e-03, 1.642e-03],\n",
       "           [1.133e-03, -9.823e-04, -1.101e-03,  ..., 1.693e-03, -1.790e-03, -2.235e-03],\n",
       "           [-1.037e-03, 1.440e-03, 1.308e-03,  ..., -2.081e-03, -1.616e-03, 1.870e-03]],\n",
       "          dtype=torch.float16),\n",
       "   'zp': tensor([[8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           ...,\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00],\n",
       "           [8.000e+00, 8.000e+00, 8.000e+00,  ..., 8.000e+00, 8.000e+00, 8.000e+00]],\n",
       "          dtype=torch.float16)},\n",
       "  'lm_head': {'data_type': 'float',\n",
       "   'bits': 16,\n",
       "   'group_size': None,\n",
       "   'sym': None,\n",
       "   'scale_dtype': torch.float16,\n",
       "   'act_bits': 16,\n",
       "   'act_group_size': 128,\n",
       "   'act_sym': True,\n",
       "   'act_dynamic': True,\n",
       "   'act_data_type': 'int'}})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# import torch\n",
    "# model_name = \"Qwen/Qwen2.5-72B-Instruct\"\n",
    "# model = AutoModelForCausalLM.from_pretrained( model_name, torch_dtype=torch.float16 )\n",
    "# tokenizer = AutoTokenizer.from_pretrained( model_name )\n",
    "\n",
    "from auto_round import AutoRound\n",
    "\n",
    "bits, group_size, sym = 4, 128, True\n",
    "\n",
    "autoround = AutoRound( merged_model, merged_tokenizer, nsamples=128, iters=512, low_gpu_mem_usage=True, batch_size=1, graddient_accumulation_steps=8, bits=bits, group_size=group_size, sym=sym)\n",
    "\n",
    "\n",
    "autoround.quantize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60ced88a9ec24a06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T19:55:42.053803Z",
     "start_time": "2025-01-13T19:55:42.047356Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/var/model/models/Ministral-8B-Instruct-2410'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir( f\"{models_root}/{model_name}\" )\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a319893998c2b23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T20:14:21.723674Z",
     "start_time": "2025-01-13T20:14:21.590983Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 24K\r\n",
      "drwxrwxr-x 6 1001 1001 4.0K Jan 13 18:04 .\r\n",
      "drwxrwxr-x 9 1001 1001 4.0K Jan  9 02:59 ..\r\n",
      "drwxr-xr-x 2 root root 4.0K Jan  9 21:56 merged-00-2025-01-09\r\n",
      "drwxr-xr-x 2 root root 4.0K Jan 13 16:29 merged-00-2025-01-09.awq\r\n",
      "drwxr-xr-x 2 root root 4.0K Jan 13 18:06 merged-00-2025-01-09.gptq\r\n",
      "drwxr-xr-x 3 root root 4.0K Jan  9 18:37 training-results-2025.01.09\r\n"
     ]
    }
   ],
   "source": [
    "! ls -alh /var/model/models/Ministral-8B-Instruct-2410/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf046babee80f590",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T21:00:01.682587Z",
     "start_time": "2025-01-13T21:00:01.676693Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./merged-00-2025-01-09.gptq\n"
     ]
    }
   ],
   "source": [
    "# Save quantized model\n",
    "gptq_path = merged_path + \".gptq\"\n",
    "print( gptq_path)\n",
    "# autoround.save_quantized( gptq_path, format='auto_gptq', inplace=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eb67ff052ad9b31f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T19:52:49.236130Z",
     "start_time": "2025-01-13T19:52:49.080205Z"
    }
   },
   "outputs": [],
   "source": [
    "# release memory\n",
    "reset_kernel( [ merged_model, merged_tokenizer ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4d8ad5c1b094e50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T20:57:24.184989Z",
     "start_time": "2025-01-13T20:57:24.179105Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./merged-00-2025-01-09.gptq'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gptq_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40c2c6b98c74916d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T20:57:29.603624Z",
     "start_time": "2025-01-13T20:57:26.854739Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:411: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(ctx, input, qweight, scales, qzeros, g_idx, bits, maxq):\n",
      "/usr/local/lib/python3.10/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:419: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n",
      "/usr/local/lib/python3.10/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float16)\n",
      "CUDA extension not installed.\n",
      "CUDA extension not installed.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:5006: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18d2e480f1ca49f7a25f0c0b88617057",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load quantized model\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "gptq_model = AutoModelForCausalLM.from_pretrained( gptq_path, device_map=\"cuda:1\" )\n",
    "gptq_tokenizer = AutoTokenizer.from_pretrained( gptq_path )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e43fb2866e0570b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Quantize using AWQ (Adaptive Weight Quantization) and write to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5f0165c58f9eb0a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T01:48:21.729911Z",
     "start_time": "2025-01-14T01:47:58.202361Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Validating ministral/Ministral-8B-Instruct-2410 w/ 10 samples\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "command\n",
      "search kagi current tab                              2\n",
      "search google new tab                                1\n",
      "agent router go to receptionist                      1\n",
      "search google current tab                            1\n",
      "search perplexity new tab                            1\n",
      "search google scholar using clipboard current tab    1\n",
      "search phind current tab                             1\n",
      "agent router go to date and time                     1\n",
      "go to current tab                                    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Reusing ConfigurationManager() singleton...\n",
      "Inserting DUPLICATE placeholders into the list. Requested length [500] > list length [74]\n",
      "Inserting DUPLICATE placeholders into the list. Requested length [500] > list length [148]\n",
      "Inserting DUPLICATE placeholders into the list. Requested length [500] > list length [296]\n",
      "Commands file for command [go to current tab] exists: True\n",
      "Commands file for command [go to new tab] exists: True\n",
      "Commands file for command [search current tab] exists: True\n",
      "Commands file for command [search new tab] exists: True\n",
      "Commands file for command [search google current tab] exists: True\n",
      "Commands file for command [search google new tab] exists: True\n",
      "Commands file for command [search google scholar current tab] exists: True\n",
      "Commands file for command [search google scholar new tab] exists: True\n",
      "Commands file for command [search kagi new tab] exists: True\n",
      "Commands file for command [search kagi current tab] exists: True\n",
      "Commands file for command [search perplexity current tab] exists: True\n",
      "Commands file for command [search perplexity new tab] exists: True\n",
      "Commands file for command [search phind current tab] exists: True\n",
      "Commands file for command [search phind new tab] exists: True\n",
      "\n",
      "Commands file for command [search using clipboard current tab] exists: True\n",
      "Commands file for command [search using clipboard new tab] exists: True\n",
      "Commands file for command [search google using clipboard current tab] exists: True\n",
      "Commands file for command [search google using clipboard new tab] exists: True\n",
      "Commands file for command [search google scholar using clipboard current tab] exists: True\n",
      "Commands file for command [search google scholar using clipboard new tab] exists: True\n",
      "Commands file for command [search kagi using clipboard current tab] exists: True\n",
      "Commands file for command [search kagi using clipboard new tab] exists: True\n",
      "Commands file for command [search perplexity using clipboard current tab] exists: True\n",
      "Commands file for command [search perplexity using clipboard new tab] exists: True\n",
      "Commands file for command [search phind using clipboard current tab] exists: True\n",
      "Commands file for command [search phind using clipboard new tab] exists: True\n",
      "Commands file for command [none] exists: True\n",
      "\n",
      "Commands file for command [agent router go to search function mapping] exists: True\n",
      "\n",
      "Commands file for command [agent router go to date and time] exists: True\n",
      "Commands file for command [agent router go to weather] exists: True\n",
      "Commands file for command [agent router go to calendar] exists: True\n",
      "Commands file for command [agent router go to receptionist] exists: True\n",
      "\n",
      "Commands file for command [agent router go to todo list] exists: True\n",
      "Commands file for command [agent router go to math] exists: True\n",
      "Commands file for command [none] exists: True\n",
      "\n",
      "Generating responses for 10 rows...\n",
      "Using HuggingFace model_name [ministral/Ministral-8B-Instruct-2410] in memory...\n",
      "\n",
      "Processing call [001] out of [10] = [10.0%]... ETA: 0 seconds\n",
      "Asking LLM [ministral/Ministral-8B-Instruct-2410]...\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Stop Token IDs \n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "1\n",
      "Asking LLM [ministral/Ministral-8B-Instruct-2410]... Done! in 4,974 ms\n",
      "Tokens per second [6.8] input tokens [604] + xml response tokens [34] = total tokens i/o [638]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Raw response\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "        <response>\n",
      "            <command>search google new tab</command>\n",
      "            <args>Connection Refused Error</args>\n",
      "        </response>\n",
      "    </s><s>\n",
      "Response: [<response><command>search google new tab</command><args>Connection Refused Error</args></response>]\n",
      "\n",
      "Processing call [002] out of [10] = [20.0%]... ETA: 19 seconds\n",
      "Asking LLM [ministral/Ministral-8B-Instruct-2410]...\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Stop Token IDs \n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "1\n",
      "Asking LLM [ministral/Ministral-8B-Instruct-2410]... Done! in 4,932 ms\n",
      "Tokens per second [6.9] input tokens [375] + xml response tokens [34] = total tokens i/o [409]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Raw response\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "        <response>\n",
      "            <command>agent router go to receptionist</command>\n",
      "            <args>Info Clerk</args>\n",
      "        </response>\n",
      "    </s><s>\n",
      "Response: [<response><command>agent router go to receptionist</command><args>Info Clerk</args></response>]\n",
      "\n",
      "Processing call [003] out of [10] = [30.0%]... ETA: 23 seconds\n",
      "Asking LLM [ministral/Ministral-8B-Instruct-2410]...\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Stop Token IDs \n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "1\n",
      "Asking LLM [ministral/Ministral-8B-Instruct-2410]... Done! in 5,979 ms\n",
      "Tokens per second [6.9] input tokens [607] + xml response tokens [41] = total tokens i/o [648]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Raw response\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "        <response>\n",
      "            <command>search kagi current tab</command>\n",
      "            <args>Unbound Local Error: Local variable referenced before assignment</args>\n",
      "        </response>\n",
      "    </s><s>\n",
      "Response: [<response><command>search kagi current tab</command><args>Unbound Local Error: Local variable referenced before assignment</args></response>]\n",
      "\n",
      "Processing call [004] out of [10] = [40.0%]... ETA: 23 seconds\n",
      "Asking LLM [ministral/Ministral-8B-Instruct-2410]...\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Stop Token IDs \n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "1\n",
      "Asking LLM [ministral/Ministral-8B-Instruct-2410]... Done! in 6,125 ms\n",
      "Tokens per second [6.9] input tokens [612] + xml response tokens [42] = total tokens i/o [654]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Raw response\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "        <response>\n",
      "            <command>search google current tab</command>\n",
      "            <args>How can system exit errors be handled gracefully in Python applications?</args>\n",
      "        </response>\n",
      "    </s><s>\n",
      "Response: [<response><command>search google current tab</command><args>How can system exit errors be handled gracefully in Python applications?</args></response>]\n",
      "\n",
      "Processing call [005] out of [10] = [50.0%]... ETA: 22 seconds\n",
      "Asking LLM [ministral/Ministral-8B-Instruct-2410]...\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Stop Token IDs \n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[36], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m stats_df \u001B[38;5;241m=\u001B[39m \u001B[43mrun_validation\u001B[49m\u001B[43m(\u001B[49m\u001B[43m \u001B[49m\u001B[43mgptq_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgptq_tokenizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mministral/Ministral-8B-Instruct-2410\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdebug\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m stats_df\n",
      "Cell \u001B[0;32mIn[35], line 16\u001B[0m, in \u001B[0;36mrun_validation\u001B[0;34m(model, tokenizer, model_name, device, sample_size, debug, verbose)\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28mprint\u001B[39m( df\u001B[38;5;241m.\u001B[39mcommand\u001B[38;5;241m.\u001B[39mvalue_counts(), end\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m )\n\u001B[1;32m     14\u001B[0m xml_ftp_generator \u001B[38;5;241m=\u001B[39m XmlFineTuningPromptGenerator( path_prefix\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/var/model/genie-in-the-box\u001B[39m\u001B[38;5;124m\"\u001B[39m, debug\u001B[38;5;241m=\u001B[39mdebug, verbose\u001B[38;5;241m=\u001B[39mverbose )\n\u001B[0;32m---> 16\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[43mxml_ftp_generator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate_responses\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     17\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mswitch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mhuggingface\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdebug\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdebug\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\n\u001B[1;32m     18\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     19\u001B[0m df \u001B[38;5;241m=\u001B[39m xml_ftp_generator\u001B[38;5;241m.\u001B[39mvalidate_responses( df )\n\u001B[1;32m     21\u001B[0m xml_ftp_generator\u001B[38;5;241m.\u001B[39mprint_validation_stats( df, title\u001B[38;5;241m=\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mValidation stats for model \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m )\n",
      "File \u001B[0;32m/var/model/genie-in-the-box/src/cosa/training/xml_fine_tuning_prompt_generator.py:1138\u001B[0m, in \u001B[0;36mXmlFineTuningPromptGenerator.generate_responses\u001B[0;34m(self, df, tokenizer, model, switch, model_name, max_new_tokens, temperature, top_k, top_p, device, debug, verbose, silent)\u001B[0m\n\u001B[1;32m   1136\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m switch \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhuggingface\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m   1137\u001B[0m     \u001B[38;5;28mprint\u001B[39m( \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUsing HuggingFace model_name [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m] in memory...\u001B[39m\u001B[38;5;124m\"\u001B[39m, end\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m )\n\u001B[0;32m-> 1138\u001B[0m     df[ \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mresponse\u001B[39m\u001B[38;5;124m\"\u001B[39m ]  \u001B[38;5;241m=\u001B[39m \u001B[43mdf\u001B[49m\u001B[43m[\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mprompt\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mcell\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_response_to_prompt\u001B[49m\u001B[43m(\u001B[49m\u001B[43m \u001B[49m\u001B[43mcell\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrows\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mswitch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mswitch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_new_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_new_tokens\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtemperature\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_k\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtop_k\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_p\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtop_p\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msilent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msilent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdebug\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdebug\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m \u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1139\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1140\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m( \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnknown switch [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mswitch\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m]\u001B[39m\u001B[38;5;124m\"\u001B[39m )\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py:4924\u001B[0m, in \u001B[0;36mSeries.apply\u001B[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001B[0m\n\u001B[1;32m   4789\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply\u001B[39m(\n\u001B[1;32m   4790\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   4791\u001B[0m     func: AggFuncType,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   4796\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m   4797\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame \u001B[38;5;241m|\u001B[39m Series:\n\u001B[1;32m   4798\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   4799\u001B[0m \u001B[38;5;124;03m    Invoke function on values of Series.\u001B[39;00m\n\u001B[1;32m   4800\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   4915\u001B[0m \u001B[38;5;124;03m    dtype: float64\u001B[39;00m\n\u001B[1;32m   4916\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m   4917\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mSeriesApply\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   4918\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4919\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4920\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconvert_dtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert_dtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4921\u001B[0m \u001B[43m        \u001B[49m\u001B[43mby_row\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mby_row\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4922\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4923\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m-> 4924\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py:1427\u001B[0m, in \u001B[0;36mSeriesApply.apply\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1424\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply_compat()\n\u001B[1;32m   1426\u001B[0m \u001B[38;5;66;03m# self.func is Callable\u001B[39;00m\n\u001B[0;32m-> 1427\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_standard\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py:1507\u001B[0m, in \u001B[0;36mSeriesApply.apply_standard\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1501\u001B[0m \u001B[38;5;66;03m# row-wise access\u001B[39;00m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m \u001B[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001B[39;00m\n\u001B[1;32m   1504\u001B[0m \u001B[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001B[39;00m\n\u001B[1;32m   1505\u001B[0m \u001B[38;5;66;03m#  Categorical (GH51645).\u001B[39;00m\n\u001B[1;32m   1506\u001B[0m action \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mignore\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(obj\u001B[38;5;241m.\u001B[39mdtype, CategoricalDtype) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 1507\u001B[0m mapped \u001B[38;5;241m=\u001B[39m \u001B[43mobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_map_values\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1508\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmapper\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcurried\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mna_action\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maction\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert_dtype\u001B[49m\n\u001B[1;32m   1509\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1511\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(mapped) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(mapped[\u001B[38;5;241m0\u001B[39m], ABCSeries):\n\u001B[1;32m   1512\u001B[0m     \u001B[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001B[39;00m\n\u001B[1;32m   1513\u001B[0m     \u001B[38;5;66;03m#  See also GH#25959 regarding EA support\u001B[39;00m\n\u001B[1;32m   1514\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\u001B[38;5;241m.\u001B[39m_constructor_expanddim(\u001B[38;5;28mlist\u001B[39m(mapped), index\u001B[38;5;241m=\u001B[39mobj\u001B[38;5;241m.\u001B[39mindex)\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/base.py:921\u001B[0m, in \u001B[0;36mIndexOpsMixin._map_values\u001B[0;34m(self, mapper, na_action, convert)\u001B[0m\n\u001B[1;32m    918\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(arr, ExtensionArray):\n\u001B[1;32m    919\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m arr\u001B[38;5;241m.\u001B[39mmap(mapper, na_action\u001B[38;5;241m=\u001B[39mna_action)\n\u001B[0;32m--> 921\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43malgorithms\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43marr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapper\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mna_action\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mna_action\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/algorithms.py:1743\u001B[0m, in \u001B[0;36mmap_array\u001B[0;34m(arr, mapper, na_action, convert)\u001B[0m\n\u001B[1;32m   1741\u001B[0m values \u001B[38;5;241m=\u001B[39m arr\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mobject\u001B[39m, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m na_action \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1743\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_infer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapper\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1745\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m lib\u001B[38;5;241m.\u001B[39mmap_infer_mask(\n\u001B[1;32m   1746\u001B[0m         values, mapper, mask\u001B[38;5;241m=\u001B[39misna(values)\u001B[38;5;241m.\u001B[39mview(np\u001B[38;5;241m.\u001B[39muint8), convert\u001B[38;5;241m=\u001B[39mconvert\n\u001B[1;32m   1747\u001B[0m     )\n",
      "File \u001B[0;32mlib.pyx:2972\u001B[0m, in \u001B[0;36mpandas._libs.lib.map_infer\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m/var/model/genie-in-the-box/src/cosa/training/xml_fine_tuning_prompt_generator.py:1138\u001B[0m, in \u001B[0;36mXmlFineTuningPromptGenerator.generate_responses.<locals>.<lambda>\u001B[0;34m(cell)\u001B[0m\n\u001B[1;32m   1136\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m switch \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhuggingface\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m   1137\u001B[0m     \u001B[38;5;28mprint\u001B[39m( \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUsing HuggingFace model_name [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m] in memory...\u001B[39m\u001B[38;5;124m\"\u001B[39m, end\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m )\n\u001B[0;32m-> 1138\u001B[0m     df[ \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mresponse\u001B[39m\u001B[38;5;124m\"\u001B[39m ]  \u001B[38;5;241m=\u001B[39m df[ \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprompt\u001B[39m\u001B[38;5;124m\"\u001B[39m ]\u001B[38;5;241m.\u001B[39mapply( \u001B[38;5;28;01mlambda\u001B[39;00m cell: \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_response_to_prompt\u001B[49m\u001B[43m(\u001B[49m\u001B[43m \u001B[49m\u001B[43mcell\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrows\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mswitch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mswitch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_new_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_new_tokens\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtemperature\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_k\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtop_k\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_p\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtop_p\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msilent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msilent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdebug\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdebug\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m \u001B[49m\u001B[43m)\u001B[49m )\n\u001B[1;32m   1139\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1140\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m( \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnknown switch [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mswitch\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m]\u001B[39m\u001B[38;5;124m\"\u001B[39m )\n",
      "File \u001B[0;32m/var/model/genie-in-the-box/src/cosa/training/xml_fine_tuning_prompt_generator.py:1119\u001B[0m, in \u001B[0;36mXmlFineTuningPromptGenerator._get_response_to_prompt\u001B[0;34m(self, prompt, rows, switch, model_name, timer, tokenizer, model, max_new_tokens, temperature, top_k, top_p, device, silent, debug, verbose)\u001B[0m\n\u001B[1;32m   1117\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_query_llm_openai( prompt[ \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m ], model_name\u001B[38;5;241m=\u001B[39mmodel_name )\n\u001B[1;32m   1118\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m switch \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhuggingface\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m-> 1119\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_query_llm_in_memory\u001B[49m\u001B[43m(\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_new_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_new_tokens\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msilent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msilent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdebug\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdebug\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1120\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1121\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m( \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnknown switch [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mswitch\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m]\u001B[39m\u001B[38;5;124m\"\u001B[39m )\n",
      "File \u001B[0;32m/var/model/genie-in-the-box/src/cosa/training/xml_fine_tuning_prompt_generator.py:1018\u001B[0m, in \u001B[0;36mXmlFineTuningPromptGenerator._query_llm_in_memory\u001B[0;34m(self, tokenizer, model, prompt, max_new_tokens, model_name, device, silent, debug, verbose)\u001B[0m\n\u001B[1;32m   1015\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_query_llm_in_memory\u001B[39m( \u001B[38;5;28mself\u001B[39m, tokenizer, model, prompt, max_new_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1024\u001B[39m, model_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mACME LLMs, Inc.\u001B[39m\u001B[38;5;124m\"\u001B[39m, device\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda:0\u001B[39m\u001B[38;5;124m\"\u001B[39m, silent\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, debug\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m ):\n\u001B[1;32m   1016\u001B[0m     \n\u001B[1;32m   1017\u001B[0m     \u001B[38;5;66;03m# We need this exact method in other places too, so do the simplest extraction and reuse here\u001B[39;00m\n\u001B[0;32m-> 1018\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mdu_llm_client\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mquery_llm_in_memory\u001B[49m\u001B[43m(\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_new_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_new_tokens\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msilent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msilent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdebug\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdebug\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1020\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdebug:\n\u001B[1;32m   1021\u001B[0m         \u001B[38;5;28mprint\u001B[39m( \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mResponse: [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresponse\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m]\u001B[39m\u001B[38;5;124m\"\u001B[39m, end\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m )\n",
      "File \u001B[0;32m/var/model/genie-in-the-box/src/cosa/app/util_llm_client.py:16\u001B[0m, in \u001B[0;36mquery_llm_in_memory\u001B[0;34m(model, tokenizer, prompt, device, model_name, max_new_tokens, silent, debug, verbose)\u001B[0m\n\u001B[1;32m     13\u001B[0m du\u001B[38;5;241m.\u001B[39mprint_banner( \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mStop Token IDs \u001B[39m\u001B[38;5;124m\"\u001B[39m )\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28mprint\u001B[39m( stop_token_id )\n\u001B[0;32m---> 16\u001B[0m generation_output \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     17\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m[\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43minput_ids\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     18\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m[\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mattention_mask\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     19\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_new_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_new_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     20\u001B[0m \u001B[43m    \u001B[49m\u001B[43meos_token_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop_token_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     21\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpad_token_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop_token_id\u001B[49m\n\u001B[1;32m     22\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;66;03m# Skip decoding the prompt part of the output\u001B[39;00m\n\u001B[1;32m     25\u001B[0m input_length \u001B[38;5;241m=\u001B[39m inputs[ \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m ]\u001B[38;5;241m.\u001B[39msize( \u001B[38;5;241m1\u001B[39m )\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:116\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    114\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    115\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 116\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2215\u001B[0m, in \u001B[0;36mGenerationMixin.generate\u001B[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001B[0m\n\u001B[1;32m   2207\u001B[0m     input_ids, model_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_expand_inputs_for_generation(\n\u001B[1;32m   2208\u001B[0m         input_ids\u001B[38;5;241m=\u001B[39minput_ids,\n\u001B[1;32m   2209\u001B[0m         expand_size\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39mnum_return_sequences,\n\u001B[1;32m   2210\u001B[0m         is_encoder_decoder\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mis_encoder_decoder,\n\u001B[1;32m   2211\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_kwargs,\n\u001B[1;32m   2212\u001B[0m     )\n\u001B[1;32m   2214\u001B[0m     \u001B[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001B[39;00m\n\u001B[0;32m-> 2215\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sample\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2216\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2217\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlogits_processor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprepared_logits_processor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2218\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstopping_criteria\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprepared_stopping_criteria\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2219\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgeneration_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2220\u001B[0m \u001B[43m        \u001B[49m\u001B[43msynced_gpus\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msynced_gpus\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2221\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstreamer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstreamer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2222\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2223\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2225\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m generation_mode \u001B[38;5;129;01min\u001B[39;00m (GenerationMode\u001B[38;5;241m.\u001B[39mBEAM_SAMPLE, GenerationMode\u001B[38;5;241m.\u001B[39mBEAM_SEARCH):\n\u001B[1;32m   2226\u001B[0m     \u001B[38;5;66;03m# 11. prepare beam search scorer\u001B[39;00m\n\u001B[1;32m   2227\u001B[0m     beam_scorer \u001B[38;5;241m=\u001B[39m BeamSearchScorer(\n\u001B[1;32m   2228\u001B[0m         batch_size\u001B[38;5;241m=\u001B[39mbatch_size,\n\u001B[1;32m   2229\u001B[0m         num_beams\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39mnum_beams,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2234\u001B[0m         max_length\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39mmax_length,\n\u001B[1;32m   2235\u001B[0m     )\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:3195\u001B[0m, in \u001B[0;36mGenerationMixin._sample\u001B[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001B[0m\n\u001B[1;32m   3192\u001B[0m unfinished_sequences \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mones(batch_size, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mlong, device\u001B[38;5;241m=\u001B[39minput_ids\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m   3193\u001B[0m model_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_initial_cache_position(input_ids, model_kwargs)\n\u001B[0;32m-> 3195\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_has_unfinished_sequences\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   3196\u001B[0m \u001B[43m    \u001B[49m\u001B[43mthis_peer_finished\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msynced_gpus\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcur_len\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcur_len\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_length\u001B[49m\n\u001B[1;32m   3197\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m   3198\u001B[0m     \u001B[38;5;66;03m# prepare model inputs\u001B[39;00m\n\u001B[1;32m   3199\u001B[0m     model_inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprepare_inputs_for_generation(input_ids, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_kwargs)\n\u001B[1;32m   3201\u001B[0m     \u001B[38;5;66;03m# prepare variable output controls (note: some models won't accept all output controls)\u001B[39;00m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2413\u001B[0m, in \u001B[0;36mGenerationMixin._has_unfinished_sequences\u001B[0;34m(self, this_peer_finished, synced_gpus, device, cur_len, max_length)\u001B[0m\n\u001B[1;32m   2411\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m this_peer_finished_flag\u001B[38;5;241m.\u001B[39mitem() \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0.0\u001B[39m:\n\u001B[1;32m   2412\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m-> 2413\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m this_peer_finished:\n\u001B[1;32m   2414\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m   2415\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "stats_df = run_validation( gptq_model, gptq_tokenizer, model_name=\"ministral/Ministral-8B-Instruct-2410\", sample_size=10, debug=True, verbose=True )\n",
    "stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3af60857cd0d920a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T02:35:55.258462Z",
     "start_time": "2025-01-14T02:35:55.010833Z"
    }
   },
   "outputs": [],
   "source": [
    "reset_kernel( [ gptq_tokenizer, gptq_model ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d551a71fe0142cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T16:04:05.053669Z",
     "start_time": "2025-01-13T16:04:05.027366Z"
    }
   },
   "outputs": [],
   "source": [
    "reset_environment()\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fedef541900b6d7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T19:56:58.792584Z",
     "start_time": "2024-01-24T19:56:58.704366Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# !pip install autoawq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6f37e59e4c4f0db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T21:32:30.864636Z",
     "start_time": "2024-12-18T21:32:30.859030Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/var/model/models/Ministral-3b-instruct\n"
     ]
    }
   ],
   "source": [
    "os.chdir( f\"{models_root}/Ministral-3b-instruct/\" )\n",
    "print( os.getcwd() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0c18bea4f9438df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T16:17:03.692121Z",
     "start_time": "2025-01-13T16:06:06.163874Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0417385df9a14901bf17d39f0afc4838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We've detected an older driver with an RTX 4000 series GPU. These drivers have issues with P2P. This can affect the multi-gpu inference when using accelerate device_map.Please make sure to update your driver to the latest version which resolves this.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da4f81288929428986a577e8736c18a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/167 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4ae367278d34a8692bd72fe9b73e8d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val.jsonl.zst:   0%|          | 0.00/471M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6abf90d99854d9d859d83ff9bfac89d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/214670 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AWQ: 100%|| 36/36 [10:28<00:00, 17.46s/it]\n"
     ]
    }
   ],
   "source": [
    "from awq          import AutoAWQForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "quant_config = { \"zero_point\": True, \"q_group_size\": 128, \"w_bit\": 4 }\n",
    "\n",
    "# Load model and tokenizer\n",
    "raw_16bit_model     = AutoAWQForCausalLM.from_pretrained( merged_path, device_map=\"auto\", safetensors=True )\n",
    "raw_16bit_tokenizer = AutoTokenizer.from_pretrained( merged_path, use_fast=True )\n",
    "\n",
    "# Quantize\n",
    "raw_16bit_model.quantize( raw_16bit_tokenizer, quant_config=quant_config )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3da12541a98d521",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T16:29:22.861728Z",
     "start_time": "2025-01-13T16:29:17.999111Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./merged-00-2025-01-09.awq/tokenizer_config.json',\n",
       " './merged-00-2025-01-09.awq/special_tokens_map.json',\n",
       " './merged-00-2025-01-09.awq/tokenizer.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save quantized model\n",
    "awq_path = merged_path + \".awq\"\n",
    "raw_16bit_model.save_quantized( awq_path, safetensors=True )\n",
    "raw_16bit_tokenizer.save_pretrained( awq_path )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fedea5a0e76d310",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## GPU RAM after quantizing model with 4bit AWQ\n",
    "```\n",
    "Wed Jan 24 12:06:36 2024\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  NVIDIA GeForce RTX 4090        On  | 00000000:01:00.0 Off |                  Off |\n",
    "|  0%   37C    P8              27W / 450W |   1320MiB / 24564MiB |      0%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "|   1  NVIDIA GeForce RTX 4090        On  | 00000000:02:00.0 Off |                  Off |\n",
    "|  0%   44C    P8              24W / 450W |   2084MiB / 24564MiB |      0%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "|    0   N/A  N/A     15181      C   /usr/bin/python3                           1310MiB |\n",
    "|    1   N/A  N/A     15181      C   /usr/bin/python3                           2074MiB |\n",
    "+---------------------------------------------------------------------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1887818bfac3413f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Validate AWQ model: In memory loaded by Jupiter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2fce43702d31a43",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T16:30:36.219574Z",
     "start_time": "2025-01-13T16:30:36.063778Z"
    }
   },
   "outputs": [],
   "source": [
    "reset_kernel( [ raw_16bit_model, raw_16bit_tokenizer ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97aefea211110c86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T16:31:30.449130Z",
     "start_time": "2025-01-13T16:31:27.952502Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "/var/model/genie-in-the-box/src\n",
      "/var/model/genie-in-the-box/src\n"
     ]
    }
   ],
   "source": [
    "reset_environment()\n",
    "set_gib_env_vars()\n",
    "import os\n",
    "\n",
    "print( os.getcwd() )\n",
    "os.chdir( \"/var/model/genie-in-the-box/src\" )\n",
    "print( os.getcwd() )\n",
    "import cosa.utils.util         as du\n",
    "import cosa.utils.util_xml     as dux\n",
    "import cosa.utils.util_pytorch as dupt\n",
    "\n",
    "from cosa.training.xml_fine_tuning_prompt_generator import XmlFineTuningPromptGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78536f70c1a952e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T20:56:35.193524Z",
     "start_time": "2025-01-13T20:56:35.187891Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/var/model/models/Ministral-8B-Instruct-2410\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir( f\"{models_root}/{model_name}/\" )\n",
    "print( os.getcwd() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "681b8bcb2278ffb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T19:54:40.549198Z",
     "start_time": "2025-01-13T19:54:40.431727Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 5.4G\r\n",
      "drwxr-xr-x 2 root root 4.0K Jan 13 16:29 .\r\n",
      "drwxrwxr-x 6 1001 1001 4.0K Jan 13 18:04 ..\r\n",
      "-rw-r--r-- 1 root root  843 Jan 13 16:29 config.json\r\n",
      "-rw-r--r-- 1 root root  132 Jan 13 16:29 generation_config.json\r\n",
      "-rw-r--r-- 1 root root 4.4G Jan 13 16:29 model-00001-of-00002.safetensors\r\n",
      "-rw-r--r-- 1 root root 1.1G Jan 13 16:29 model-00002-of-00002.safetensors\r\n",
      "-rw-r--r-- 1 root root  67K Jan 13 16:29 model.safetensors.index.json\r\n",
      "-rw-r--r-- 1 root root  551 Jan 13 16:29 special_tokens_map.json\r\n",
      "-rw-r--r-- 1 root root  17M Jan 13 16:29 tokenizer.json\r\n",
      "-rw-r--r-- 1 root root 178K Jan 13 16:29 tokenizer_config.json\r\n"
     ]
    }
   ],
   "source": [
    "! ls -alh /var/model/models/Ministral-8B-Instruct-2410/merged-00-2025-01-09.awq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5e35ea922857196",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T16:33:39.690369Z",
     "start_time": "2025-01-13T16:33:37.841187Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5002a76bd9f448dad71c565ce42c357",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from awq          import AutoAWQForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "awq_path      = f\"./merged-00-{merge_date}.awq\"\n",
    "model_aqw     = AutoAWQForCausalLM.from_pretrained( awq_path, device_map=\"cuda:1\", safetensors=True )\n",
    "tokenizer_awq = AutoTokenizer.from_pretrained( awq_path, use_fast=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c45b4ebe0d4d18ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T16:37:10.728183Z",
     "start_time": "2025-01-13T16:34:17.064748Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Validating ministral/Ministral-3b-instruct w/ 100 samples\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "command\n",
      "go to new tab                                        8\n",
      "search perplexity new tab                            7\n",
      "search google new tab                                7\n",
      "agent router go to calendar                          7\n",
      "search perplexity current tab                        7\n",
      "go to current tab                                    6\n",
      "search google scholar current tab                    6\n",
      "search kagi new tab                                  6\n",
      "search kagi current tab                              5\n",
      "search phind new tab                                 5\n",
      "search google current tab                            5\n",
      "agent router go to receptionist                      4\n",
      "agent router go to date and time                     4\n",
      "agent router go to weather                           3\n",
      "agent router go to todo list                         3\n",
      "search google scholar new tab                        3\n",
      "search new tab                                       3\n",
      "search phind current tab                             2\n",
      "search google scholar using clipboard current tab    2\n",
      "agent router go to math                              2\n",
      "search current tab                                   2\n",
      "search phind using clipboard new tab                 2\n",
      "search phind using clipboard current tab             1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Instantiating ConfigurationManager() singleton...\n",
      "\n",
      "Using environment variables to instantiate configuration manager\n",
      "[0]th arg = [config_path=/src/conf/gib-app.ini]... done!\n",
      "[1]th arg = [splainer_path=/src/conf/gib-app-splainer.ini]... done!\n",
      "[2]th arg = [config_block_id=Genie+in+the+Box:+Development]... done!\n",
      "\n",
      "Name value dictionary pairs:\n",
      "\n",
      "[ config_block_id] = [Genie in the Box: Development]\n",
      "[     config_path] = [/src/conf/gib-app.ini]\n",
      "[   splainer_path] = [/src/conf/gib-app-splainer.ini]\n",
      "\n",
      "File exists! [/var/model/genie-in-the-box/src/conf/gib-app.ini]\n",
      "File exists! [/var/model/genie-in-the-box/src/conf/gib-app-splainer.ini]\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Initializing configuration_manager [/var/model/genie-in-the-box/src/conf/gib-app.ini]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Splainer path [/var/model/genie-in-the-box/src/conf/gib-app-splainer.ini]\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Sections, '*' = current block ID\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "  Genie in the Box: Baseline\n",
      "* Genie in the Box: Development\n",
      "  Genie in the Box: Production\n",
      "  default\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Calculating inheritance... * = parent block\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "* [Genie in the Box: Development] inherits from [Genie in the Box: Baseline]\n",
      "Scanning for immutable keys...\n",
      "Scanning for immutable keys... Done!\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Calculating defaults...\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Loading splainer file [/var/model/genie-in-the-box/src/conf/gib-app-splainer.ini]...\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Inserting DUPLICATE placeholders into the list. Requested length [500] > list length [74]\n",
      "Inserting DUPLICATE placeholders into the list. Requested length [500] > list length [148]\n",
      "Inserting DUPLICATE placeholders into the list. Requested length [500] > list length [296]\n",
      "Commands file for command [go to current tab] exists: True\n",
      "Commands file for command [go to new tab] exists: True\n",
      "Commands file for command [search current tab] exists: True\n",
      "Commands file for command [search new tab] exists: True\n",
      "Commands file for command [search google current tab] exists: True\n",
      "Commands file for command [search google new tab] exists: True\n",
      "Commands file for command [search google scholar current tab] exists: True\n",
      "Commands file for command [search google scholar new tab] exists: True\n",
      "Commands file for command [search kagi new tab] exists: True\n",
      "Commands file for command [search kagi current tab] exists: True\n",
      "Commands file for command [search perplexity current tab] exists: True\n",
      "Commands file for command [search perplexity new tab] exists: True\n",
      "Commands file for command [search phind current tab] exists: True\n",
      "Commands file for command [search phind new tab] exists: True\n",
      "\n",
      "Commands file for command [search using clipboard current tab] exists: True\n",
      "Commands file for command [search using clipboard new tab] exists: True\n",
      "Commands file for command [search google using clipboard current tab] exists: True\n",
      "Commands file for command [search google using clipboard new tab] exists: True\n",
      "Commands file for command [search google scholar using clipboard current tab] exists: True\n",
      "Commands file for command [search google scholar using clipboard new tab] exists: True\n",
      "Commands file for command [search kagi using clipboard current tab] exists: True\n",
      "Commands file for command [search kagi using clipboard new tab] exists: True\n",
      "Commands file for command [search perplexity using clipboard current tab] exists: True\n",
      "Commands file for command [search perplexity using clipboard new tab] exists: True\n",
      "Commands file for command [search phind using clipboard current tab] exists: True\n",
      "Commands file for command [search phind using clipboard new tab] exists: True\n",
      "Commands file for command [none] exists: True\n",
      "\n",
      "Commands file for command [agent router go to search function mapping] exists: True\n",
      "\n",
      "Commands file for command [agent router go to date and time] exists: True\n",
      "Commands file for command [agent router go to weather] exists: True\n",
      "Commands file for command [agent router go to calendar] exists: True\n",
      "Commands file for command [agent router go to receptionist] exists: True\n",
      "\n",
      "Commands file for command [agent router go to todo list] exists: True\n",
      "Commands file for command [agent router go to math] exists: True\n",
      "Commands file for command [none] exists: True\n",
      "\n",
      "Generating responses for 100 rows...\n",
      "Using HuggingFace model_name [ministral/Ministral-3b-instruct] in memory...\n",
      "\n",
      "Processing call [001] out of [100] = [1.0%]... ETA: 0 seconds\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 3,513 ms\n",
      "Tokens per second [9.7] input tokens [604] + xml response tokens [34] = total tokens i/o [638]\n",
      "Response: [<response><command>search google new tab</command><args>Connection Refused Error</args></response>]\n",
      "\n",
      "Processing call [002] out of [100] = [2.0%]... ETA mm:ss 2:52\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,545 ms\n",
      "Tokens per second [22.0] input tokens [375] + xml response tokens [34] = total tokens i/o [409]\n",
      "Response: [<response><command>agent router go to receptionist</command><args>Info Clerk</args></response>]\n",
      "\n",
      "Processing call [003] out of [100] = [3.0%]... ETA mm:ss 2:43\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,905 ms\n",
      "Tokens per second [21.5] input tokens [607] + xml response tokens [41] = total tokens i/o [648]\n",
      "Response: [<response><command>search kagi current tab</command><args>Unbound Local Error: Local variable referenced before assignment</args></response>]\n",
      "\n",
      "Processing call [004] out of [100] = [4.0%]... ETA mm:ss 2:47\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,952 ms\n",
      "Tokens per second [21.5] input tokens [612] + xml response tokens [42] = total tokens i/o [654]\n",
      "Response: [<response><command>search google current tab</command><args>How can system exit errors be handled gracefully in Python applications?</args></response>]\n",
      "\n",
      "Processing call [005] out of [100] = [5.0%]... ETA mm:ss 2:49\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 2,124 ms\n",
      "Tokens per second [21.7] input tokens [618] + xml response tokens [46] = total tokens i/o [664]\n",
      "Response: [<response><command>search perplexity new tab</command><args>AI for health diagnostics: How does AI contribute to advancements in health diagnostics?</args></response>]\n",
      "\n",
      "Processing call [006] out of [100] = [6.0%]... ETA mm:ss 2:52\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,776 ms\n",
      "Tokens per second [21.4] input tokens [607] + xml response tokens [38] = total tokens i/o [645]\n",
      "Response: [<response><command>search kagi current tab</command><args>Saving Pandas DataFrame to CSV</args></response>]\n",
      "\n",
      "Processing call [007] out of [100] = [7.0%]... ETA mm:ss 2:50\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,518 ms\n",
      "Tokens per second [21.1] input tokens [612] + xml response tokens [32] = total tokens i/o [644]\n",
      "Response: [<response><command>search google scholar using clipboard current tab</command><args></args></response>]\n",
      "\n",
      "Processing call [008] out of [100] = [8.0%]... ETA mm:ss 2:44\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 2,129 ms\n",
      "Tokens per second [21.6] input tokens [616] + xml response tokens [46] = total tokens i/o [662]\n",
      "Response: [<response><command>search phind current tab</command><args>What are bytes warnings in Python, and how are they significant in data handling?</args></response>]\n",
      "\n",
      "Processing call [009] out of [100] = [9.0%]... ETA mm:ss 2:46\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,716 ms\n",
      "Tokens per second [22.1] input tokens [369] + xml response tokens [38] = total tokens i/o [407]\n",
      "Response: [<response><command>agent router go to date and time</command><args>Fremont, California</args></response>]\n",
      "\n",
      "Processing call [010] out of [100] = [10.0%]... ETA mm:ss 2:43\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,729 ms\n",
      "Tokens per second [21.4] input tokens [605] + xml response tokens [37] = total tokens i/o [642]\n",
      "Response: [<response><command>go to current tab</command><args>login.excitingunicorn.net</args></response>]\n",
      "\n",
      "Processing call [011] out of [100] = [11.0%]... ETA mm:ss 2:41\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,365 ms\n",
      "Tokens per second [22.0] input tokens [361] + xml response tokens [30] = total tokens i/o [391]\n",
      "Response: [<response><command>agent router go to calendar</command><args></args></response>]\n",
      "\n",
      "Processing call [012] out of [100] = [12.0%]... ETA mm:ss 2:36\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,639 ms\n",
      "Tokens per second [21.4] input tokens [605] + xml response tokens [35] = total tokens i/o [640]\n",
      "Response: [<response><command>search kagi new tab</command><args>Connection Refused Error</args></response>]\n",
      "\n",
      "Processing call [013] out of [100] = [13.0%]... ETA mm:ss 2:33\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,601 ms\n",
      "Tokens per second [21.2] input tokens [599] + xml response tokens [34] = total tokens i/o [633]\n",
      "Response: [<response><command>search perplexity current tab</command><args>Broken Pipe Error</args></response>]\n",
      "\n",
      "Processing call [014] out of [100] = [14.0%]... ETA mm:ss 2:30\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,555 ms\n",
      "Tokens per second [21.2] input tokens [606] + xml response tokens [33] = total tokens i/o [639]\n",
      "Response: [<response><command>search kagi new tab</command><args>ImportWarning</args></response>]\n",
      "\n",
      "Processing call [015] out of [100] = [15.0%]... ETA mm:ss 2:27\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,641 ms\n",
      "Tokens per second [21.3] input tokens [607] + xml response tokens [35] = total tokens i/o [642]\n",
      "Response: [<response><command>search google new tab</command><args>best movies of all time</args></response>]\n",
      "\n",
      "Processing call [016] out of [100] = [16.0%]... ETA mm:ss 2:25\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,998 ms\n",
      "Tokens per second [21.5] input tokens [609] + xml response tokens [43] = total tokens i/o [652]\n",
      "Response: [<response><command>search google scholar current tab</command><args>How do you fix indentation errors that affect code structure in Python?</args></response>]\n",
      "\n",
      "Processing call [017] out of [100] = [17.0%]... ETA mm:ss 2:25\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,796 ms\n",
      "Tokens per second [20.6] input tokens [606] + xml response tokens [37] = total tokens i/o [643]\n",
      "Response: [<response><command>go to new tab</command><args>blog.jubilantquartz.com</args></response>]\n",
      "\n",
      "Processing call [018] out of [100] = [18.0%]... ETA mm:ss 2:23\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,678 ms\n",
      "Tokens per second [21.5] input tokens [371] + xml response tokens [36] = total tokens i/o [407]\n",
      "Response: [<response><command>agent router go to weather</command><args>Brussels, Belgium</args></response>]\n",
      "\n",
      "Processing call [019] out of [100] = [19.0%]... ETA mm:ss 2:21\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,412 ms\n",
      "Tokens per second [22.0] input tokens [364] + xml response tokens [31] = total tokens i/o [395]\n",
      "Response: [<response><command>agent router go to todo list</command><args></args></response>]\n",
      "\n",
      "Processing call [020] out of [100] = [20.0%]... ETA mm:ss 2:18\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,367 ms\n",
      "Tokens per second [21.9] input tokens [378] + xml response tokens [30] = total tokens i/o [408]\n",
      "Response: [<response><command>agent router go to calendar</command><args></args></response>]\n",
      "\n",
      "Processing call [021] out of [100] = [21.0%]... ETA mm:ss 2:15\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 2,010 ms\n",
      "Tokens per second [21.4] input tokens [615] + xml response tokens [43] = total tokens i/o [658]\n",
      "Response: [<response><command>search phind new tab</command><args>File Not Found Error: File not found or path is incorrect</args></response>]\n",
      "\n",
      "Processing call [022] out of [100] = [22.0%]... ETA mm:ss 2:14\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,685 ms\n",
      "Tokens per second [21.4] input tokens [606] + xml response tokens [36] = total tokens i/o [642]\n",
      "Response: [<response><command>search google new tab</command><args>how to make homemade bread</args></response>]\n",
      "\n",
      "Processing call [023] out of [100] = [23.0%]... ETA mm:ss 2:12\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 2,223 ms\n",
      "Tokens per second [21.1] input tokens [615] + xml response tokens [47] = total tokens i/o [662]\n",
      "Response: [<response><command>search google new tab</command><args>Neural Network hyperparameter tuning: What approaches are recommended for neural network hyperparameter tuning?</args></response>]\n",
      "\n",
      "Processing call [024] out of [100] = [24.0%]... ETA mm:ss 2:12\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 2,084 ms\n",
      "Tokens per second [21.6] input tokens [618] + xml response tokens [45] = total tokens i/o [663]\n",
      "Response: [<response><command>search phind new tab</command><args>What are common causes and solutions for errors related to incorrect syntax in Python?</args></response>]\n",
      "\n",
      "Processing call [025] out of [100] = [25.0%]... ETA mm:ss 2:11\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,781 ms\n",
      "Tokens per second [21.3] input tokens [605] + xml response tokens [38] = total tokens i/o [643]\n",
      "Response: [<response><command>go to current tab</command><args>beta.remarkablezebra.info</args></response>]\n",
      "\n",
      "Processing call [026] out of [100] = [26.0%]... ETA mm:ss 2:10\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,987 ms\n",
      "Tokens per second [21.6] input tokens [612] + xml response tokens [43] = total tokens i/o [655]\n",
      "Response: [<response><command>search perplexity current tab</command><args>How can you resolve errors related to ZIP file operations in Python?</args></response>]\n",
      "\n",
      "Processing call [027] out of [100] = [27.0%]... ETA mm:ss 2:09\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,722 ms\n",
      "Tokens per second [21.5] input tokens [609] + xml response tokens [37] = total tokens i/o [646]\n",
      "Response: [<response><command>search phind new tab</command><args>Reading CSV files in Pandas</args></response>]\n",
      "\n",
      "Processing call [028] out of [100] = [28.0%]... ETA mm:ss 2:07\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,443 ms\n",
      "Tokens per second [21.5] input tokens [367] + xml response tokens [31] = total tokens i/o [398]\n",
      "Response: [<response><command>agent router go to todo list</command><args></args></response>]\n",
      "\n",
      "Processing call [029] out of [100] = [29.0%]... ETA mm:ss 2:04\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,844 ms\n",
      "Tokens per second [19.5] input tokens [608] + xml response tokens [36] = total tokens i/o [644]\n",
      "Response: [<response><command>search google new tab</command><args>JavaScript libraries for beginners</args></response>]\n",
      "\n",
      "Processing call [030] out of [100] = [30.0%]... ETA mm:ss 2:03\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,732 ms\n",
      "Tokens per second [21.4] input tokens [604] + xml response tokens [37] = total tokens i/o [641]\n",
      "Response: [<response><command>go to new tab</command><args>login.fantasticvolcano.org</args></response>]\n",
      "\n",
      "Processing call [031] out of [100] = [31.0%]... ETA mm:ss 2:01\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,731 ms\n",
      "Tokens per second [21.4] input tokens [605] + xml response tokens [37] = total tokens i/o [642]\n",
      "Response: [<response><command>search perplexity current tab</command><args>best vacation spots in the world</args></response>]\n",
      "\n",
      "Processing call [032] out of [100] = [32.0%]... ETA mm:ss 1:59\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,589 ms\n",
      "Tokens per second [22.0] input tokens [363] + xml response tokens [35] = total tokens i/o [398]\n",
      "Response: [<response><command>agent router go to receptionist</command><args>Office Coordinator Agent</args></response>]\n",
      "\n",
      "Processing call [033] out of [100] = [33.0%]... ETA mm:ss 1:57\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,366 ms\n",
      "Tokens per second [22.0] input tokens [361] + xml response tokens [30] = total tokens i/o [391]\n",
      "Response: [<response><command>agent router go to math</command><args></args></response>]\n",
      "\n",
      "Processing call [034] out of [100] = [34.0%]... ETA mm:ss 1:54\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,553 ms\n",
      "Tokens per second [21.2] input tokens [599] + xml response tokens [33] = total tokens i/o [632]\n",
      "Response: [<response><command>search google current tab</command><args>best online games</args></response>]\n",
      "\n",
      "Processing call [035] out of [100] = [35.0%]... ETA mm:ss 1:52\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 2,214 ms\n",
      "Tokens per second [21.7] input tokens [620] + xml response tokens [48] = total tokens i/o [668]\n",
      "Response: [<response><command>search phind new tab</command><args>Google Cloud AI services: What AI services does Google Cloud offer for developers and data scientists?</args></response>]\n",
      "\n",
      "Processing call [036] out of [100] = [36.0%]... ETA mm:ss 1:51\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 2,083 ms\n",
      "Tokens per second [21.1] input tokens [613] + xml response tokens [44] = total tokens i/o [657]\n",
      "Response: [<response><command>search kagi current tab</command><args>Deep Learning optimization: What strategies are used for optimizing deep learning models?</args></response>]\n",
      "\n",
      "Processing call [037] out of [100] = [37.0%]... ETA mm:ss 1:50\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,777 ms\n",
      "Tokens per second [21.4] input tokens [607] + xml response tokens [38] = total tokens i/o [645]\n",
      "Response: [<response><command>go to new tab</command><args>blog.hilariouspenguin.gov</args></response>]\n",
      "\n",
      "Processing call [038] out of [100] = [38.0%]... ETA mm:ss 1:49\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,773 ms\n",
      "Tokens per second [21.4] input tokens [607] + xml response tokens [38] = total tokens i/o [645]\n",
      "Response: [<response><command>search google scholar new tab</command><args>Type Error: Incorrect type comparison</args></response>]\n",
      "\n",
      "Processing call [039] out of [100] = [39.0%]... ETA mm:ss 1:47\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,725 ms\n",
      "Tokens per second [21.4] input tokens [606] + xml response tokens [37] = total tokens i/o [643]\n",
      "Response: [<response><command>go to new tab</command><args>login.hilariousrainbow.io</args></response>]\n",
      "\n",
      "Processing call [040] out of [100] = [40.0%]... ETA mm:ss 1:45\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,730 ms\n",
      "Tokens per second [21.4] input tokens [603] + xml response tokens [37] = total tokens i/o [640]\n",
      "Response: [<response><command>search google scholar current tab</command><args>JavaScript libraries for beginners</args></response>]\n",
      "\n",
      "Processing call [041] out of [100] = [41.0%]... ETA mm:ss 1:43\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,730 ms\n",
      "Tokens per second [21.4] input tokens [602] + xml response tokens [37] = total tokens i/o [639]\n",
      "Response: [<response><command>search google scholar current tab</command><args>Neural Network hyperparameter tuning</args></response>]\n",
      "\n",
      "Processing call [042] out of [100] = [42.0%]... ETA mm:ss 1:41\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,598 ms\n",
      "Tokens per second [21.3] input tokens [601] + xml response tokens [34] = total tokens i/o [635]\n",
      "Response: [<response><command>search google scholar current tab</command><args>best online games</args></response>]\n",
      "\n",
      "Processing call [043] out of [100] = [43.0%]... ETA mm:ss 1:39\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 2,006 ms\n",
      "Tokens per second [21.4] input tokens [609] + xml response tokens [43] = total tokens i/o [652]\n",
      "Response: [<response><command>search kagi current tab</command><args>File Not Found Error: File not found or path is incorrect</args></response>]\n",
      "\n",
      "Processing call [044] out of [100] = [44.0%]... ETA mm:ss 1:38\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,667 ms\n",
      "Tokens per second [22.2] input tokens [372] + xml response tokens [37] = total tokens i/o [409]\n",
      "Response: [<response><command>agent router go to date and time</command><args>Denver, Colorado</args></response>]\n",
      "\n",
      "Processing call [045] out of [100] = [45.0%]... ETA mm:ss 1:36\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,603 ms\n",
      "Tokens per second [21.2] input tokens [606] + xml response tokens [34] = total tokens i/o [640]\n",
      "Response: [<response><command>search kagi new tab</command><args>learning Japanese online</args></response>]\n",
      "\n",
      "Processing call [046] out of [100] = [46.0%]... ETA mm:ss 1:34\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,514 ms\n",
      "Tokens per second [21.1] input tokens [600] + xml response tokens [32] = total tokens i/o [632]\n",
      "Response: [<response><command>search current tab</command><args>SQL database management</args></response>]\n",
      "\n",
      "Processing call [047] out of [100] = [47.0%]... ETA mm:ss 1:32\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,584 ms\n",
      "Tokens per second [22.1] input tokens [362] + xml response tokens [35] = total tokens i/o [397]\n",
      "Response: [<response><command>agent router go to weather</command><args>Stockton, California</args></response>]\n",
      "\n",
      "Processing call [048] out of [100] = [48.0%]... ETA mm:ss 1:30\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,669 ms\n",
      "Tokens per second [22.2] input tokens [368] + xml response tokens [37] = total tokens i/o [405]\n",
      "Response: [<response><command>agent router go to weather</command><args>Ulaanbaatar, Mongolia</args></response>]\n",
      "\n",
      "Processing call [049] out of [100] = [49.0%]... ETA mm:ss 1:28\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,600 ms\n",
      "Tokens per second [21.2] input tokens [606] + xml response tokens [34] = total tokens i/o [640]\n",
      "Response: [<response><command>search phind new tab</command><args>Broken Pipe Error</args></response>]\n",
      "\n",
      "Processing call [050] out of [100] = [50.0%]... ETA mm:ss 1:27\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,726 ms\n",
      "Tokens per second [21.4] input tokens [605] + xml response tokens [37] = total tokens i/o [642]\n",
      "Response: [<response><command>go to new tab</command><args>blog.jubilantquartz.com</args></response>]\n",
      "\n",
      "Processing call [051] out of [100] = [51.0%]... ETA mm:ss 1:25\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,366 ms\n",
      "Tokens per second [22.0] input tokens [366] + xml response tokens [30] = total tokens i/o [396]\n",
      "Response: [<response><command>agent router go to calendar</command><args></args></response>]\n",
      "\n",
      "Processing call [052] out of [100] = [52.0%]... ETA mm:ss 1:23\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,570 ms\n",
      "Tokens per second [21.0] input tokens [599] + xml response tokens [33] = total tokens i/o [632]\n",
      "Response: [<response><command>search perplexity current tab</command><args>Runtime Error</args></response>]\n",
      "\n",
      "Processing call [053] out of [100] = [53.0%]... ETA mm:ss 1:21\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,676 ms\n",
      "Tokens per second [21.5] input tokens [609] + xml response tokens [36] = total tokens i/o [645]\n",
      "Response: [<response><command>search new tab</command><args>Saving Pandas DataFrame to CSV</args></response>]\n",
      "\n",
      "Processing call [054] out of [100] = [54.0%]... ETA mm:ss 1:19\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,364 ms\n",
      "Tokens per second [22.0] input tokens [403] + xml response tokens [30] = total tokens i/o [433]\n",
      "Response: [<response><command>agent router go to calendar</command><args></args></response>]\n",
      "\n",
      "Processing call [055] out of [100] = [55.0%]... ETA mm:ss 1:17\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,731 ms\n",
      "Tokens per second [21.4] input tokens [605] + xml response tokens [37] = total tokens i/o [642]\n",
      "Response: [<response><command>go to new tab</command><args>beta.beautifulvolcano.org</args></response>]\n",
      "\n",
      "Processing call [056] out of [100] = [56.0%]... ETA mm:ss 1:15\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 2,263 ms\n",
      "Tokens per second [21.7] input tokens [620] + xml response tokens [49] = total tokens i/o [669]\n",
      "Response: [<response><command>search google current tab</command><args>TensorFlow latest updates: What are the latest features added to TensorFlow in its most recent update?</args></response>]\n",
      "\n",
      "Processing call [057] out of [100] = [57.0%]... ETA mm:ss 1:14\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,736 ms\n",
      "Tokens per second [21.3] input tokens [601] + xml response tokens [37] = total tokens i/o [638]\n",
      "Response: [<response><command>go to current tab</command><args>test.spectacularxylophone.com</args></response>]\n",
      "\n",
      "Processing call [058] out of [100] = [58.0%]... ETA mm:ss 1:12\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,380 ms\n",
      "Tokens per second [21.7] input tokens [421] + xml response tokens [30] = total tokens i/o [451]\n",
      "Response: [<response><command>agent router go to calendar</command><args></args></response>]\n",
      "\n",
      "Processing call [059] out of [100] = [59.0%]... ETA mm:ss 1:10\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,668 ms\n",
      "Tokens per second [20.4] input tokens [600] + xml response tokens [34] = total tokens i/o [634]\n",
      "Response: [<response><command>search google scholar current tab</command><args>SQL database management</args></response>]\n",
      "\n",
      "Processing call [060] out of [100] = [60.0%]... ETA mm:ss 1:09\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,716 ms\n",
      "Tokens per second [21.6] input tokens [604] + xml response tokens [37] = total tokens i/o [641]\n",
      "Response: [<response><command>search perplexity current tab</command><args>Reinforcement Learning breakthroughs</args></response>]\n",
      "\n",
      "Processing call [061] out of [100] = [61.0%]... ETA mm:ss 1:07\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,714 ms\n",
      "Tokens per second [21.6] input tokens [606] + xml response tokens [37] = total tokens i/o [643]\n",
      "Response: [<response><command>go to new tab</command><args>blog.jubilantunicorn.io</args></response>]\n",
      "\n",
      "Processing call [062] out of [100] = [62.0%]... ETA mm:ss 1:05\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 2,060 ms\n",
      "Tokens per second [21.8] input tokens [610] + xml response tokens [45] = total tokens i/o [655]\n",
      "Response: [<response><command>search perplexity current tab</command><args>Voice Recognition technologies: What are the latest advancements in voice recognition technology?</args></response>]\n",
      "\n",
      "Processing call [063] out of [100] = [63.0%]... ETA mm:ss 1:04\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,578 ms\n",
      "Tokens per second [22.2] input tokens [387] + xml response tokens [35] = total tokens i/o [422]\n",
      "Response: [<response><command>agent router go to receptionist</command><args>Info Clerk Agent</args></response>]\n",
      "\n",
      "Processing call [064] out of [100] = [64.0%]... ETA mm:ss 1:02\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,558 ms\n",
      "Tokens per second [21.2] input tokens [603] + xml response tokens [33] = total tokens i/o [636]\n",
      "Response: [<response><command>search perplexity new tab</command><args>RuntimeWarning</args></response>]\n",
      "\n",
      "Processing call [065] out of [100] = [65.0%]... ETA: 60 seconds\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,956 ms\n",
      "Tokens per second [21.5] input tokens [610] + xml response tokens [42] = total tokens i/o [652]\n",
      "Response: [<response><command>search perplexity current tab</command><args>AI in gaming: How is AI changing the gaming industry?</args></response>]\n",
      "\n",
      "Processing call [066] out of [100] = [66.0%]... ETA: 58 seconds\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 2,168 ms\n",
      "Tokens per second [21.7] input tokens [618] + xml response tokens [47] = total tokens i/o [665]\n",
      "Response: [<response><command>search google new tab</command><args>Google Cloud AI services: What AI services does Google Cloud offer for developers and data scientists?</args></response>]\n",
      "\n",
      "Processing call [067] out of [100] = [67.0%]... ETA: 57 seconds\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,365 ms\n",
      "Tokens per second [22.0] input tokens [361] + xml response tokens [30] = total tokens i/o [391]\n",
      "Response: [<response><command>agent router go to math</command><args></args></response>]\n",
      "\n",
      "Processing call [068] out of [100] = [68.0%]... ETA: 55 seconds\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,588 ms\n",
      "Tokens per second [21.4] input tokens [601] + xml response tokens [34] = total tokens i/o [635]\n",
      "Response: [<response><command>search kagi new tab</command><args>hip hop music</args></response>]\n",
      "\n",
      "Processing call [069] out of [100] = [69.0%]... ETA: 53 seconds\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,816 ms\n",
      "Tokens per second [21.5] input tokens [608] + xml response tokens [39] = total tokens i/o [647]\n",
      "Response: [<response><command>go to new tab</command><args>dev.magnificentstrawberry.org</args></response>]\n",
      "\n",
      "Processing call [070] out of [100] = [70.0%]... ETA: 51 seconds\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,643 ms\n",
      "Tokens per second [21.3] input tokens [600] + xml response tokens [35] = total tokens i/o [635]\n",
      "Response: [<response><command>go to current tab</command><args>amazingiceberg.org</args></response>]\n",
      "\n",
      "Processing call [071] out of [100] = [71.0%]... ETA: 50 seconds\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,364 ms\n",
      "Tokens per second [22.0] input tokens [377] + xml response tokens [30] = total tokens i/o [407]\n",
      "Response: [<response><command>agent router go to calendar</command><args></args></response>]\n",
      "\n",
      "Processing call [072] out of [100] = [72.0%]... ETA: 48 seconds\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,624 ms\n",
      "Tokens per second [22.2] input tokens [364] + xml response tokens [36] = total tokens i/o [400]\n",
      "Response: [<response><command>agent router go to date and time</command><args>Sydney, Australia</args></response>]\n",
      "\n",
      "Processing call [073] out of [100] = [73.0%]... ETA: 46 seconds\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,991 ms\n",
      "Tokens per second [21.6] input tokens [612] + xml response tokens [43] = total tokens i/o [655]\n",
      "Response: [<response><command>search kagi current tab</command><args>How can system exit errors be handled gracefully in Python applications?</args></response>]\n",
      "\n",
      "Processing call [074] out of [100] = [74.0%]... ETA: 44 seconds\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 2,129 ms\n",
      "Tokens per second [21.6] input tokens [616] + xml response tokens [46] = total tokens i/o [662]\n",
      "Response: [<response><command>search google scholar new tab</command><args>Quantum AI computing developments: What are the latest developments in quantum AI computing?</args></response>]\n",
      "\n",
      "Processing call [075] out of [100] = [75.0%]... ETA: 43 seconds\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,862 ms\n",
      "Tokens per second [20.4] input tokens [606] + xml response tokens [38] = total tokens i/o [644]\n",
      "Response: [<response><command>go to current tab</command><args>beta.spectacularzebra.com</args></response>]\n",
      "\n",
      "Processing call [076] out of [100] = [76.0%]... ETA: 41 seconds\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,598 ms\n",
      "Tokens per second [21.3] input tokens [602] + xml response tokens [34] = total tokens i/o [636]\n",
      "Response: [<response><command>search google current tab</command><args>FileNotFoundError</args></response>]\n",
      "\n",
      "Processing call [077] out of [100] = [77.0%]... ETA: 39 seconds\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,642 ms\n",
      "Tokens per second [21.3] input tokens [606] + xml response tokens [35] = total tokens i/o [641]\n",
      "Response: [<response><command>search perplexity new tab</command><args>AI in financial forecasting</args></response>]\n",
      "\n",
      "Processing call [078] out of [100] = [78.0%]... ETA: 38 seconds\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,571 ms\n",
      "Tokens per second [22.3] input tokens [366] + xml response tokens [35] = total tokens i/o [401]\n",
      "Response: [<response><command>agent router go to receptionist</command><args>Service Desk</args></response>]\n",
      "\n",
      "Processing call [079] out of [100] = [79.0%]... ETA: 36 seconds\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,645 ms\n",
      "Tokens per second [21.3] input tokens [604] + xml response tokens [35] = total tokens i/o [639]\n",
      "Response: [<response><command>search kagi new tab</command><args>Connection Refused Error</args></response>]\n",
      "\n",
      "Processing call [080] out of [100] = [80.0%]... ETA: 34 seconds\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,955 ms\n",
      "Tokens per second [21.5] input tokens [614] + xml response tokens [42] = total tokens i/o [656]\n",
      "Response: [<response><command>search kagi new tab</command><args>AI in gaming: How is AI changing the gaming industry?</args></response>]\n",
      "\n",
      "Processing call [081] out of [100] = [81.0%]... ETA: 32 seconds\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,442 ms\n",
      "Tokens per second [20.8] input tokens [600] + xml response tokens [30] = total tokens i/o [630]\n",
      "Response: [<response><command>search using clipboard current tab</command><args></args></response>]\n",
      "\n",
      "Processing call [082] out of [100] = [82.0%]... ETA: 31 seconds\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,646 ms\n",
      "Tokens per second [21.3] input tokens [607] + xml response tokens [35] = total tokens i/o [642]\n",
      "Response: [<response><command>search perplexity new tab</command><args>IsADirectoryError</args></response>]\n",
      "\n",
      "Processing call [083] out of [100] = [83.0%]... ETA: 29 seconds\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,952 ms\n",
      "Tokens per second [21.5] input tokens [616] + xml response tokens [42] = total tokens i/o [658]\n",
      "Response: [<response><command>search perplexity new tab</command><args>AI in gaming: How is AI changing the gaming industry?</args></response>]\n",
      "\n",
      "Processing call [084] out of [100] = [84.0%]... ETA: 27 seconds\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,519 ms\n",
      "Tokens per second [21.1] input tokens [614] + xml response tokens [32] = total tokens i/o [646]\n",
      "Response: [<response><command>search phind using clipboard new tab</command><args></args></response>]\n",
      "\n",
      "Processing call [085] out of [100] = [85.0%]... ETA: 25 seconds\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,599 ms\n",
      "Tokens per second [21.2] input tokens [603] + xml response tokens [34] = total tokens i/o [637]\n",
      "Response: [<response><command>search new tab</command><args>AWS SageMaker for ML</args></response>]\n",
      "\n",
      "Processing call [086] out of [100] = [86.0%]... ETA: 24 seconds\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,557 ms\n",
      "Tokens per second [21.2] input tokens [602] + xml response tokens [33] = total tokens i/o [635]\n",
      "Response: [<response><command>search google current tab</command><args>AI transparency initiatives</args></response>]\n",
      "\n",
      "Processing call [087] out of [100] = [87.0%]... ETA: 22 seconds\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,601 ms\n",
      "Tokens per second [21.2] input tokens [606] + xml response tokens [34] = total tokens i/o [640]\n",
      "Response: [<response><command>search google new tab</command><args>FileNotFoundError</args></response>]\n",
      "\n",
      "Processing call [088] out of [100] = [88.0%]... ETA: 20 seconds\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,411 ms\n",
      "Tokens per second [22.0] input tokens [363] + xml response tokens [31] = total tokens i/o [394]\n",
      "Response: [<response><command>agent router go to todo list</command><args></args></response>]\n",
      "\n",
      "Processing call [089] out of [100] = [89.0%]... ETA: 18 seconds\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,745 ms\n",
      "Tokens per second [21.2] input tokens [608] + xml response tokens [37] = total tokens i/o [645]\n",
      "Response: [<response><command>search perplexity new tab</command><args>Reading CSV files in Pandas</args></response>]\n",
      "\n",
      "Processing call [090] out of [100] = [90.0%]... ETA: 17 seconds\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 2,213 ms\n",
      "Tokens per second [21.7] input tokens [619] + xml response tokens [48] = total tokens i/o [667]\n",
      "Response: [<response><command>search google scholar new tab</command><args>Sentiment Analysis tools: What tools are widely used for sentiment analysis in social media data?</args></response>]\n",
      "\n",
      "Processing call [091] out of [100] = [91.0%]... ETA: 15 seconds\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,690 ms\n",
      "Tokens per second [21.3] input tokens [606] + xml response tokens [36] = total tokens i/o [642]\n",
      "Response: [<response><command>search google scholar current tab</command><args>NoSQL databases for AI</args></response>]\n",
      "\n",
      "Processing call [092] out of [100] = [92.0%]... ETA: 13 seconds\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,511 ms\n",
      "Tokens per second [21.2] input tokens [604] + xml response tokens [32] = total tokens i/o [636]\n",
      "Response: [<response><command>search phind using clipboard new tab</command><args></args></response>]\n",
      "\n",
      "Processing call [093] out of [100] = [93.0%]... ETA: 12 seconds\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,668 ms\n",
      "Tokens per second [22.2] input tokens [363] + xml response tokens [37] = total tokens i/o [400]\n",
      "Response: [<response><command>agent router go to date and time</command><args>Anaheim, California</args></response>]\n",
      "\n",
      "Processing call [094] out of [100] = [94.0%]... ETA: 10 seconds\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,729 ms\n",
      "Tokens per second [21.4] input tokens [606] + xml response tokens [37] = total tokens i/o [643]\n",
      "Response: [<response><command>go to current tab</command><args>login.incredibleiceberg.com</args></response>]\n",
      "\n",
      "Processing call [095] out of [100] = [95.0%]... ETA: 8 seconds\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,688 ms\n",
      "Tokens per second [21.3] input tokens [606] + xml response tokens [36] = total tokens i/o [642]\n",
      "Response: [<response><command>search perplexity new tab</command><args>AWS SageMaker for ML</args></response>]\n",
      "\n",
      "Processing call [096] out of [100] = [96.0%]... ETA: 6 seconds\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,911 ms\n",
      "Tokens per second [21.5] input tokens [613] + xml response tokens [41] = total tokens i/o [654]\n",
      "Response: [<response><command>search new tab</command><args>How can you resolve errors related to ZIP file operations in Python?</args></response>]\n",
      "\n",
      "Processing call [097] out of [100] = [97.0%]... ETA: 5 seconds\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,866 ms\n",
      "Tokens per second [21.4] input tokens [609] + xml response tokens [40] = total tokens i/o [649]\n",
      "Response: [<response><command>search current tab</command><args>AI in gaming: How is AI changing the gaming industry?</args></response>]\n",
      "\n",
      "Processing call [098] out of [100] = [98.0%]... ETA: 3 seconds\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,599 ms\n",
      "Tokens per second [21.2] input tokens [601] + xml response tokens [34] = total tokens i/o [635]\n",
      "Response: [<response><command>search phind current tab</command><args>best online games</args></response>]\n",
      "\n",
      "Processing call [099] out of [100] = [99.0%]... ETA: 1 seconds\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,373 ms\n",
      "Tokens per second [21.8] input tokens [387] + xml response tokens [30] = total tokens i/o [417]\n",
      "Response: [<response><command>agent router go to calendar</command><args></args></response>]\n",
      "\n",
      "Processing call [100] out of [100] = [100.0%]... ETA: 0 seconds\n",
      "Asking LLM [ministral/Ministral-3b-instruct]...\n",
      "Asking LLM [ministral/Ministral-3b-instruct]... Done! in 1,511 ms\n",
      "Tokens per second [21.2] input tokens [602] + xml response tokens [32] = total tokens i/o [634]\n",
      "Response: [<response><command>search google scholar using clipboard current tab</command><args></args></response>]\n",
      "\n",
      "\n",
      "Generating responses for 100 rows... Done! in 02:53\n",
      "[1,733.3] ms per item\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Validation stats for model ministral/Ministral-3b-instruct\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "               Is valid xml 100.0%\n",
      "        Contains <response> 100.0%\n",
      "         Contains <command> 100.0%\n",
      "            Contains <args> 100.0%\n",
      "          Response is exact 99.0%\n",
      "Response has correct values 99.0%\n",
      "         Command is correct 99.0%\n",
      "            Args is correct 100.0%\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Validation stats for model ministral/Ministral-3b-instruct: Accuracy per command\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "                                            command     mean  sum  count\n",
      "                        agent router go to calendar  100.00%    7      7\n",
      "                   agent router go to date and time  100.00%    4      4\n",
      "                               search phind new tab  100.00%    5      5\n",
      "                           search phind current tab  100.00%    2      2\n",
      "                          search perplexity new tab  100.00%    7      7\n",
      "                      search perplexity current tab  100.00%    7      7\n",
      "                                     search new tab  100.00%    3      3\n",
      "                                search kagi new tab  100.00%    6      6\n",
      "                            search kagi current tab  100.00%    5      5\n",
      "  search google scholar using clipboard current tab  100.00%    2      2\n",
      "                      search google scholar new tab  100.00%    3      3\n",
      "                  search google scholar current tab  100.00%    6      6\n",
      "                              search google new tab  100.00%    7      7\n",
      "                          search google current tab  100.00%    5      5\n",
      "                                 search current tab  100.00%    2      2\n",
      "                                      go to new tab  100.00%    8      8\n",
      "                                  go to current tab  100.00%    6      6\n",
      "                         agent router go to weather  100.00%    3      3\n",
      "                       agent router go to todo list  100.00%    3      3\n",
      "                    agent router go to receptionist  100.00%    4      4\n",
      "                            agent router go to math  100.00%    2      2\n",
      "               search phind using clipboard new tab  100.00%    2      2\n",
      "           search phind using clipboard current tab    0.00%    0      1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>command</th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>prompt</th>\n",
       "      <th>gpt_message</th>\n",
       "      <th>response</th>\n",
       "      <th>response_xml_is_valid</th>\n",
       "      <th>contains_response</th>\n",
       "      <th>contains_command</th>\n",
       "      <th>contains_args</th>\n",
       "      <th>response_is_exact</th>\n",
       "      <th>response_has_correct_values</th>\n",
       "      <th>command_is_correct</th>\n",
       "      <th>args_is_correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3136</th>\n",
       "      <td>search google new tab</td>\n",
       "      <td>Your job is to discern the intent of a human v...</td>\n",
       "      <td>\\n        Below is the raw human voice command...</td>\n",
       "      <td>\\n        &lt;response&gt;\\n            &lt;command&gt;sea...</td>\n",
       "      <td>### Instruction:\\n        \\n    Use the Task a...</td>\n",
       "      <td>{'messages': [{'role': 'system', 'content': 'I...</td>\n",
       "      <td>&lt;response&gt;&lt;command&gt;search google new tab&lt;/comm...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2118</th>\n",
       "      <td>agent router go to receptionist</td>\n",
       "      <td>Your job is to discern the intent of a human v...</td>\n",
       "      <td>\\n        Below is the raw human voice command...</td>\n",
       "      <td>\\n        &lt;response&gt;\\n            &lt;command&gt;age...</td>\n",
       "      <td>### Instruction:\\n        \\n    Use the Task a...</td>\n",
       "      <td>{'messages': [{'role': 'system', 'content': 'I...</td>\n",
       "      <td>&lt;response&gt;&lt;command&gt;agent router go to receptio...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1811</th>\n",
       "      <td>search kagi current tab</td>\n",
       "      <td>Your job is to discern the intent of a human v...</td>\n",
       "      <td>\\n        Below is the raw human voice command...</td>\n",
       "      <td>\\n        &lt;response&gt;\\n            &lt;command&gt;sea...</td>\n",
       "      <td>### Instruction:\\n        \\n    Use the Task a...</td>\n",
       "      <td>{'messages': [{'role': 'system', 'content': 'I...</td>\n",
       "      <td>&lt;response&gt;&lt;command&gt;search kagi current tab&lt;/co...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>search google current tab</td>\n",
       "      <td>Your job is to discern the intent of a human v...</td>\n",
       "      <td>\\n        Below is the raw human voice command...</td>\n",
       "      <td>\\n        &lt;response&gt;\\n            &lt;command&gt;sea...</td>\n",
       "      <td>### Instruction:\\n        \\n    Use the Task a...</td>\n",
       "      <td>{'messages': [{'role': 'system', 'content': 'I...</td>\n",
       "      <td>&lt;response&gt;&lt;command&gt;search google current tab&lt;/...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2609</th>\n",
       "      <td>search perplexity new tab</td>\n",
       "      <td>Your job is to discern the intent of a human v...</td>\n",
       "      <td>\\n        Below is the raw human voice command...</td>\n",
       "      <td>\\n        &lt;response&gt;\\n            &lt;command&gt;sea...</td>\n",
       "      <td>### Instruction:\\n        \\n    Use the Task a...</td>\n",
       "      <td>{'messages': [{'role': 'system', 'content': 'I...</td>\n",
       "      <td>&lt;response&gt;&lt;command&gt;search perplexity new tab&lt;/...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2528</th>\n",
       "      <td>search new tab</td>\n",
       "      <td>Your job is to discern the intent of a human v...</td>\n",
       "      <td>\\n        Below is the raw human voice command...</td>\n",
       "      <td>\\n        &lt;response&gt;\\n            &lt;command&gt;sea...</td>\n",
       "      <td>### Instruction:\\n        \\n    Use the Task a...</td>\n",
       "      <td>{'messages': [{'role': 'system', 'content': 'I...</td>\n",
       "      <td>&lt;response&gt;&lt;command&gt;search new tab&lt;/command&gt;&lt;ar...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3386</th>\n",
       "      <td>search current tab</td>\n",
       "      <td>Your job is to discern the intent of a human v...</td>\n",
       "      <td>\\n        Below is the raw human voice command...</td>\n",
       "      <td>\\n        &lt;response&gt;\\n            &lt;command&gt;sea...</td>\n",
       "      <td>### Instruction:\\n        \\n    Use the Task a...</td>\n",
       "      <td>{'messages': [{'role': 'system', 'content': 'I...</td>\n",
       "      <td>&lt;response&gt;&lt;command&gt;search current tab&lt;/command...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3731</th>\n",
       "      <td>search phind current tab</td>\n",
       "      <td>Your job is to discern the intent of a human v...</td>\n",
       "      <td>\\n        Below is the raw human voice command...</td>\n",
       "      <td>\\n        &lt;response&gt;\\n            &lt;command&gt;sea...</td>\n",
       "      <td>### Instruction:\\n        \\n    Use the Task a...</td>\n",
       "      <td>{'messages': [{'role': 'system', 'content': 'I...</td>\n",
       "      <td>&lt;response&gt;&lt;command&gt;search phind current tab&lt;/c...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006</th>\n",
       "      <td>agent router go to calendar</td>\n",
       "      <td>Your job is to discern the intent of a human v...</td>\n",
       "      <td>\\n        Below is the raw human voice command...</td>\n",
       "      <td>\\n        &lt;response&gt;\\n            &lt;command&gt;age...</td>\n",
       "      <td>### Instruction:\\n        \\n    Use the Task a...</td>\n",
       "      <td>{'messages': [{'role': 'system', 'content': 'I...</td>\n",
       "      <td>&lt;response&gt;&lt;command&gt;agent router go to calendar...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2555</th>\n",
       "      <td>search google scholar using clipboard current tab</td>\n",
       "      <td>Your job is to discern the intent of a human v...</td>\n",
       "      <td>\\n        Below is the raw human voice command...</td>\n",
       "      <td>\\n        &lt;response&gt;\\n            &lt;command&gt;sea...</td>\n",
       "      <td>### Instruction:\\n        \\n    Use the Task a...</td>\n",
       "      <td>{'messages': [{'role': 'system', 'content': 'I...</td>\n",
       "      <td>&lt;response&gt;&lt;command&gt;search google scholar using...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows  15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                command  \\\n",
       "3136                              search google new tab   \n",
       "2118                    agent router go to receptionist   \n",
       "1811                            search kagi current tab   \n",
       "70                            search google current tab   \n",
       "2609                          search perplexity new tab   \n",
       "...                                                 ...   \n",
       "2528                                     search new tab   \n",
       "3386                                 search current tab   \n",
       "3731                           search phind current tab   \n",
       "1006                        agent router go to calendar   \n",
       "2555  search google scholar using clipboard current tab   \n",
       "\n",
       "                                            instruction  \\\n",
       "3136  Your job is to discern the intent of a human v...   \n",
       "2118  Your job is to discern the intent of a human v...   \n",
       "1811  Your job is to discern the intent of a human v...   \n",
       "70    Your job is to discern the intent of a human v...   \n",
       "2609  Your job is to discern the intent of a human v...   \n",
       "...                                                 ...   \n",
       "2528  Your job is to discern the intent of a human v...   \n",
       "3386  Your job is to discern the intent of a human v...   \n",
       "3731  Your job is to discern the intent of a human v...   \n",
       "1006  Your job is to discern the intent of a human v...   \n",
       "2555  Your job is to discern the intent of a human v...   \n",
       "\n",
       "                                                  input  \\\n",
       "3136  \\n        Below is the raw human voice command...   \n",
       "2118  \\n        Below is the raw human voice command...   \n",
       "1811  \\n        Below is the raw human voice command...   \n",
       "70    \\n        Below is the raw human voice command...   \n",
       "2609  \\n        Below is the raw human voice command...   \n",
       "...                                                 ...   \n",
       "2528  \\n        Below is the raw human voice command...   \n",
       "3386  \\n        Below is the raw human voice command...   \n",
       "3731  \\n        Below is the raw human voice command...   \n",
       "1006  \\n        Below is the raw human voice command...   \n",
       "2555  \\n        Below is the raw human voice command...   \n",
       "\n",
       "                                                 output  \\\n",
       "3136  \\n        <response>\\n            <command>sea...   \n",
       "2118  \\n        <response>\\n            <command>age...   \n",
       "1811  \\n        <response>\\n            <command>sea...   \n",
       "70    \\n        <response>\\n            <command>sea...   \n",
       "2609  \\n        <response>\\n            <command>sea...   \n",
       "...                                                 ...   \n",
       "2528  \\n        <response>\\n            <command>sea...   \n",
       "3386  \\n        <response>\\n            <command>sea...   \n",
       "3731  \\n        <response>\\n            <command>sea...   \n",
       "1006  \\n        <response>\\n            <command>age...   \n",
       "2555  \\n        <response>\\n            <command>sea...   \n",
       "\n",
       "                                                 prompt  \\\n",
       "3136  ### Instruction:\\n        \\n    Use the Task a...   \n",
       "2118  ### Instruction:\\n        \\n    Use the Task a...   \n",
       "1811  ### Instruction:\\n        \\n    Use the Task a...   \n",
       "70    ### Instruction:\\n        \\n    Use the Task a...   \n",
       "2609  ### Instruction:\\n        \\n    Use the Task a...   \n",
       "...                                                 ...   \n",
       "2528  ### Instruction:\\n        \\n    Use the Task a...   \n",
       "3386  ### Instruction:\\n        \\n    Use the Task a...   \n",
       "3731  ### Instruction:\\n        \\n    Use the Task a...   \n",
       "1006  ### Instruction:\\n        \\n    Use the Task a...   \n",
       "2555  ### Instruction:\\n        \\n    Use the Task a...   \n",
       "\n",
       "                                            gpt_message  \\\n",
       "3136  {'messages': [{'role': 'system', 'content': 'I...   \n",
       "2118  {'messages': [{'role': 'system', 'content': 'I...   \n",
       "1811  {'messages': [{'role': 'system', 'content': 'I...   \n",
       "70    {'messages': [{'role': 'system', 'content': 'I...   \n",
       "2609  {'messages': [{'role': 'system', 'content': 'I...   \n",
       "...                                                 ...   \n",
       "2528  {'messages': [{'role': 'system', 'content': 'I...   \n",
       "3386  {'messages': [{'role': 'system', 'content': 'I...   \n",
       "3731  {'messages': [{'role': 'system', 'content': 'I...   \n",
       "1006  {'messages': [{'role': 'system', 'content': 'I...   \n",
       "2555  {'messages': [{'role': 'system', 'content': 'I...   \n",
       "\n",
       "                                               response  \\\n",
       "3136  <response><command>search google new tab</comm...   \n",
       "2118  <response><command>agent router go to receptio...   \n",
       "1811  <response><command>search kagi current tab</co...   \n",
       "70    <response><command>search google current tab</...   \n",
       "2609  <response><command>search perplexity new tab</...   \n",
       "...                                                 ...   \n",
       "2528  <response><command>search new tab</command><ar...   \n",
       "3386  <response><command>search current tab</command...   \n",
       "3731  <response><command>search phind current tab</c...   \n",
       "1006  <response><command>agent router go to calendar...   \n",
       "2555  <response><command>search google scholar using...   \n",
       "\n",
       "      response_xml_is_valid  contains_response  contains_command  \\\n",
       "3136                   True               True              True   \n",
       "2118                   True               True              True   \n",
       "1811                   True               True              True   \n",
       "70                     True               True              True   \n",
       "2609                   True               True              True   \n",
       "...                     ...                ...               ...   \n",
       "2528                   True               True              True   \n",
       "3386                   True               True              True   \n",
       "3731                   True               True              True   \n",
       "1006                   True               True              True   \n",
       "2555                   True               True              True   \n",
       "\n",
       "      contains_args  response_is_exact  response_has_correct_values  \\\n",
       "3136           True               True                         True   \n",
       "2118           True               True                         True   \n",
       "1811           True               True                         True   \n",
       "70             True               True                         True   \n",
       "2609           True               True                         True   \n",
       "...             ...                ...                          ...   \n",
       "2528           True               True                         True   \n",
       "3386           True               True                         True   \n",
       "3731           True               True                         True   \n",
       "1006           True               True                         True   \n",
       "2555           True               True                         True   \n",
       "\n",
       "      command_is_correct  args_is_correct  \n",
       "3136                True             True  \n",
       "2118                True             True  \n",
       "1811                True             True  \n",
       "70                  True             True  \n",
       "2609                True             True  \n",
       "...                  ...              ...  \n",
       "2528                True             True  \n",
       "3386                True             True  \n",
       "3731                True             True  \n",
       "1006                True             True  \n",
       "2555                True             True  \n",
       "\n",
       "[100 rows x 15 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_df = run_validation( model_aqw, tokenizer_awq, sample_size=100 )\n",
    "stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4344fca6a442009",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T16:49:52.765020Z",
     "start_time": "2025-01-13T16:49:52.574601Z"
    }
   },
   "outputs": [],
   "source": [
    "reset_kernel( [ model_aqw, tokenizer_awq ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78a8c1bfab511a5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## GPU RAM after loading & validating AWQ model with 4bit AWQ: Device 1\n",
    "```\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "|   1  NVIDIA GeForce RTX 4090        On  | 00000000:02:00.0 Off |                  Off |\n",
    "| 65%   49C    P2              73W / 450W |   2700MiB / 24564MiB |      0%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "```\n",
    "\n",
    "```\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "|   1  NVIDIA GeForce RTX 4090        On  | 00000000:02:00.0 Off |                  Off |\n",
    "|  0%   43C    P8              22W / 450W |   5578MiB / 24564MiB |      0%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddb38ad67cce740",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## - Validation stats for model mistralai/Mistral-7B-Instruct-v0.2: ~40 Tokens/sec\n",
    "```\n",
    "Generating responses for 100 rows... Done! in 01:41\n",
    "[1014.6] ms per item\n",
    "\n",
    "------------------------------------------------------------------------------------------------------------------------\n",
    "- Validation stats for model mistralai/Mistral-7B-Instruct-v0.2\n",
    "------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "               Is valid xml 100.0%\n",
    "          Contains response 100.0%\n",
    " Contains <browser-command> 100.0%\n",
    "            Contains <args> 100.0%\n",
    "          Response is exact 100.0%\n",
    "Response has correct values 100.0%\n",
    " Browser command is correct 100.0%\n",
    "            Args is correct 100.0%\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae798c6fc1afbf0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Validate AWQ model: TGI service listening on port 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c7f843ae2950e5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T22:19:10.943633Z",
     "start_time": "2024-12-18T22:19:10.877402Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "/var/model/genie-in-the-box/src\n",
      "/var/model/genie-in-the-box/src\n"
     ]
    }
   ],
   "source": [
    "reset_environment()\n",
    "set_gib_env_vars()\n",
    "%autoreload\n",
    "\n",
    "import os\n",
    "print( os.getcwd() )\n",
    "os.chdir( \"/var/model/genie-in-the-box/src\" )\n",
    "print( os.getcwd() )\n",
    "import lib.utils.util         as du\n",
    "import lib.utils.util_xml     as dux\n",
    "import lib.utils.util_pytorch as dupt\n",
    "\n",
    "from ephemera.prompts.xml_fine_tuning_prompt_generator import XmlFineTuningPromptGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1a0e688cb8e66f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T22:21:23.956737Z",
     "start_time": "2024-12-18T22:21:23.213700Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reusing ConfigurationManager() singleton...\n",
      "Inserting DUPLICATE placeholders into the list. Requested length [500] > list length [74]\n",
      "Inserting DUPLICATE placeholders into the list. Requested length [500] > list length [148]\n",
      "Inserting DUPLICATE placeholders into the list. Requested length [500] > list length [296]\n",
      "Commands file for command [go to current tab] exists: True\n",
      "Commands file for command [go to new tab] exists: True\n",
      "Commands file for command [search current tab] exists: True\n",
      "Commands file for command [search new tab] exists: True\n",
      "Commands file for command [search google current tab] exists: True\n",
      "Commands file for command [search google new tab] exists: True\n",
      "Commands file for command [search google scholar current tab] exists: True\n",
      "Commands file for command [search google scholar new tab] exists: True\n",
      "Commands file for command [search kagi new tab] exists: True\n",
      "Commands file for command [search kagi current tab] exists: True\n",
      "Commands file for command [search perplexity current tab] exists: True\n",
      "Commands file for command [search perplexity new tab] exists: True\n",
      "Commands file for command [search phind current tab] exists: True\n",
      "Commands file for command [search phind new tab] exists: True\n",
      "\n",
      "Commands file for command [search using clipboard current tab] exists: True\n",
      "Commands file for command [search using clipboard new tab] exists: True\n",
      "Commands file for command [search google using clipboard current tab] exists: True\n",
      "Commands file for command [search google using clipboard new tab] exists: True\n",
      "Commands file for command [search google scholar using clipboard current tab] exists: True\n",
      "Commands file for command [search google scholar using clipboard new tab] exists: True\n",
      "Commands file for command [search kagi using clipboard current tab] exists: True\n",
      "Commands file for command [search kagi using clipboard new tab] exists: True\n",
      "Commands file for command [search perplexity using clipboard current tab] exists: True\n",
      "Commands file for command [search perplexity using clipboard new tab] exists: True\n",
      "Commands file for command [search phind using clipboard current tab] exists: True\n",
      "Commands file for command [search phind using clipboard new tab] exists: True\n",
      "Commands file for command [none] exists: True\n",
      "\n",
      "Commands file for command [agent router go to search function mapping] exists: True\n",
      "\n",
      "Commands file for command [agent router go to date and time] exists: True\n",
      "Commands file for command [agent router go to weather] exists: True\n",
      "Commands file for command [agent router go to calendar] exists: True\n",
      "Commands file for command [agent router go to receptionist] exists: True\n",
      "\n",
      "Commands file for command [agent router go to todo list] exists: True\n",
      "Commands file for command [agent router go to math] exists: True\n",
      "Commands file for command [none] exists: True\n",
      "\n",
      "Generating responses for 3,951 rows...\n",
      "Using TGI w/ model_name [ministral/Ministral-3b-instruct-AWQ]...\n",
      "Processing call [001] out of [3951] = [0.0%]... ETA: 0 seconds\n",
      "Asking LLM [ministral/Ministral-3b-instruct-AWQ]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/inference/_client.py:2232: FutureWarning: `stop_sequences` is a deprecated argument for `text_generation` task and will be removed in version '0.28.0'. Use `stop` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "(ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')), '(Request ID: 44d30b0e-26f9-4061-927f-bdb742745517)')",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mConnectionResetError\u001B[0m                      Traceback (most recent call last)",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py:789\u001B[0m, in \u001B[0;36mHTTPConnectionPool.urlopen\u001B[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001B[0m\n\u001B[1;32m    788\u001B[0m \u001B[38;5;66;03m# Make the request on the HTTPConnection object\u001B[39;00m\n\u001B[0;32m--> 789\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    790\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    791\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    792\u001B[0m \u001B[43m    \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    793\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout_obj\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    794\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    795\u001B[0m \u001B[43m    \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    796\u001B[0m \u001B[43m    \u001B[49m\u001B[43mchunked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunked\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    797\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mretries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    798\u001B[0m \u001B[43m    \u001B[49m\u001B[43mresponse_conn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresponse_conn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    799\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpreload_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpreload_content\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    800\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdecode_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecode_content\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    801\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mresponse_kw\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    802\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    804\u001B[0m \u001B[38;5;66;03m# Everything went great!\u001B[39;00m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py:536\u001B[0m, in \u001B[0;36mHTTPConnectionPool._make_request\u001B[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001B[0m\n\u001B[1;32m    535\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 536\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mconn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetresponse\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    537\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (BaseSSLError, \u001B[38;5;167;01mOSError\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connection.py:507\u001B[0m, in \u001B[0;36mHTTPConnection.getresponse\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    506\u001B[0m \u001B[38;5;66;03m# Get the response from http.client.HTTPConnection\u001B[39;00m\n\u001B[0;32m--> 507\u001B[0m httplib_response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetresponse\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    509\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[0;32m/usr/lib/python3.10/http/client.py:1375\u001B[0m, in \u001B[0;36mHTTPConnection.getresponse\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1374\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1375\u001B[0m     \u001B[43mresponse\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbegin\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1376\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mConnectionError\u001B[39;00m:\n",
      "File \u001B[0;32m/usr/lib/python3.10/http/client.py:318\u001B[0m, in \u001B[0;36mHTTPResponse.begin\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    317\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 318\u001B[0m     version, status, reason \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_read_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    319\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m status \u001B[38;5;241m!=\u001B[39m CONTINUE:\n",
      "File \u001B[0;32m/usr/lib/python3.10/http/client.py:279\u001B[0m, in \u001B[0;36mHTTPResponse._read_status\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    278\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_read_status\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m--> 279\u001B[0m     line \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreadline\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_MAXLINE\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124miso-8859-1\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    280\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(line) \u001B[38;5;241m>\u001B[39m _MAXLINE:\n",
      "File \u001B[0;32m/usr/lib/python3.10/socket.py:705\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    704\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 705\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrecv_into\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    706\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n",
      "\u001B[0;31mConnectionResetError\u001B[0m: [Errno 104] Connection reset by peer",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mProtocolError\u001B[0m                             Traceback (most recent call last)",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/requests/adapters.py:667\u001B[0m, in \u001B[0;36mHTTPAdapter.send\u001B[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001B[0m\n\u001B[1;32m    666\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 667\u001B[0m     resp \u001B[38;5;241m=\u001B[39m \u001B[43mconn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43murlopen\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    668\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    669\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    670\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    671\u001B[0m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    672\u001B[0m \u001B[43m        \u001B[49m\u001B[43mredirect\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    673\u001B[0m \u001B[43m        \u001B[49m\u001B[43massert_same_host\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    674\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpreload_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    675\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdecode_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    676\u001B[0m \u001B[43m        \u001B[49m\u001B[43mretries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_retries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    677\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    678\u001B[0m \u001B[43m        \u001B[49m\u001B[43mchunked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunked\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    679\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    681\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (ProtocolError, \u001B[38;5;167;01mOSError\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py:843\u001B[0m, in \u001B[0;36mHTTPConnectionPool.urlopen\u001B[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001B[0m\n\u001B[1;32m    841\u001B[0m     new_e \u001B[38;5;241m=\u001B[39m ProtocolError(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mConnection aborted.\u001B[39m\u001B[38;5;124m\"\u001B[39m, new_e)\n\u001B[0;32m--> 843\u001B[0m retries \u001B[38;5;241m=\u001B[39m \u001B[43mretries\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mincrement\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    844\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43merror\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnew_e\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_pool\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_stacktrace\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msys\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexc_info\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m]\u001B[49m\n\u001B[1;32m    845\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    846\u001B[0m retries\u001B[38;5;241m.\u001B[39msleep()\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/util/retry.py:474\u001B[0m, in \u001B[0;36mRetry.increment\u001B[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001B[0m\n\u001B[1;32m    473\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m read \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m method \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_is_method_retryable(method):\n\u001B[0;32m--> 474\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[43mreraise\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mtype\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43merror\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43merror\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_stacktrace\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    475\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m read \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/util/util.py:38\u001B[0m, in \u001B[0;36mreraise\u001B[0;34m(tp, value, tb)\u001B[0m\n\u001B[1;32m     37\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m value\u001B[38;5;241m.\u001B[39m__traceback__ \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m tb:\n\u001B[0;32m---> 38\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m value\u001B[38;5;241m.\u001B[39mwith_traceback(tb)\n\u001B[1;32m     39\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m value\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py:789\u001B[0m, in \u001B[0;36mHTTPConnectionPool.urlopen\u001B[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001B[0m\n\u001B[1;32m    788\u001B[0m \u001B[38;5;66;03m# Make the request on the HTTPConnection object\u001B[39;00m\n\u001B[0;32m--> 789\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    790\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    791\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    792\u001B[0m \u001B[43m    \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    793\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout_obj\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    794\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    795\u001B[0m \u001B[43m    \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    796\u001B[0m \u001B[43m    \u001B[49m\u001B[43mchunked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunked\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    797\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mretries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    798\u001B[0m \u001B[43m    \u001B[49m\u001B[43mresponse_conn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresponse_conn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    799\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpreload_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpreload_content\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    800\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdecode_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecode_content\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    801\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mresponse_kw\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    802\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    804\u001B[0m \u001B[38;5;66;03m# Everything went great!\u001B[39;00m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py:536\u001B[0m, in \u001B[0;36mHTTPConnectionPool._make_request\u001B[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001B[0m\n\u001B[1;32m    535\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 536\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mconn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetresponse\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    537\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (BaseSSLError, \u001B[38;5;167;01mOSError\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connection.py:507\u001B[0m, in \u001B[0;36mHTTPConnection.getresponse\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    506\u001B[0m \u001B[38;5;66;03m# Get the response from http.client.HTTPConnection\u001B[39;00m\n\u001B[0;32m--> 507\u001B[0m httplib_response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetresponse\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    509\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[0;32m/usr/lib/python3.10/http/client.py:1375\u001B[0m, in \u001B[0;36mHTTPConnection.getresponse\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1374\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1375\u001B[0m     \u001B[43mresponse\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbegin\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1376\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mConnectionError\u001B[39;00m:\n",
      "File \u001B[0;32m/usr/lib/python3.10/http/client.py:318\u001B[0m, in \u001B[0;36mHTTPResponse.begin\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    317\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 318\u001B[0m     version, status, reason \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_read_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    319\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m status \u001B[38;5;241m!=\u001B[39m CONTINUE:\n",
      "File \u001B[0;32m/usr/lib/python3.10/http/client.py:279\u001B[0m, in \u001B[0;36mHTTPResponse._read_status\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    278\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_read_status\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m--> 279\u001B[0m     line \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreadline\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_MAXLINE\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124miso-8859-1\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    280\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(line) \u001B[38;5;241m>\u001B[39m _MAXLINE:\n",
      "File \u001B[0;32m/usr/lib/python3.10/socket.py:705\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    704\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 705\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrecv_into\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    706\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n",
      "\u001B[0;31mProtocolError\u001B[0m: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mConnectionError\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[15], line 10\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# model_name     = \"Phind-CodeLlama-34B-v2 w/ BnB 4nf\"\u001B[39;00m\n\u001B[1;32m      8\u001B[0m validate_df    \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_json( \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/var/model/genie-in-the-box/src/ephemera/prompts/data/voice-commands-xml-validate.jsonl\u001B[39m\u001B[38;5;124m\"\u001B[39m, lines\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m )\n\u001B[0;32m---> 10\u001B[0m validate_df    \u001B[38;5;241m=\u001B[39m \u001B[43mtgi_validator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate_responses\u001B[49m\u001B[43m(\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidate_df\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mswitch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtgi\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_name\u001B[49m\u001B[43m \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     11\u001B[0m validate_df    \u001B[38;5;241m=\u001B[39m tgi_validator\u001B[38;5;241m.\u001B[39mvalidate_responses( validate_df )\n\u001B[1;32m     13\u001B[0m tgi_validator\u001B[38;5;241m.\u001B[39mprint_validation_stats( validate_df, title\u001B[38;5;241m=\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mValidation Stats for `\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m` on TGI:3000\u001B[39m\u001B[38;5;124m\"\u001B[39m )\n",
      "File \u001B[0;32m/var/model/genie-in-the-box/src/ephemera/prompts/xml_fine_tuning_prompt_generator.py:1134\u001B[0m, in \u001B[0;36mXmlFineTuningPromptGenerator.generate_responses\u001B[0;34m(self, df, tokenizer, model, switch, model_name, max_new_tokens, temperature, top_k, top_p, device, silent)\u001B[0m\n\u001B[1;32m   1132\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m switch \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtgi\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m   1133\u001B[0m     \u001B[38;5;28mprint\u001B[39m( \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUsing TGI w/ model_name [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m]...\u001B[39m\u001B[38;5;124m\"\u001B[39m )\n\u001B[0;32m-> 1134\u001B[0m     df[ \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mresponse\u001B[39m\u001B[38;5;124m\"\u001B[39m ]  \u001B[38;5;241m=\u001B[39m \u001B[43mdf\u001B[49m\u001B[43m[\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mprompt\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mcell\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_response_to_prompt\u001B[49m\u001B[43m(\u001B[49m\u001B[43m \u001B[49m\u001B[43mcell\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrows\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mswitch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mswitch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_new_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_new_tokens\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtemperature\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_k\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtop_k\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_p\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtop_p\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msilent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msilent\u001B[49m\u001B[43m \u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1135\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m switch \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mopenai\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m   1136\u001B[0m     \u001B[38;5;28mprint\u001B[39m( \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUsing OPENAI w/ model_name [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m]...\u001B[39m\u001B[38;5;124m\"\u001B[39m )\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py:4924\u001B[0m, in \u001B[0;36mSeries.apply\u001B[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001B[0m\n\u001B[1;32m   4789\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply\u001B[39m(\n\u001B[1;32m   4790\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   4791\u001B[0m     func: AggFuncType,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   4796\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m   4797\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame \u001B[38;5;241m|\u001B[39m Series:\n\u001B[1;32m   4798\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   4799\u001B[0m \u001B[38;5;124;03m    Invoke function on values of Series.\u001B[39;00m\n\u001B[1;32m   4800\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   4915\u001B[0m \u001B[38;5;124;03m    dtype: float64\u001B[39;00m\n\u001B[1;32m   4916\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m   4917\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mSeriesApply\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   4918\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4919\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4920\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconvert_dtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert_dtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4921\u001B[0m \u001B[43m        \u001B[49m\u001B[43mby_row\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mby_row\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4922\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4923\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m-> 4924\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py:1427\u001B[0m, in \u001B[0;36mSeriesApply.apply\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1424\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply_compat()\n\u001B[1;32m   1426\u001B[0m \u001B[38;5;66;03m# self.func is Callable\u001B[39;00m\n\u001B[0;32m-> 1427\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_standard\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py:1507\u001B[0m, in \u001B[0;36mSeriesApply.apply_standard\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1501\u001B[0m \u001B[38;5;66;03m# row-wise access\u001B[39;00m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m \u001B[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001B[39;00m\n\u001B[1;32m   1504\u001B[0m \u001B[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001B[39;00m\n\u001B[1;32m   1505\u001B[0m \u001B[38;5;66;03m#  Categorical (GH51645).\u001B[39;00m\n\u001B[1;32m   1506\u001B[0m action \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mignore\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(obj\u001B[38;5;241m.\u001B[39mdtype, CategoricalDtype) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 1507\u001B[0m mapped \u001B[38;5;241m=\u001B[39m \u001B[43mobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_map_values\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1508\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmapper\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcurried\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mna_action\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maction\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert_dtype\u001B[49m\n\u001B[1;32m   1509\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1511\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(mapped) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(mapped[\u001B[38;5;241m0\u001B[39m], ABCSeries):\n\u001B[1;32m   1512\u001B[0m     \u001B[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001B[39;00m\n\u001B[1;32m   1513\u001B[0m     \u001B[38;5;66;03m#  See also GH#25959 regarding EA support\u001B[39;00m\n\u001B[1;32m   1514\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\u001B[38;5;241m.\u001B[39m_constructor_expanddim(\u001B[38;5;28mlist\u001B[39m(mapped), index\u001B[38;5;241m=\u001B[39mobj\u001B[38;5;241m.\u001B[39mindex)\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/base.py:921\u001B[0m, in \u001B[0;36mIndexOpsMixin._map_values\u001B[0;34m(self, mapper, na_action, convert)\u001B[0m\n\u001B[1;32m    918\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(arr, ExtensionArray):\n\u001B[1;32m    919\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m arr\u001B[38;5;241m.\u001B[39mmap(mapper, na_action\u001B[38;5;241m=\u001B[39mna_action)\n\u001B[0;32m--> 921\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43malgorithms\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43marr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapper\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mna_action\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mna_action\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/algorithms.py:1743\u001B[0m, in \u001B[0;36mmap_array\u001B[0;34m(arr, mapper, na_action, convert)\u001B[0m\n\u001B[1;32m   1741\u001B[0m values \u001B[38;5;241m=\u001B[39m arr\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mobject\u001B[39m, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m na_action \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1743\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_infer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapper\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1745\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m lib\u001B[38;5;241m.\u001B[39mmap_infer_mask(\n\u001B[1;32m   1746\u001B[0m         values, mapper, mask\u001B[38;5;241m=\u001B[39misna(values)\u001B[38;5;241m.\u001B[39mview(np\u001B[38;5;241m.\u001B[39muint8), convert\u001B[38;5;241m=\u001B[39mconvert\n\u001B[1;32m   1747\u001B[0m     )\n",
      "File \u001B[0;32mlib.pyx:2972\u001B[0m, in \u001B[0;36mpandas._libs.lib.map_infer\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m/var/model/genie-in-the-box/src/ephemera/prompts/xml_fine_tuning_prompt_generator.py:1134\u001B[0m, in \u001B[0;36mXmlFineTuningPromptGenerator.generate_responses.<locals>.<lambda>\u001B[0;34m(cell)\u001B[0m\n\u001B[1;32m   1132\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m switch \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtgi\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m   1133\u001B[0m     \u001B[38;5;28mprint\u001B[39m( \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUsing TGI w/ model_name [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m]...\u001B[39m\u001B[38;5;124m\"\u001B[39m )\n\u001B[0;32m-> 1134\u001B[0m     df[ \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mresponse\u001B[39m\u001B[38;5;124m\"\u001B[39m ]  \u001B[38;5;241m=\u001B[39m df[ \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprompt\u001B[39m\u001B[38;5;124m\"\u001B[39m ]\u001B[38;5;241m.\u001B[39mapply( \u001B[38;5;28;01mlambda\u001B[39;00m cell: \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_response_to_prompt\u001B[49m\u001B[43m(\u001B[49m\u001B[43m \u001B[49m\u001B[43mcell\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrows\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mswitch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mswitch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_new_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_new_tokens\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtemperature\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_k\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtop_k\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_p\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtop_p\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msilent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msilent\u001B[49m\u001B[43m \u001B[49m\u001B[43m)\u001B[49m )\n\u001B[1;32m   1135\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m switch \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mopenai\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m   1136\u001B[0m     \u001B[38;5;28mprint\u001B[39m( \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUsing OPENAI w/ model_name [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m]...\u001B[39m\u001B[38;5;124m\"\u001B[39m )\n",
      "File \u001B[0;32m/var/model/genie-in-the-box/src/ephemera/prompts/xml_fine_tuning_prompt_generator.py:1117\u001B[0m, in \u001B[0;36mXmlFineTuningPromptGenerator.get_response_to_prompt\u001B[0;34m(self, prompt, rows, switch, model_name, timer, tokenizer, model, max_new_tokens, temperature, top_k, top_p, device, silent)\u001B[0m\n\u001B[1;32m   1114\u001B[0m         \u001B[38;5;28mprint\u001B[39m( \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mETA: Error \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m )\n\u001B[1;32m   1116\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m switch \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtgi\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m-> 1117\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mquery_llm_tgi\u001B[49m\u001B[43m(\u001B[49m\u001B[43m \u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_new_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_new_tokens\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtemperature\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_k\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtop_k\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_p\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtop_p\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msilent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msilent\u001B[49m\u001B[43m \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1118\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m switch \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mopenai\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m   1119\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_query_llm_openai( prompt[ \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m ], model_name\u001B[38;5;241m=\u001B[39mmodel_name )\n",
      "File \u001B[0;32m/var/model/genie-in-the-box/src/ephemera/prompts/xml_fine_tuning_prompt_generator.py:1039\u001B[0m, in \u001B[0;36mXmlFineTuningPromptGenerator.query_llm_tgi\u001B[0;34m(self, prompt, model_name, max_new_tokens, temperature, top_k, top_p, silent)\u001B[0m\n\u001B[1;32m   1036\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m prompt\u001B[38;5;241m.\u001B[39msplit( \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m ):\n\u001B[1;32m   1037\u001B[0m         \u001B[38;5;28mprint\u001B[39m( line )\n\u001B[0;32m-> 1039\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m token \u001B[38;5;129;01min\u001B[39;00m \u001B[43mclient\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtext_generation\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1040\u001B[0m \u001B[43m    \u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_new_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_new_tokens\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtemperature\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_k\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtop_k\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_p\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtop_p\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop_sequences\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m</response>\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m]\u001B[49m\n\u001B[1;32m   1041\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m   1042\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdebug:\n\u001B[1;32m   1043\u001B[0m         \u001B[38;5;28mprint\u001B[39m( token, end\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m )\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/inference/_client.py:2302\u001B[0m, in \u001B[0;36mInferenceClient.text_generation\u001B[0;34m(self, prompt, details, stream, model, adapter_id, best_of, decoder_input_details, do_sample, frequency_penalty, grammar, max_new_tokens, repetition_penalty, return_full_text, seed, stop, stop_sequences, temperature, top_k, top_n_tokens, top_p, truncate, typical_p, watermark)\u001B[0m\n\u001B[1;32m   2300\u001B[0m \u001B[38;5;66;03m# Handle errors separately for more precise error messages\u001B[39;00m\n\u001B[1;32m   2301\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 2302\u001B[0m     bytes_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpost\u001B[49m\u001B[43m(\u001B[49m\u001B[43mjson\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpayload\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtext-generation\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n\u001B[1;32m   2303\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m HTTPError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m   2304\u001B[0m     match \u001B[38;5;241m=\u001B[39m MODEL_KWARGS_NOT_USED_REGEX\u001B[38;5;241m.\u001B[39msearch(\u001B[38;5;28mstr\u001B[39m(e))\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/inference/_client.py:281\u001B[0m, in \u001B[0;36mInferenceClient.post\u001B[0;34m(self, json, data, model, task, stream)\u001B[0m\n\u001B[1;32m    279\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m _open_as_binary(data) \u001B[38;5;28;01mas\u001B[39;00m data_as_binary:\n\u001B[1;32m    280\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 281\u001B[0m         response \u001B[38;5;241m=\u001B[39m \u001B[43mget_session\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpost\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    282\u001B[0m \u001B[43m            \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    283\u001B[0m \u001B[43m            \u001B[49m\u001B[43mjson\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mjson\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    284\u001B[0m \u001B[43m            \u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_as_binary\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    285\u001B[0m \u001B[43m            \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    286\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcookies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcookies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    287\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    288\u001B[0m \u001B[43m            \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    289\u001B[0m \u001B[43m            \u001B[49m\u001B[43mproxies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    290\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    291\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTimeoutError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n\u001B[1;32m    292\u001B[0m         \u001B[38;5;66;03m# Convert any `TimeoutError` to a `InferenceTimeoutError`\u001B[39;00m\n\u001B[1;32m    293\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m InferenceTimeoutError(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInference call timed out: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00murl\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merror\u001B[39;00m  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py:637\u001B[0m, in \u001B[0;36mSession.post\u001B[0;34m(self, url, data, json, **kwargs)\u001B[0m\n\u001B[1;32m    626\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpost\u001B[39m(\u001B[38;5;28mself\u001B[39m, url, data\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, json\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    627\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Sends a POST request. Returns :class:`Response` object.\u001B[39;00m\n\u001B[1;32m    628\u001B[0m \n\u001B[1;32m    629\u001B[0m \u001B[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    634\u001B[0m \u001B[38;5;124;03m    :rtype: requests.Response\u001B[39;00m\n\u001B[1;32m    635\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 637\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mPOST\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mjson\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mjson\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py:589\u001B[0m, in \u001B[0;36mSession.request\u001B[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001B[0m\n\u001B[1;32m    584\u001B[0m send_kwargs \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    585\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtimeout\u001B[39m\u001B[38;5;124m\"\u001B[39m: timeout,\n\u001B[1;32m    586\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mallow_redirects\u001B[39m\u001B[38;5;124m\"\u001B[39m: allow_redirects,\n\u001B[1;32m    587\u001B[0m }\n\u001B[1;32m    588\u001B[0m send_kwargs\u001B[38;5;241m.\u001B[39mupdate(settings)\n\u001B[0;32m--> 589\u001B[0m resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprep\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43msend_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    591\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m resp\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py:703\u001B[0m, in \u001B[0;36mSession.send\u001B[0;34m(self, request, **kwargs)\u001B[0m\n\u001B[1;32m    700\u001B[0m start \u001B[38;5;241m=\u001B[39m preferred_clock()\n\u001B[1;32m    702\u001B[0m \u001B[38;5;66;03m# Send the request\u001B[39;00m\n\u001B[0;32m--> 703\u001B[0m r \u001B[38;5;241m=\u001B[39m \u001B[43madapter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    705\u001B[0m \u001B[38;5;66;03m# Total elapsed time of the request (approximately)\u001B[39;00m\n\u001B[1;32m    706\u001B[0m elapsed \u001B[38;5;241m=\u001B[39m preferred_clock() \u001B[38;5;241m-\u001B[39m start\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_http.py:93\u001B[0m, in \u001B[0;36mUniqueRequestIdAdapter.send\u001B[0;34m(self, request, *args, **kwargs)\u001B[0m\n\u001B[1;32m     91\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Catch any RequestException to append request id to the error message for debugging.\"\"\"\u001B[39;00m\n\u001B[1;32m     92\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 93\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     94\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m requests\u001B[38;5;241m.\u001B[39mRequestException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     95\u001B[0m     request_id \u001B[38;5;241m=\u001B[39m request\u001B[38;5;241m.\u001B[39mheaders\u001B[38;5;241m.\u001B[39mget(X_AMZN_TRACE_ID)\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/requests/adapters.py:682\u001B[0m, in \u001B[0;36mHTTPAdapter.send\u001B[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001B[0m\n\u001B[1;32m    667\u001B[0m     resp \u001B[38;5;241m=\u001B[39m conn\u001B[38;5;241m.\u001B[39murlopen(\n\u001B[1;32m    668\u001B[0m         method\u001B[38;5;241m=\u001B[39mrequest\u001B[38;5;241m.\u001B[39mmethod,\n\u001B[1;32m    669\u001B[0m         url\u001B[38;5;241m=\u001B[39murl,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    678\u001B[0m         chunked\u001B[38;5;241m=\u001B[39mchunked,\n\u001B[1;32m    679\u001B[0m     )\n\u001B[1;32m    681\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (ProtocolError, \u001B[38;5;167;01mOSError\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[0;32m--> 682\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mConnectionError\u001B[39;00m(err, request\u001B[38;5;241m=\u001B[39mrequest)\n\u001B[1;32m    684\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m MaxRetryError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    685\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(e\u001B[38;5;241m.\u001B[39mreason, ConnectTimeoutError):\n\u001B[1;32m    686\u001B[0m         \u001B[38;5;66;03m# TODO: Remove this in 3.0.0: see #2811\u001B[39;00m\n",
      "\u001B[0;31mConnectionError\u001B[0m: (ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')), '(Request ID: 44d30b0e-26f9-4061-927f-bdb742745517)')"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "tgi_validator  = XmlFineTuningPromptGenerator( path_prefix=\"/var/model/genie-in-the-box\", tgi_url=\"http://192.168.1.21:3000\", debug=True )\n",
    "\n",
    "model_name = \"Ministral-8B-Instruct-2410\"\n",
    "# model_name     = \"ministral/Ministral-3b-instruct-AWQ\"\n",
    "# model_name     = \"Phind-CodeLlama-34B-v2 w/ BnB 4nf\"\n",
    "\n",
    "validate_df    = pd.read_json( \"/var/model/genie-in-the-box/src/ephemera/prompts/data/voice-commands-xml-validate.jsonl\", lines=True )\n",
    "\n",
    "validate_df    = tgi_validator.generate_responses( validate_df, switch=\"tgi\", model_name=model_name )\n",
    "validate_df    = tgi_validator.validate_responses( validate_df )\n",
    "\n",
    "tgi_validator.print_validation_stats( validate_df, title=f\"Validation Stats for `{model_name}` on TGI:3000\" )\n",
    "\n",
    "# Generating responses for 100 rows... Done! in 50 seconds\n",
    "# [502.1] ms per item\n",
    "# \n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# - Validation Stats for `mistralai/Mistral-7B-Instruct-v0.2-AWQ` on TGI:3000\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# \n",
    "#                Is valid xml 100.0%\n",
    "#           Contains response 100.0%\n",
    "#  Contains <browser-command> 100.0%\n",
    "#             Contains <args> 100.0%\n",
    "#           Response is exact 100.0%\n",
    "# Response has correct values 100.0%\n",
    "#  Browser command is correct 100.0%\n",
    "#             Args is correct 100.0%\n",
    "\n",
    "\n",
    "# Generating responses for 100 rows... Done! in 01:12\n",
    "# [722.3] ms per item\n",
    "# \n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# - Validation Stats for `mistralai/Mistral-7B-Instruct-v0.2-BnB-4nf` on TGI:3000 with BnB 4nf \n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# \n",
    "#                Is valid xml 100.0%\n",
    "#           Contains response 100.0%\n",
    "#  Contains <browser-command> 100.0%\n",
    "#             Contains <args> 100.0%\n",
    "#           Response is exact 99.0%\n",
    "# Response has correct values 99.0%\n",
    "#  Browser command is correct 100.0%\n",
    "#             Args is correct 99.0%\n",
    "\n",
    "\n",
    "# Generating responses for 100 rows... Done! in 02:26\n",
    "# [1461.4] ms per item\n",
    "# \n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# - Validation Stats for `Phind-CodeLlama-34B-v2 w/ BnB 4nf` on TGI:3000\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# \n",
    "#                Is valid xml 100.0%\n",
    "#           Contains response 100.0%\n",
    "#  Contains <browser-command> 100.0%\n",
    "#             Contains <args> 100.0%\n",
    "#           Response is exact 42.0%\n",
    "# Response has correct values 42.0%\n",
    "#  Browser command is correct 46.0%\n",
    "#             Args is correct 82.0%\n",
    "# \n",
    "# Mon Jan 22 13:23:25 2024\n",
    "# +---------------------------------------------------------------------------------------+\n",
    "# | NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
    "# |-----------------------------------------+----------------------+----------------------+\n",
    "# | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "# | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "# |                                         |                      |               MIG M. |\n",
    "# |=========================================+======================+======================|\n",
    "# |   0  NVIDIA GeForce RTX 4090        On  | 00000000:01:00.0 Off |                  Off |\n",
    "# |  0%   40C    P8              29W / 450W |  18064MiB / 24564MiB |      0%      Default |\n",
    "# |                                         |                      |                  N/A |\n",
    "# +-----------------------------------------+----------------------+----------------------+\n",
    "# |   1  NVIDIA GeForce RTX 4090        On  | 00000000:02:00.0 Off |                  Off |\n",
    "# |  0%   45C    P8              22W / 450W |   4994MiB / 24564MiB |      0%      Default |\n",
    "# |                                         |                      |                  N/A |\n",
    "# +-----------------------------------------+----------------------+----------------------+\n",
    "# \n",
    "# +---------------------------------------------------------------------------------------+\n",
    "# | Processes:                                                                            |\n",
    "# |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "# |        ID   ID                                                             Usage      |\n",
    "# |=======================================================================================|\n",
    "# |    0   N/A  N/A     22240      C   /opt/conda/bin/python3.10                 18054MiB |\n",
    "# |    1   N/A  N/A     23207      C   /usr/bin/python3                           4984MiB |\n",
    "# # +---------------------------------------------------------------------------------------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "744dfb365db3e551",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T22:22:30.816098Z",
     "start_time": "2024-12-18T22:22:30.780340Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_aqw' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[19], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m reset_kernel( [ \u001B[43mmodel_aqw\u001B[49m, tokenizer_awq ] )\n",
      "\u001B[0;31mNameError\u001B[0m: name 'model_aqw' is not defined"
     ]
    }
   ],
   "source": [
    "reset_kernel( [ model_aqw, tokenizer_awq ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27f1cb30653b65f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## GPU ram after loading AWQ model: ~83 Tokens/s!\n",
    "```\n",
    "|=========================================+======================+======================|\n",
    "|   0  NVIDIA GeForce RTX 4090        On  | 00000000:01:00.0 Off |                  Off |\n",
    "|  0%   42C    P2              70W / 450W |  20240MiB / 24564MiB |      0%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "\n",
    "Generating responses for 100 rows... Done! in 50 seconds\n",
    "[502.1] ms per item\n",
    "\n",
    "------------------------------------------------------------------------------------------------------------------------\n",
    "- Validation Stats for `mistralai/Mistral-7B-Instruct-v0.2-AWQ` on TGI:3000\n",
    "------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "               Is valid xml 100.0%\n",
    "          Contains response 100.0%\n",
    " Contains <browser-command> 100.0%\n",
    "            Contains <args> 100.0%\n",
    "          Response is exact 100.0%\n",
    "Response has correct values 100.0%\n",
    " Browser command is correct 100.0%\n",
    "            Args is correct 100.0%\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f6cb1c66d18b0d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## See: [Phind advice for freeing GPU RAM](https://www.phind.com/search?cache=kh81ys0uelwxs8zpykdzv0d8)\n",
    "### It worked... Once?!?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b51231c842d4c0e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T20:24:06.442933Z",
     "start_time": "2024-01-24T20:24:06.435744Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Accomplishes the same thing\n",
    "\n",
    "dupt.release_gpu_memory( model_aqw )\n",
    "\n",
    "# import gc\n",
    "# import torch\n",
    "# \n",
    "# model_aqw.device = torch.device( \"cpu\" )\n",
    "# tokenizer_awq.device = torch.device( \"cpu\" )\n",
    "# \n",
    "# model_aqw     = None\n",
    "# tokenizer_awq = None\n",
    "# \n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefd53be69f5cee0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
