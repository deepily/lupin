{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a93e67c6-3cda-4283-962d-68d02521e9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install groq\n",
    "\n",
    "# ! pip install google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c05dc51c1556825",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-08T17:07:28.423166Z",
     "start_time": "2024-02-08T17:07:28.401730Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ad17d259f359e1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-08T17:07:47.071504Z",
     "start_time": "2024-02-08T17:07:47.040200Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/var/model/genie-in-the-box/src\n",
      "/var/model/genie-in-the-box/src\n"
     ]
    }
   ],
   "source": [
    "print( os.getcwd() )\n",
    "# change working directory\n",
    "os.chdir( \"/var/model/genie-in-the-box/src\" )\n",
    "print( os.getcwd() )\n",
    "import lib.utils.util as du\n",
    "import lib.utils.util_xml as dux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cff25142-898f-4571-833c-74c0a5fb00cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[ \"GIB_CONFIG_MGR_CLI_ARGS\" ] = \"config_path=/src/conf/gib-app.ini splainer_path=/src/conf/gib-app-splainer.ini config_block_id=Genie+in+the+Box:+Development\"\n",
    "os.environ[ \"GENIE_IN_THE_BOX_ROOT\" ] = \"/var/model/genie-in-the-box\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c02262a8-66c0-4261-a34e-1ee5dfef09d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config_path=/src/conf/gib-app.ini splainer_path=/src/conf/gib-app-splainer.ini config_block_id=Genie+in+the+Box:+Development\n"
     ]
    }
   ],
   "source": [
    "! echo $GIB_CONFIG_MGR_CLI_ARGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55896ef1-4647-47d0-8320-edb9291326be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/var/model/genie-in-the-box\n"
     ]
    }
   ],
   "source": [
    "! echo $GENIE_IN_THE_BOX_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "984563e3206b751d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-08T17:10:17.174879Z",
     "start_time": "2024-02-08T17:10:17.069518Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reusing ConfigurationManager() singleton...\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "from lib.utils.util_stopwatch import Stopwatch\n",
    "from ephemera.prompts.xml_fine_tuning_prompt_generator import XmlFineTuningPromptGenerator\n",
    "\n",
    "xml_ftp_generator = XmlFineTuningPromptGenerator( tgi_url=\"http://172.17.0.4:3000\", debug=False, silent=True, init_prompt_templates=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "10d679ba69d718b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-08T19:30:19.565477Z",
     "start_time": "2024-02-08T19:30:19.548462Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "path_prefix = \"/var/model/genie-in-the-box\"\n",
    "sql_prompt_template    = du.get_file_as_string( path_prefix + \"/src/conf/prompts/sql-proofreading-template.txt\" )\n",
    "python_prompt_template = du.get_file_as_string( path_prefix + \"/src/conf/prompts/python-proofreading-template.txt\" )\n",
    "# for line in sql_prompt_template.split( \"\\n\" ): print( line ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178ebee86ecaf459",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-08T19:32:38.083606Z",
     "start_time": "2024-02-08T19:32:37.939300Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%autoreload\n",
    "context     = \"def increment_by_n( self, foo, n ):\"\n",
    "# description = \"Add n to foo. temp value = input plus n.\"\n",
    "# description = \"I want to add n to foo. temp value = foo plus n.\"\n",
    "description = \"I want to create a method signature. def increment by N ( self,foo,n )\"\n",
    "python_prompt  = python_prompt_template.format( voice_command=description )\n",
    "for line in python_prompt.split( \"\\n\" ): print( line )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5a377453bf467f4d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-08T19:32:41.834799Z",
     "start_time": "2024-02-08T19:32:40.775780Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asking LLM [Phind-CodeLlama-34B-v2]...\n",
      ".........................\n",
      "Asking LLM [Phind-CodeLlama-34B-v2]... Done! in 1,008 ms\n",
      "Tokens per second [24.8]\n",
      "<response><python>def increment(self, foo, n):</python></response>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'def increment(self, foo, n):'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%autoreload\n",
    "response = xml_ftp_generator.query_llm_tgi( python_prompt, model_name=\"Phind-CodeLlama-34B-v2\", max_new_tokens=1024, temperature=0.25, top_k=10, top_p=0.9, silent=False )\n",
    "# print( response )\n",
    "response = dux.strip_all_white_space( response )\n",
    "print( response )\n",
    "python = dux.get_value_by_xml_tag_name( response, \"python\" )\n",
    "python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0026ff32308291",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-08T17:50:08.758397Z",
     "start_time": "2024-02-08T17:50:08.731787Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%autoreload\n",
    "description = \"I want to select all the countries whose names contain the letter 'x'. select name from country where name like 'percent X percent '\"\n",
    "sql_prompt  = sql_prompt_template.format( voice_command=description )\n",
    "for line in sql_prompt.split( \"\\n\" ): print( line )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d8d04460e2a6091",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-08T17:50:13.626069Z",
     "start_time": "2024-02-08T17:50:10.844551Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asking LLM [Phind-CodeLlama-34B-v2]...\n",
      "..........................\n",
      "Asking LLM [Phind-CodeLlama-34B-v2]... Done! in 1,025 ms\n",
      "Tokens per second [25.4]\n",
      "<response>\n",
      "    <sql>SELECT name FROM country WHERE name LIKE '%x%'</sql>\n",
      "</response>\n",
      "<response><sql>SELECT name FROM country WHERE name LIKE '%x%'</sql></response>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"SELECT name FROM country WHERE name LIKE '%x%'\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%autoreload\n",
    "response = xml_ftp_generator.query_llm_tgi( sql_prompt, model_name=\"Phind-CodeLlama-34B-v2\", max_new_tokens=1024, temperature=0.25, top_k=10, top_p=0.9, silent=False )\n",
    "print( response )\n",
    "response = dux.strip_all_white_space( response )\n",
    "print( response )\n",
    "sql = dux.get_value_by_xml_tag_name( response, \"sql\" )\n",
    "sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "850b013a10bdac5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-07T19:21:18.427345Z",
     "start_time": "2024-02-07T19:21:18.413660Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "foo = \"\"\"\n",
    "The corrected Python code would be:\n",
    "\n",
    "```python\n",
    "tail_seeker = linked_list.head_node\n",
    "```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2222aa2c84a2f714",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-07T19:31:37.613743Z",
     "start_time": "2024-02-07T19:31:37.601107Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'asdfasdfasdfasdf'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _extract_string_from_backticked_llm_output( raw_string, tag_name=\"python\" ):\n",
    "    \n",
    "    try:\n",
    "        return raw_string.split( \"```\" + tag_name )[ 1 ].split( \"```\" )[ 0 ].strip()\n",
    "    except:\n",
    "        print( f\"Error extracting string from ```{tag_name} ...CODE...```, returning raw_string\" )\n",
    "        return raw_string\n",
    "    \n",
    "# _extract_string_from_backticked_llm_output( foo, tag_name=\"python\" )\n",
    "_extract_string_from_backticked_llm_output( \"```sql asdfasdfasdfasdf```\", tag_name=\"sql\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7bbb131aadef752e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-05T18:24:06.884747Z",
     "start_time": "2024-02-05T18:24:06.756050Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 88M\n",
      "drwxr--r-- 4 1001 1001 4.0K Sep 21 19:44 .\n",
      "drwxr--r-- 5 1001 1001 4.0K Sep 21 17:01 ..\n",
      "-rwxr--r-- 1 1001 1001 6.1K Jun 20  2023 .DS_Store\n",
      "-rwxr--r-- 1 1001 1001 4.0K Jun 20  2023 ._.DS_Store\n",
      "-rw-rw-r-- 1 1001 1001    0 Oct 13  2023 __init__.py\n",
      "drwxrwxr-x 2 1001 1001 4.0K Oct 13  2023 fine-tuning-results\n",
      "drwxrwxr-x 2 1001 1001 4.0K Oct 13  2023 jsonl\n",
      "-rw-rw-r-- 1 1001 1001 3.2K Feb 28  2024 munger.py\n",
      "-rw-rw-r-- 1 1001 1001 2.6K Feb 28  2024 placeholders-calendaring-dates-and-times.txt\n",
      "-rw-rw-r-- 1 1001 1001  414 Feb 28  2024 placeholders-calendaring-events.txt\n",
      "-rw-rw-r-- 1 1001 1001 5.4K Feb 28  2024 placeholders-calendaring-locations.txt\n",
      "-rw-rw-r-- 1 1001 1001 1.4K Feb 28  2024 placeholders-calendaring-people.txt\n",
      "-rw-rw-r-- 1 1001 1001 3.6K Feb 28  2024 placeholders-cities-and-countries.txt\n",
      "-rw-rw-r-- 1 1001 1001  360 Apr 23 02:06 placeholders-interjections-um-er-uh-etc.txt\n",
      "-rw-rw-r-- 1 1001 1001  342 Apr 23 02:06 placeholders-receptionist-names.txt\n",
      "-rw-rw-r-- 1 1001 1001 1.7K Apr 23 02:06 placeholders-receptionist-salutations.txt\n",
      "-rw-rw-r-- 1 1001 1001  982 Mar  7  2024 placeholders-receptionist-titles.txt\n",
      "-rw-rw-r-- 1 1001 1001  36K Feb 28  2024 placeholders-search-terms.txt\n",
      "-rw-rw-r-- 1 1001 1001  18K Feb 28  2024 synthetic-data-agent-routing-calendaring.txt\n",
      "-rw-rw-r-- 1 1001 1001 5.6K Feb 28  2024 synthetic-data-agent-routing-date-and-time.txt\n",
      "-rwxr--r-- 1 1001 1001  28K Sep 21 02:03 synthetic-data-agent-routing-math.txt\n",
      "-rw-rw-r-- 1 1001 1001  13K Apr 23 02:06 synthetic-data-agent-routing-receptionist.txt\n",
      "-rw-rw-r-- 1 1001 1001  25K Feb 28  2024 synthetic-data-agent-routing-todo-lists.txt\n",
      "-rw-rw-r-- 1 1001 1001  11K Feb 28  2024 synthetic-data-agent-routing-weather.txt\n",
      "-rw-rw-r-- 1 1001 1001  20K Apr 23 02:06 synthetic-data-agent-search-static-vs-dynamic.txt\n",
      "-rw-rw-r-- 1 1001 1001  15K Apr 23 02:06 synthetic-data-agent-search-static-vs-dynamic.xml\n",
      "-rw-rw-r-- 1 1001 1001 7.0K Jan 24  2024 synthetic-data-load-url-current-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001 7.3K Oct 13  2023 synthetic-data-load-url-new-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001  11K Feb 28  2024 synthetic-data-none-of-the-above.txt\n",
      "-rw-rw-r-- 1 1001 1001  12K Jan 24  2024 synthetic-data-search-clipboard-GENERIC-current-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001  12K Jan 24  2024 synthetic-data-search-clipboard-GENERIC-new-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001  11K Jan 24  2024 synthetic-data-search-clipboard-google-in-current-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001  11K Jan 24  2024 synthetic-data-search-clipboard-google-in-new-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001  12K Jan 24  2024 synthetic-data-search-clipboard-google-scholar-in-current-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001  12K Jan 24  2024 synthetic-data-search-clipboard-google-scholar-in-new-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001  14K Jan 24  2024 synthetic-data-search-clipboard-in-current-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001  13K Jan 24  2024 synthetic-data-search-clipboard-in-new-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001  11K Apr 23 02:06 synthetic-data-search-clipboard-kagi-in-current-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001 9.8K Apr 23 02:06 synthetic-data-search-clipboard-kagi-in-new-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001  12K Jan 24  2024 synthetic-data-search-clipboard-perplexity-in-current-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001  12K Jan 24  2024 synthetic-data-search-clipboard-perplexity-in-new-tab.txt\n",
      "-rw-rw-rw- 1 1001 1001  11K Sep 21 17:51 synthetic-data-search-clipboard-phind-in-current-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001  11K Feb 28  2024 synthetic-data-search-clipboard-phind-in-new-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001 8.5K Jan 24  2024 synthetic-data-search-google-in-current-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001  11K Oct 13  2023 synthetic-data-search-google-in-new-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001 8.5K Oct 13  2023 synthetic-data-search-google-scholar-in-current-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001  11K Oct 13  2023 synthetic-data-search-google-scholar-in-new-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001 7.6K Oct 13  2023 synthetic-data-search-in-current-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001 8.9K Jan 24  2024 synthetic-data-search-in-new-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001 7.9K Apr 23 02:06 synthetic-data-search-kagi-in-current-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001 9.5K Apr 23 02:06 synthetic-data-search-kagi-in-new-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001 4.6K Jan 24  2024 synthetic-data-search-perplexity-in-current-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001 5.5K Jan 24  2024 synthetic-data-search-perplexity-in-new-tab.txt\n",
      "-rw-rw-rw- 1 1001 1001 4.2K Sep 21 17:51 synthetic-data-search-phind-in-current-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001 5.2K Feb 28  2024 synthetic-data-search-phind-in-new-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001  723 Oct 13  2023 training-commands.map\n",
      "-rw-rw-rw- 1 1001 1001 7.0M Feb  5  2024 voice-commands-xml-test-gpt.jsonl\n",
      "-rw-rw-rw- 1 1001 1001  56M Feb  5  2024 voice-commands-xml-train-gpt.jsonl\n",
      "-rw-rw-rw- 1 1001 1001  25M Feb  5  2024 voice-commands-xml-validate-gpt.jsonl\n"
     ]
    }
   ],
   "source": [
    "! ls -alh /var/model/genie-in-the-box/src/ephemera/prompts/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9feb9097838c568d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T15:40:33.253864Z",
     "start_time": "2024-02-01T15:40:33.140365Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 116K\n",
      "drwxr--r-- 11 1001 1001 4.0K Sep 15 02:11 .\n",
      "drwxr--r-- 37 1001 1001 4.0K May 11 01:17 ..\n",
      "-rw-r--r--  1 1001 1001  11K Sep  7 23:49 .DS_Store\n",
      "-rw-r--r--  1 1001 1001 4.0K Oct  9  2023 ._.DS_Store\n",
      "-rw-r--r--  1 1001 1001 4.0K Sep  8 00:02 ._.idea\n",
      "drwxr--r--  8 1001 1001 4.0K Sep 21 16:41 .git\n",
      "-rwxrwxr-x  1 1001 1001 1.3K Sep 15 02:11 .gitignore\n",
      "drwxr-xr-x  3 1001 1001 4.0K Sep 21 19:57 .idea\n",
      "-rwxr--r--  1 1001 1001  190 Sep 26  2023 .mailmap\n",
      "-rw-rw-r--  1 1001 1001  34K Oct 13  2023 LICENSE\n",
      "-rw-rw-r--  1 1001 1001 3.5K Apr 24 01:38 README.md\n",
      "drwxr--r--  2 1001 1001 4.0K Jun 14  2023 __pycache__\n",
      "drwxr--r--  6 1001 1001 4.0K Nov 20  2023 docker\n",
      "drwxr--r--  3 1001 1001 4.0K Mar 17  2023 genie.app\n",
      "drwxr--r--  4 1001 1001 4.0K Sep 21 00:24 io\n",
      "drwxr-xr-x  3 1001 1001 4.0K Jan 24  2024 models\n",
      "-rw-------  1 root root  882 Sep 26  2023 nohup.out\n",
      "drwxr--r-- 11 1001 1001 4.0K Sep 15 23:20 src\n",
      "drwxr--r--  6 1001 1001 4.0K Sep  7 23:48 venv\n"
     ]
    }
   ],
   "source": [
    "! ls -alh /var/model/genie-in-the-box"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab581df1cd9e7d1a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Re-purpose google search queries for use with PHIND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "45bedec696c7bf50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-23T21:12:34.872001Z",
     "start_time": "2024-01-23T21:12:34.863464Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "os.chdir( \"/var/model/genie-in-the-box/src/ephemera/prompts/data\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8b07fa466fd3fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-23T21:20:02.529140Z",
     "start_time": "2024-01-23T21:20:02.448387Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "file = \"/var/model/genie-in-the-box/src/ephemera/prompts/data/synthetic-data-search-clipboard-phind-in-current-tab.txt\"\n",
    "lines = du.get_file_as_list( file, clean=True )#[ :10 ]\n",
    "\n",
    "# 1/4 of the time replace PHIND with find, 1/4 of the time with p find and 1/4 of the time do nothing\n",
    "for idx, line in enumerate( lines ):\n",
    "    \n",
    "    if line.find( \"phind\" ) > -1:\n",
    "        if random.random() < 0.25:\n",
    "            line = line.replace( \"phind\", \"find\" )\n",
    "        elif random.random() < 0.50:\n",
    "            line = line.replace( \"phind\", \"p find\" )\n",
    "        elif random.random() < 0.75:\n",
    "            line = line.replace( \"phind\", \"pea find\" )\n",
    "        else:\n",
    "            line = line.replace( \"phind\", \"phind\" )\n",
    "            \n",
    "    lines[ idx ] = line\n",
    "    \n",
    "for line in lines: print( line )\n",
    "\n",
    "du.write_lines_to_file( file, lines, strip_blank_lines=True, world_read_write=True )\n",
    "what does open ai mean to you?\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a2d6fffb020a05e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T21:11:21.842925Z",
     "start_time": "2024-01-22T21:11:21.756428Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%autoreload\n",
    "file = \"/var/model/genie-in-the-box/src/ephemera/prompts/data/synthetic-data-search-phind-in-current-tab.txt\"\n",
    "lines = du.get_file_as_list( file, clean=True )#[ :10 ]\n",
    "du.write_lines_to_file( file, lines, strip_blank_lines=True, world_read_write=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f2a3c680fe730fc4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T15:30:08.969522Z",
     "start_time": "2024-01-17T15:30:08.962515Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# associate commands with files containing examples\n",
    "browser_commands = {\n",
    "    \"search new tab\": \"/src/ephemera/prompts/data/synthetic-data-search-in-new-tab.txt\",\n",
    "    \"search current tab\": \"/src/ephemera/prompts/data/synthetic-data-search-in-current-tab.txt\",\n",
    "    \"search google new tab\": \"/src/ephemera/prompts/data/synthetic-data-search-google-in-new-tab.txt\",\n",
    "    \"search google current tab\": \"/src/ephemera/prompts/data/synthetic-data-search-google-in-current-tab.txt\",\n",
    "    \"search google scholar new tab\": \"/src/ephemera/prompts/data/synthetic-data-search-google-scholar-in-new-tab.txt\",\n",
    "    \"search google scholar current tab\": \"/src/ephemera/prompts/data/synthetic-data-search-google-scholar-in-current-tab.txt\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cc3f7b6e767d780e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T15:30:11.389747Z",
     "start_time": "2024-01-17T15:30:11.382421Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "prefix = \"/var/model/genie-in-the-box\"\n",
    "# prefix = du.get_project_root()\n",
    "\n",
    "for command in browser_commands.keys():\n",
    "    print( os.path.exists( prefix + browser_commands[ command ] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T15:32:51.484042Z",
     "start_time": "2024-01-17T15:32:51.424592Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SEARCH_TERMS new tab search',\n",
       " 'New tab, search SEARCH_TERMS',\n",
       " 'Open new tab, search for SEARCH_TERMS',\n",
       " 'Pull up SEARCH_TERMS in another tab',\n",
       " 'Find SEARCH_TERMS in a new tab',\n",
       " 'Hunt for SEARCH_TERMS, new tab',\n",
       " 'Display SEARCH_TERMS results in new tab',\n",
       " 'Do a search for SEARCH_TERMS and show results in another tab',\n",
       " 'Execute SEARCH_TERMS search in a new tab',\n",
       " 'Seek SEARCH_TERMS in a fresh tab']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_lines = du.get_file_as_list( prefix + \"/src/ephemera/prompts/data/synthetic-data-search-in-new-tab.txt\", clean=True, randomize=False )[ 0:100 ]\n",
    "raw_lines[ 0:10 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "39a8e819c4e89638",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T15:33:29.786637Z",
     "start_time": "2024-01-17T15:33:29.771980Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def get_search_terms( requested_length ):\n",
    "    \n",
    "    # Load search terms file\n",
    "    search_terms = du.get_file_as_list( prefix + \"/src/ephemera/prompts/data/placeholders-search-terms.txt\", lower_case=False, clean=True, randomize=True )\n",
    "    \n",
    "    # If we don't have enough search terms, append copies of the search term list until we do\n",
    "    while requested_length > len( search_terms ):\n",
    "        search_terms += search_terms\n",
    "        \n",
    "    # Truncate the search terms list to equal the requested len\n",
    "    search_terms = search_terms[ :requested_length ]\n",
    "    \n",
    "    return search_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f16cbbcf81d784f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T15:33:31.591200Z",
     "start_time": "2024-01-17T15:33:31.580223Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AssertionError',\n",
       " 'AI in content moderation',\n",
       " 'Handling JSON data in Pandas',\n",
       " 'RuntimeWarning',\n",
       " 'Voice Recognition technologies: What are the latest advancements in voice recognition technology?',\n",
       " 'How do you handle broken pipe errors in Python, especially in network communications?',\n",
       " 'What are the best practices for handling reset connections in network communications in Python?',\n",
       " 'What are common causes and solutions for errors related to incorrect syntax in Python?',\n",
       " 'Cross-tabulation in Pandas',\n",
       " 'Pandas and geospatial data']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_terms = get_search_terms( len( raw_lines ) )\n",
    "search_terms[ 0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b538b5f3645f851f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T15:33:39.298286Z",
     "start_time": "2024-01-17T15:33:39.287017Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'search new tab', 'search current tab', 'search google new tab', 'search google current tab', 'search google scholar new tab', 'search google scholar current tab' and 'none'\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "command_choices = \"'\" + \"', '\".join( browser_commands.keys() ) + \"' and 'none'\"\n",
    "command_choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d01beabe0654fcf0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T17:58:27.115076Z",
     "start_time": "2024-01-16T17:58:27.041273Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "gpt_instruction = f\"\"\"INSTRUCTIONS:\n",
    "Your job is to discern the intent of a human voice command transcription and translate it into a standardized command that a browser on your computer would understand.\n",
    "\n",
    "You will be given a human voice command as INPUT as well as a list of possible standardized commands. You must choose the correct standardized command from the following list: `{command_choices}`.\n",
    "\n",
    "RESPONSE FORMAT: MUST be returned wrapped in simple, well-formed XML\n",
    "<response>\n",
    "    <browser-command></browser-command>\n",
    "    <args></args>\n",
    "</response>\n",
    "\"\"\"\n",
    "\n",
    "# gpt_instruction = f\"\"\"\n",
    "# INSTRUCTIONS:\n",
    "# Your job is to discern the intent of a human voice command transcription and translate it into a standardized command that a browser on your computer would understand.\n",
    "# \n",
    "# You will be given a human voice command and a list of possible standardized commands. You must choose the correct standardized command from the following list: `{command_choices}`.\n",
    "# \n",
    "# Requirement: You MUST NOT use python code to answer this question.\n",
    "# Requirement: You MUST use your linguistic knowledge and intuition to answer this question.\n",
    "# Hint: Anything that isn't a part of the command itself should be treated as arguments related to the command.\n",
    "# \n",
    "# INPUT FORMAT: You will receive a raw human voice command transcription tagged using simple XML\n",
    "# <human>\n",
    "#     <voice-command></voice-command>\n",
    "# </human>\n",
    "# \n",
    "# RESPONSE FORMAT: MUST be returned wrapped in simple, well-formed XML\n",
    "# <response>\n",
    "#     <browser-command></browser-command>\n",
    "#     <args></args>\n",
    "# </response>\n",
    "# \"\"\"\n",
    "\n",
    "# gpt_input_template = \"\"\"\n",
    "# INPUT:\n",
    "# <human>\n",
    "#     <voice-command>{voice_command}</voice-command>\n",
    "# </human>\n",
    "# \n",
    "# YOUR RESPONSE:\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "output_template = \"\"\"\n",
    "<response>\n",
    "    <browser-command>{browser_command}</browser-command>\n",
    "    <args>{args}</args>\n",
    "</response>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2ea7d8cffc581ac9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T17:58:28.598717Z",
     "start_time": "2024-01-16T17:58:28.588766Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INSTRUCTIONS:\n",
      "Your job is to discern the intent of a human voice command transcription and translate it into a standardized command that a browser on your computer would understand.\n",
      "\n",
      "You will be given a human voice command as INPUT as well as a list of possible standardized commands. You must choose the correct standardized command from the following list: `'search new tab', 'search current tab', 'search google new tab', 'search google current tab', 'search google scholar new tab', 'search google scholar current tab' and 'none'`.\n",
      "\n",
      "RESPONSE FORMAT: MUST be returned wrapped in simple, well-formed XML\n",
      "<response>\n",
      "    <browser-command></browser-command>\n",
      "    <args></args>\n",
      "</response>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for line in gpt_instruction.split( \"\\n\" ):\n",
    "    print( line )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c3537ada45cdf79b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T17:58:33.998508Z",
     "start_time": "2024-01-16T17:58:33.987573Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "instruction_template = \"\"\"Your job is to discern the intent of a human voice command transcription and translate it into a standardized command that a browser on your computer would understand.\n",
    "\n",
    "You will be given a human voice command and a list of possible standardized commands. You must choose the correct standardized command from the following list: `{command_choices}`.\n",
    "\n",
    "Requirement: You MUST NOT use python code to answer this question.\n",
    "Requirement: You MUST use your linguistic knowledge and intuition to answer this question.\n",
    "Hint: Anything that isn't a part of the command itself should be treated as arguments related to the command.\"\"\"\n",
    "\n",
    "input_template = \"\"\"\n",
    "Below is the raw human voice command transcription formatted using simple XML: \n",
    "{human_says}\n",
    "\n",
    "The standardized command that you translate MUST be returned wrapped in simple, well-formed XML:\n",
    "{response_format}\"\"\"\n",
    "\n",
    "human_says_template = \"\"\"\n",
    "<human>\n",
    "    <voice-command>{voice_command}</voice-command>\n",
    "</human>\"\"\"\n",
    "\n",
    "response_format = \"\"\"\n",
    "<response>\n",
    "    <browser-command></browser-command>\n",
    "    <args></args>\n",
    "</response>\n",
    "\n",
    "Requirement: The first word of your response MUST be `<response>`\"\"\"\n",
    "\n",
    "output_template = \"\"\"\n",
    "<response>\n",
    "    <browser-command>{browser_command}</browser-command>\n",
    "    <args>{args}</args>\n",
    "</response>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "60274be0aad8737a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T16:23:24.980628Z",
     "start_time": "2024-01-16T16:23:24.971579Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your job is to discern the intent of a human voice command transcription and translate it into a standardized command that a browser on your computer would understand.\n",
      "\n",
      "You will be given a human voice command and a list of possible standardized commands. You must choose the correct standardized command from the following list: `{command_choices}`.\n",
      "\n",
      "Requirement: You MUST NOT use python code to answer this question.\n",
      "Requirement: You MUST use your linguistic knowledge and intuition to answer this question.\n",
      "Hint: Anything that isn't a part of the command itself should be treated as arguments related to the command.\n"
     ]
    }
   ],
   "source": [
    "for line in instruction_template.split( \"\\n\" ):\n",
    "    print( line )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e7996237fec316c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T15:38:09.116184Z",
     "start_time": "2024-01-16T15:38:09.108137Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Your job is to discern the intent of a human voice command transcription and translate it into a standardized command that a browser on your computer would understand.\\n\\nYou will be given a human voice command and a list of possible standardized commands. You must choose the correct standardized command from the following list: `'search new tab', 'search current tab', 'search google new tab', 'search google current tab', 'search google scholar new tab', 'search google scholar current tab' and 'none'`.\\n\\nRequirement: You MUST NOT use python code to answer this question.\\nRequirement: You MUST use your linguistic knowledge and intuition to answer this question.\\nHint: Anything that isn't a part of the command itself should be treated as arguments related to the command.\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruction = instruction_template.format( command_choices=command_choices )\n",
    "instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea54f4c983515b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T15:38:12.418421Z",
     "start_time": "2024-01-16T15:38:12.408858Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "50e11006cf38e624",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T15:38:22.252054Z",
     "start_time": "2024-01-16T15:38:22.245528Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def prompt_instruction_format( instruction, input ):\n",
    "\n",
    "    return f\"\"\"### Instruction:\n",
    "Use the Task and Input given below to write a Response that can solve the following Task.\n",
    "\n",
    "### Task:\n",
    "{instruction}\n",
    "\n",
    "### Input:{input}\n",
    "### Response:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6f7dc74ad42f15da",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'voice_command' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[35], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m messages \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m      2\u001B[0m     { \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrole\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msystem\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcontent\u001B[39m\u001B[38;5;124m\"\u001B[39m: gpt_instruction },\n\u001B[0;32m----> 3\u001B[0m     { \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrole\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muser\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcontent\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[43mvoice_command\u001B[49m },\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;66;03m# { \"role\": \"user\", \"content\": gpt_input_template.format( voice_command=voice_command ) },\u001B[39;00m\n\u001B[1;32m      5\u001B[0m     { \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrole\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124massistant\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcontent\u001B[39m\u001B[38;5;124m\"\u001B[39m: output_template\u001B[38;5;241m.\u001B[39mformat( browser_command\u001B[38;5;241m=\u001B[39mbrowser_command, args\u001B[38;5;241m=\u001B[39mterms ) }\n\u001B[1;32m      6\u001B[0m ]\n\u001B[1;32m      7\u001B[0m messages\n",
      "\u001B[0;31mNameError\u001B[0m: name 'voice_command' is not defined"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    { \"role\": \"system\", \"content\": gpt_instruction },\n",
    "    { \"role\": \"user\", \"content\": voice_command },\n",
    "    # { \"role\": \"user\", \"content\": gpt_input_template.format( voice_command=voice_command ) },\n",
    "    { \"role\": \"assistant\", \"content\": output_template.format( browser_command=browser_command, args=terms ) }\n",
    "]\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df63613a4a89fd76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T18:37:52.064488Z",
     "start_time": "2024-01-16T18:37:50.799082Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "instructions = []\n",
    "inputs       = []\n",
    "outputs      = []\n",
    "prompts      = []\n",
    "# gpt_prompts  = []\n",
    "# gpt_inputs   = []\n",
    "gpt_messages = []\n",
    "\n",
    "for browser_command in browser_commands.keys():\n",
    "    \n",
    "    du.print_banner( browser_command, prepend_nl=True, end=\"\\n\" )\n",
    "    \n",
    "    raw_lines = du.get_file_as_list( prefix + browser_commands[ browser_command ], clean=True )\n",
    "    for line in raw_lines[ 0:100 ]: #[ 0:2 ]:#\n",
    "        \n",
    "        # get newly randomized search terms on every iteration\n",
    "        for terms in get_search_terms( len( raw_lines ) ):#[ 0:10 ]: #[ 0:2]:#:\n",
    "            \n",
    "            voice_command = line.replace( \"SEARCH_TERMS\", terms )\n",
    "            instruction   = instruction_template.format( command_choices=command_choices )\n",
    "            human_says    = human_says_template.format( voice_command=voice_command )\n",
    "            input         = input_template.format( human_says=human_says, response_format=response_format )\n",
    "            # gpt_input     = gpt_input_template.format( voice_command=voice_command )\n",
    "            output        = output_template.format( browser_command=browser_command, args=terms )\n",
    "            sql_prompt        = prompt_instruction_format( instruction, input )\n",
    "    \n",
    "            instructions.append( instruction )\n",
    "            inputs.append( input )\n",
    "            outputs.append( output )\n",
    "            prompts.append( sql_prompt )\n",
    "            # gpt_inputs.append( gpt_input )\n",
    "            # gpt_prompts.append( gpt_instruction )\n",
    "            gpt_messages.append( { \n",
    "                \"messages\": [\n",
    "                    { \"role\": \"system\", \"content\": gpt_instruction },\n",
    "                    { \"role\": \"user\", \"content\": voice_command },\n",
    "                    { \"role\": \"assistant\", \"content\": output_template.format( browser_command=browser_command, args=terms ) }\n",
    "                ] \n",
    "            } )\n",
    "    \n",
    "            print( \".\", end=\"\" )\n",
    "        \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4e8325c691d7b7b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T17:58:51.328404Z",
     "start_time": "2024-01-16T17:58:51.315879Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': \"INSTRUCTIONS:\\nYour job is to discern the intent of a human voice command transcription and translate it into a standardized command that a browser on your computer would understand.\\n\\nYou will be given a human voice command as INPUT as well as a list of possible standardized commands. You must choose the correct standardized command from the following list: `'search new tab', 'search current tab', 'search google new tab', 'search google current tab', 'search google scholar new tab', 'search google scholar current tab' and 'none'`.\\n\\nRESPONSE FORMAT: MUST be returned wrapped in simple, well-formed XML\\n<response>\\n    <browser-command></browser-command>\\n    <args></args>\\n</response>\\n\"},\n",
       " {'role': 'user', 'content': 'what is the stock market? new tab search'},\n",
       " {'role': 'assistant',\n",
       "  'content': '\\n<response>\\n    <browser-command>search new tab</browser-command>\\n    <args>what is the stock market?</args>\\n</response>'}]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_messages[ 0 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f816307c20687d3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T17:59:05.631629Z",
     "start_time": "2024-01-16T17:59:05.622910Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "Use the Task and Input given below to write a Response that can solve the following Task.\n",
      "\n",
      "### Task:\n",
      "Your job is to discern the intent of a human voice command transcription and translate it into a standardized command that a browser on your computer would understand.\n",
      "\n",
      "You will be given a human voice command and a list of possible standardized commands. You must choose the correct standardized command from the following list: `'search new tab', 'search current tab', 'search google new tab', 'search google current tab', 'search google scholar new tab', 'search google scholar current tab' and 'none'`.\n",
      "\n",
      "Requirement: You MUST NOT use python code to answer this question.\n",
      "Requirement: You MUST use your linguistic knowledge and intuition to answer this question.\n",
      "Hint: Anything that isn't a part of the command itself should be treated as arguments related to the command.\n",
      "\n",
      "### Input:\n",
      "Below is the raw human voice command transcription formatted using simple XML: \n",
      "\n",
      "<human>\n",
      "    <voice-command>what is the stock market? new tab search</voice-command>\n",
      "</human>\n",
      "\n",
      "The standardized command that you translate MUST be returned wrapped in simple, well-formed XML:\n",
      "\n",
      "<response>\n",
      "    <browser-command></browser-command>\n",
      "    <args></args>\n",
      "</response>\n",
      "\n",
      "Requirement: The first word of your response MUST be `<response>`\n",
      "### Response:\n"
     ]
    }
   ],
   "source": [
    "for line in prompts[ 0 ].split( \"\\n\" ):\n",
    "    print( line )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ff379435dddb3a10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T16:33:08.736632Z",
     "start_time": "2024-01-16T16:33:08.724512Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your job is to discern the intent of a human voice command transcription and translate it into a standardized command that a browser on your computer would understand.\n",
      "\n",
      "You will be given a human voice command and a list of possible standardized commands. You must choose the correct standardized command from the following list: `'search new tab', 'search current tab', 'search google new tab', 'search google current tab', 'search google scholar new tab', 'search google scholar current tab' and 'none'`.\n",
      "\n",
      "Requirement: You MUST NOT use python code to answer this question.\n",
      "Requirement: You MUST use your linguistic knowledge and intuition to answer this question.\n",
      "Hint: Anything that isn't a part of the command itself should be treated as arguments related to the command.\n",
      "\n",
      "Below is the raw human voice command transcription formatted using simple XML: \n",
      "\n",
      "<human>\n",
      "    <voice-command>best hiking trails near me new tab search</voice-command>\n",
      "</human>\n",
      "\n",
      "The standardized command that you translate MUST be returned wrapped in simple, well-formed XML:\n",
      "\n",
      "<response>\n",
      "    <browser-command></browser-command>\n",
      "    <args></args>\n",
      "</response>\n",
      "\n",
      "Requirement: The first word of your response MUST be `<response>`\n",
      "\n",
      "<response>\n",
      "    <browser-command>search new tab</browser-command>\n",
      "    <args>best hiking trails near me</args>\n",
      "</response>\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "\n",
    "for line in instructions[ idx ].split( \"\\n\" ):\n",
    "    print( line )\n",
    "    \n",
    "for line in inputs[ idx ].split( \"\\n\" ):\n",
    "    print( line )\n",
    "    \n",
    "for line in outputs[ idx ].split( \"\\n\" ):\n",
    "    print( line )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "10936b21cad13826",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T16:33:51.885598Z",
     "start_time": "2024-01-16T16:33:51.870086Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def inject_space( line ):\n",
    "    if random.random() < 0.5:\n",
    "        index = random.randint( 0, len( line ) - 1 )\n",
    "        return line[ :index ] + ' ' + line[ index: ]\n",
    "    else:\n",
    "        return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "69cd9131a026aa59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T17:59:15.166474Z",
     "start_time": "2024-01-16T17:59:15.018776Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [{'role': 'system', 'content': 'INSTRUCTIONS:\n",
       "...\n",
       "1    [{'role': 'system', 'content': 'INSTRUCTIONS:\n",
       "...\n",
       "2    [{'role': 'system', 'content': 'INSTRUCTIONS:\n",
       "...\n",
       "3    [{'role': 'system', 'content': 'INSTRUCTIONS:\n",
       "...\n",
       "4    [{'role': 'system', 'content': 'INSTRUCTIONS:\n",
       "...\n",
       "Name: gpt_messages, dtype: object"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qna_df = pd.DataFrame( { \"instruction\": instructions, \"input\": inputs, \"output\": outputs, \"prompt\": prompts, \"gpt_message\": gpt_messages } )\n",
    "\n",
    "# Perturb the answer to create a faked response  \n",
    "qna_df[ \"response\" ] = qna_df[ \"output\" ].apply( lambda cell: inject_space( cell ) )\n",
    "qna_df.gpt_messages.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6c0eeb4ab6429c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Validate the structure of the xml response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9cf0e4b55266f65b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T17:18:22.451767Z",
     "start_time": "2024-01-16T17:18:20.571529Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xmlschema in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
      "Requirement already satisfied: elementpath<5.0.0,>=4.1.5 in /usr/local/lib/python3.10/dist-packages (from xmlschema) (4.1.5)\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0m\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.3.2\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.2\u001B[0m\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpython3 -m pip install --upgrade pip\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install xmlschema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ebae05bcfc626b0c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T17:18:23.722823Z",
     "start_time": "2024-01-16T17:18:23.716700Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "from xmlschema import XMLSchema\n",
    "\n",
    "xml_str_1 = \"\"\"\n",
    "<response>\n",
    "  <browser-command></browser-command>\n",
    "  <args></args>\n",
    "</response>\n",
    "\"\"\"\n",
    "xml_str_2 = \"\"\"\n",
    "<response>\n",
    "  <browser_command></browser_command>\n",
    "  <args></args>\n",
    "</response>\n",
    "\"\"\"\n",
    "xml_str_3 = \"\"\"\n",
    "Sure here's your command!\n",
    "<response>\n",
    "  <browser-command></browser-command>\n",
    "  <args></args>\n",
    "</response>\n",
    "\"\"\"\n",
    "\n",
    "xml_str_4 = \"\"\"\n",
    "```xml\n",
    "<response>\n",
    "  <browser-command></browser-command>\n",
    "  <args></args>\n",
    "</response>\n",
    "\"\"\"\n",
    "\n",
    "xsd_string = \"\"\"\n",
    "<xs:schema xmlns:xs=\"http://www.w3.org/2001/XMLSchema\">\n",
    "  <xs:element name=\"response\">\n",
    "    <xs:complexType>\n",
    "      <xs:sequence>\n",
    "        <xs:element name=\"browser-command\" type=\"xs:string\"/>\n",
    "        <xs:element name=\"args\" type=\"xs:string\"/>\n",
    "      </xs:sequence>\n",
    "    </xs:complexType>\n",
    "  </xs:element>\n",
    "</xs:schema>\n",
    "\"\"\"\n",
    "\n",
    "def is_valid_xml( xml_str, schema ):\n",
    "    \n",
    "    try:\n",
    "        schema.is_valid( xml_str )\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        # print( \"Invalid XML: \" + str( e ) )\n",
    "        return False\n",
    "\n",
    "schema   = XMLSchema( xsd_string )\n",
    "\n",
    "print( is_valid_xml( xml_str_1, schema ) )  # Output: ???\n",
    "print( is_valid_xml( xml_str_2, schema ) )  # Output: ???\n",
    "print( is_valid_xml( xml_str_3, schema ) )  # Output: False\n",
    "print( is_valid_xml( xml_str_4, schema ) )  # Output: False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "476168c0862285f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T17:18:47.496521Z",
     "start_time": "2024-01-16T17:18:25.287001Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>prompt</th>\n",
       "      <th>gpt_messages</th>\n",
       "      <th>response</th>\n",
       "      <th>response_xml_is_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Your job is to discern the intent of a human v...</td>\n",
       "      <td>\\nBelow is the raw human voice command transcr...</td>\n",
       "      <td>\\n&lt;response&gt;\\n    &lt;browser-command&gt;search new ...</td>\n",
       "      <td>### Instruction:\\nUse the Task and Input given...</td>\n",
       "      <td>[{'role': 'system', 'content': '\n",
       "INSTRUCTIONS:...</td>\n",
       "      <td>\\n&lt;response&gt;\\n    &lt;browser-command&gt;search new ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Your job is to discern the intent of a human v...</td>\n",
       "      <td>\\nBelow is the raw human voice command transcr...</td>\n",
       "      <td>\\n&lt;response&gt;\\n    &lt;browser-command&gt;search new ...</td>\n",
       "      <td>### Instruction:\\nUse the Task and Input given...</td>\n",
       "      <td>[{'role': 'system', 'content': '\n",
       "INSTRUCTIONS:...</td>\n",
       "      <td>\\n&lt;response&gt;\\n    &lt;browser-command&gt;search new ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Your job is to discern the intent of a human v...</td>\n",
       "      <td>\\nBelow is the raw human voice command transcr...</td>\n",
       "      <td>\\n&lt;response&gt;\\n    &lt;browser-command&gt;search new ...</td>\n",
       "      <td>### Instruction:\\nUse the Task and Input given...</td>\n",
       "      <td>[{'role': 'system', 'content': '\n",
       "INSTRUCTIONS:...</td>\n",
       "      <td>\\n&lt;response&gt;\\n    &lt;browser-command&gt;search new ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Your job is to discern the intent of a human v...</td>\n",
       "      <td>\\nBelow is the raw human voice command transcr...</td>\n",
       "      <td>\\n&lt;response&gt;\\n    &lt;browser-command&gt;search new ...</td>\n",
       "      <td>### Instruction:\\nUse the Task and Input given...</td>\n",
       "      <td>[{'role': 'system', 'content': '\n",
       "INSTRUCTIONS:...</td>\n",
       "      <td>\\n&lt;response&gt;\\n    &lt;browser-command&gt;search new ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Your job is to discern the intent of a human v...</td>\n",
       "      <td>\\nBelow is the raw human voice command transcr...</td>\n",
       "      <td>\\n&lt;response&gt;\\n    &lt;browser-command&gt;search new ...</td>\n",
       "      <td>### Instruction:\\nUse the Task and Input given...</td>\n",
       "      <td>[{'role': 'system', 'content': '\n",
       "INSTRUCTIONS:...</td>\n",
       "      <td>\\n&lt;response&gt;\\n     &lt;browser-command&gt;search new...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         instruction  \\\n",
       "0  Your job is to discern the intent of a human v...   \n",
       "1  Your job is to discern the intent of a human v...   \n",
       "2  Your job is to discern the intent of a human v...   \n",
       "3  Your job is to discern the intent of a human v...   \n",
       "4  Your job is to discern the intent of a human v...   \n",
       "\n",
       "                                               input  \\\n",
       "0  \\nBelow is the raw human voice command transcr...   \n",
       "1  \\nBelow is the raw human voice command transcr...   \n",
       "2  \\nBelow is the raw human voice command transcr...   \n",
       "3  \\nBelow is the raw human voice command transcr...   \n",
       "4  \\nBelow is the raw human voice command transcr...   \n",
       "\n",
       "                                              output  \\\n",
       "0  \\n<response>\\n    <browser-command>search new ...   \n",
       "1  \\n<response>\\n    <browser-command>search new ...   \n",
       "2  \\n<response>\\n    <browser-command>search new ...   \n",
       "3  \\n<response>\\n    <browser-command>search new ...   \n",
       "4  \\n<response>\\n    <browser-command>search new ...   \n",
       "\n",
       "                                              prompt  \\\n",
       "0  ### Instruction:\\nUse the Task and Input given...   \n",
       "1  ### Instruction:\\nUse the Task and Input given...   \n",
       "2  ### Instruction:\\nUse the Task and Input given...   \n",
       "3  ### Instruction:\\nUse the Task and Input given...   \n",
       "4  ### Instruction:\\nUse the Task and Input given...   \n",
       "\n",
       "                                        gpt_messages  \\\n",
       "0  [{'role': 'system', 'content': '\n",
       "INSTRUCTIONS:...   \n",
       "1  [{'role': 'system', 'content': '\n",
       "INSTRUCTIONS:...   \n",
       "2  [{'role': 'system', 'content': '\n",
       "INSTRUCTIONS:...   \n",
       "3  [{'role': 'system', 'content': '\n",
       "INSTRUCTIONS:...   \n",
       "4  [{'role': 'system', 'content': '\n",
       "INSTRUCTIONS:...   \n",
       "\n",
       "                                            response  response_xml_is_valid  \n",
       "0  \\n<response>\\n    <browser-command>search new ...                   True  \n",
       "1  \\n<response>\\n    <browser-command>search new ...                   True  \n",
       "2  \\n<response>\\n    <browser-command>search new ...                  False  \n",
       "3  \\n<response>\\n    <browser-command>search new ...                   True  \n",
       "4  \\n<response>\\n     <browser-command>search new...                   True  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qna_df[ \"response_xml_is_valid\" ] = qna_df[ \"response\" ].apply( lambda cell: is_valid_xml( cell, schema ) )\n",
    "qna_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1de4a9107307a2fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T17:18:53.756596Z",
     "start_time": "2024-01-16T17:18:53.738684Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7940583333333333"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qna_df.response_xml_is_valid.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cd990344958fe1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T17:18:56.156285Z",
     "start_time": "2024-01-16T17:18:56.135615Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def contains_valid_xml_tag( xml_str, tag_name ):\n",
    "    \n",
    "    try:\n",
    "        return \"<\" + tag_name + \">\" in xml_str and \"</\" + tag_name + \">\" in xml_str\n",
    "    except Exception as e:\n",
    "        return False\n",
    "    \n",
    "contains_valid_xml_tag( \"```xml<response><browser-command></browser-command><args></args></response>\", \"browser-command\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7b5958266e9c1a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Validate the structure of the xml response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a93536b26f1ab89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T17:19:01.200010Z",
     "start_time": "2024-01-16T17:19:01.019929Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "qna_df[ \"contains_response\" ]        = qna_df[ \"response\" ].apply( lambda cell: contains_valid_xml_tag( cell, \"response\" ) )\n",
    "qna_df[ \"contains_browser_command\" ] = qna_df[ \"response\" ].apply( lambda cell: contains_valid_xml_tag( cell, \"browser-command\" ) )\n",
    "qna_df[ \"contains_args\" ]            = qna_df[ \"response\" ].apply( lambda cell: contains_valid_xml_tag( cell, \"args\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4dfecb7ae4d37c63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T17:19:02.639731Z",
     "start_time": "2024-01-16T17:19:02.610123Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9306083333333334\n",
      "0.8822166666666666\n",
      "0.9600916666666667\n"
     ]
    }
   ],
   "source": [
    "print( qna_df[ \"contains_response\" ].mean() )\n",
    "print( qna_df[ \"contains_browser_command\" ].mean() )\n",
    "print( qna_df[ \"contains_args\" ].mean() )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1789a964c4b89ea",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Validate the response by removing all white space and comparing to the original answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "17461163fc843dde",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T17:19:07.167036Z",
     "start_time": "2024-01-16T17:19:07.123514Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<response><browser-command>Whos your favorite browser?</browser-command><args>Bar browser</args></response>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "xml_str = \"\"\"\n",
    "<response>\n",
    "  <browser-command>Whos your favorite browser?</browser-command>\n",
    "  <args>Bar browser</args>\n",
    "</response>\n",
    "\"\"\"\n",
    "\n",
    "# Remove white space outside XML tags\n",
    "xml_str = re.sub(r'>\\s+<', '><', xml_str.strip())\n",
    "\n",
    "print(xml_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dcf24366c637b66b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T17:19:08.851437Z",
     "start_time": "2024-01-16T17:19:08.845905Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def is_response_exact( response, answer ):\n",
    "    \n",
    "    # Remove white space outside XML tags\n",
    "    response = re.sub( r'>\\s+<', '><', response.strip() )\n",
    "    \n",
    "    # Remove white space outside XML tags\n",
    "    answer = re.sub( r'>\\s+<', '><', answer.strip() )\n",
    "    \n",
    "    return response == answer\n",
    "\n",
    "is_response_exact( \"<response><browser-command>Whos your favorite browser?</browser-command><args>Bar browser</args></response>\", \"<response><browser-command>Whos your favorite browser?</browser-command><args>Bar browser</args></response>\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5d7316ca3f07b221",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T17:19:10.278652Z",
     "start_time": "2024-01-16T17:19:10.239047Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dux' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[58], line 11\u001B[0m\n\u001B[1;32m      8\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m<response><browser-command>Whos your favorite browser?</browser-command><args>Bar browser</args></response>\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      9\u001B[0m answer   \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m<response><browser-command>Whos your favorite browser?</browser-command><args>Bar browser</args></response>\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m---> 11\u001B[0m \u001B[38;5;28mprint\u001B[39m( \u001B[43mtag_values_are_equal\u001B[49m\u001B[43m(\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtag_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mbrowser-command\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m)\u001B[49m )\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28mprint\u001B[39m( tag_values_are_equal( response, answer, tag_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124margs\u001B[39m\u001B[38;5;124m\"\u001B[39m ) )\n",
      "Cell \u001B[0;32mIn[58], line 3\u001B[0m, in \u001B[0;36mtag_values_are_equal\u001B[0;34m(response, answer, tag_name)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtag_values_are_equal\u001B[39m( response, answer, tag_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbrowser-command\u001B[39m\u001B[38;5;124m\"\u001B[39m ):\n\u001B[0;32m----> 3\u001B[0m     command_response \u001B[38;5;241m=\u001B[39m \u001B[43mdux\u001B[49m\u001B[38;5;241m.\u001B[39mget_value_by_xml_tag_name( response, tag_name, default_value\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbroken\u001B[39m\u001B[38;5;124m\"\u001B[39m )\n\u001B[1;32m      4\u001B[0m     command_answer   \u001B[38;5;241m=\u001B[39m dux\u001B[38;5;241m.\u001B[39mget_value_by_xml_tag_name(   answer, tag_name, default_value\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbroken\u001B[39m\u001B[38;5;124m\"\u001B[39m )\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m command_response \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbroken\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m command_answer \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbroken\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m command_response \u001B[38;5;241m==\u001B[39m command_answer\n",
      "\u001B[0;31mNameError\u001B[0m: name 'dux' is not defined"
     ]
    }
   ],
   "source": [
    "def tag_values_are_equal( response, answer, tag_name=\"browser-command\" ):\n",
    "    \n",
    "    command_response = dux.get_value_by_xml_tag_name( response, tag_name, default_value=\"broken\" )\n",
    "    command_answer   = dux.get_value_by_xml_tag_name(   answer, tag_name, default_value=\"broken\" )\n",
    "    \n",
    "    return command_response != \"broken\" and command_answer != \"broken\" and command_response == command_answer\n",
    "\n",
    "response = \"<response><browser-command>Whos your favorite browser?</browser-command><args>Bar browser</args></response>\"\n",
    "answer   = \"<response><browser-command>Whos your favorite browser?</browser-command><args>Bar browser</args></response>\"\n",
    "\n",
    "print( tag_values_are_equal( response, answer, tag_name=\"browser-command\" ) )\n",
    "print( tag_values_are_equal( response, answer, tag_name=\"args\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5648ef7387c840f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T17:19:12.220650Z",
     "start_time": "2024-01-16T17:19:12.208324Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def contains_correct_response_values( response, answer ):\n",
    "    \n",
    "    \"\"\"Check to see if the most common formatting error (```xml) is hiding a correct response\"\"\"\n",
    "    response = \"<response>\" + dux.get_value_by_xml_tag_name( response, \"response\", default_value=\"broken\" ) + \"</response>\"\n",
    "    response = dux.get_xml_tag_and_value_by_name( response, \"response\", default_value=\"broken\" )\n",
    "    if response == \"broken\":\n",
    "        return False\n",
    "    \n",
    "    # Remove white space outside XML tags\n",
    "    response = re.sub( r'>\\s+<', '><', response.strip() )\n",
    "    \n",
    "    # Remove white space outside XML tags\n",
    "    answer = re.sub( r'>\\s+<', '><', answer.strip() )\n",
    "    \n",
    "    return response == answer\n",
    "\n",
    "contains_correct_response_values( \"```xml<response><browser-command>Whos your favorite browser?</browser-command><args>Bar browser</args></response>\", \"<response><browser-command>Whos your favorite browser?</browser-command><args>Bar browser</args></response>\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d1f5537569851fb6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T17:19:15.651795Z",
     "start_time": "2024-01-16T17:19:14.536369Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "qna_df[ \"response_is_correct\" ] = qna_df.apply( lambda row: is_response_exact( row[ \"response\" ], row[ \"output\" ] ), axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6776bf949b6f1538",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T17:19:16.867638Z",
     "start_time": "2024-01-16T17:19:16.851340Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5582"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qna_df.response_is_correct.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "26f8b8cef970a601",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T17:19:19.848536Z",
     "start_time": "2024-01-16T17:19:19.798993Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>prompt</th>\n",
       "      <th>gpt_messages</th>\n",
       "      <th>response</th>\n",
       "      <th>response_xml_is_valid</th>\n",
       "      <th>contains_response</th>\n",
       "      <th>contains_browser_command</th>\n",
       "      <th>contains_args</th>\n",
       "      <th>response_is_correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Your job is to discern the intent of a human v...</td>\n",
       "      <td>\\nBelow is the raw human voice command transcr...</td>\n",
       "      <td>\\n&lt;response&gt;\\n    &lt;browser-command&gt;search new ...</td>\n",
       "      <td>### Instruction:\\nUse the Task and Input given...</td>\n",
       "      <td>[{'role': 'system', 'content': '\n",
       "INSTRUCTIONS:...</td>\n",
       "      <td>\\n&lt;response&gt;\\n    &lt;browser-command&gt;search new ...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Your job is to discern the intent of a human v...</td>\n",
       "      <td>\\nBelow is the raw human voice command transcr...</td>\n",
       "      <td>\\n&lt;response&gt;\\n    &lt;browser-command&gt;search new ...</td>\n",
       "      <td>### Instruction:\\nUse the Task and Input given...</td>\n",
       "      <td>[{'role': 'system', 'content': '\n",
       "INSTRUCTIONS:...</td>\n",
       "      <td>\\n&lt;response&gt;\\n    &lt;browser-command&gt;search new ...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Your job is to discern the intent of a human v...</td>\n",
       "      <td>\\nBelow is the raw human voice command transcr...</td>\n",
       "      <td>\\n&lt;response&gt;\\n    &lt;browser-command&gt;search new ...</td>\n",
       "      <td>### Instruction:\\nUse the Task and Input given...</td>\n",
       "      <td>[{'role': 'system', 'content': '\n",
       "INSTRUCTIONS:...</td>\n",
       "      <td>\\n&lt;response&gt;\\n    &lt;browser-command&gt;search new ...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Your job is to discern the intent of a human v...</td>\n",
       "      <td>\\nBelow is the raw human voice command transcr...</td>\n",
       "      <td>\\n&lt;response&gt;\\n    &lt;browser-command&gt;search new ...</td>\n",
       "      <td>### Instruction:\\nUse the Task and Input given...</td>\n",
       "      <td>[{'role': 'system', 'content': '\n",
       "INSTRUCTIONS:...</td>\n",
       "      <td>\\n&lt;response&gt;\\n    &lt;browser-command&gt;search new ...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Your job is to discern the intent of a human v...</td>\n",
       "      <td>\\nBelow is the raw human voice command transcr...</td>\n",
       "      <td>\\n&lt;response&gt;\\n    &lt;browser-command&gt;search new ...</td>\n",
       "      <td>### Instruction:\\nUse the Task and Input given...</td>\n",
       "      <td>[{'role': 'system', 'content': '\n",
       "INSTRUCTIONS:...</td>\n",
       "      <td>\\n&lt;response&gt;\\n    &lt;browser-command&gt;search new ...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Your job is to discern the intent of a human v...</td>\n",
       "      <td>\\nBelow is the raw human voice command transcr...</td>\n",
       "      <td>\\n&lt;response&gt;\\n    &lt;browser-command&gt;search new ...</td>\n",
       "      <td>### Instruction:\\nUse the Task and Input given...</td>\n",
       "      <td>[{'role': 'system', 'content': '\n",
       "INSTRUCTIONS:...</td>\n",
       "      <td>\\n&lt;response&gt;\\n    &lt;browser-command&gt;search new ...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Your job is to discern the intent of a human v...</td>\n",
       "      <td>\\nBelow is the raw human voice command transcr...</td>\n",
       "      <td>\\n&lt;response&gt;\\n    &lt;browser-command&gt;search new ...</td>\n",
       "      <td>### Instruction:\\nUse the Task and Input given...</td>\n",
       "      <td>[{'role': 'system', 'content': '\n",
       "INSTRUCTIONS:...</td>\n",
       "      <td>\\n&lt;response&gt;\\n    &lt;browser-command&gt;search new ...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Your job is to discern the intent of a human v...</td>\n",
       "      <td>\\nBelow is the raw human voice command transcr...</td>\n",
       "      <td>\\n&lt;response&gt;\\n    &lt;browser-command&gt;search new ...</td>\n",
       "      <td>### Instruction:\\nUse the Task and Input given...</td>\n",
       "      <td>[{'role': 'system', 'content': '\n",
       "INSTRUCTIONS:...</td>\n",
       "      <td>\\n&lt;response&gt;\\n    &lt;browser-command&gt;search  new...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Your job is to discern the intent of a human v...</td>\n",
       "      <td>\\nBelow is the raw human voice command transcr...</td>\n",
       "      <td>\\n&lt;response&gt;\\n    &lt;browser-command&gt;search new ...</td>\n",
       "      <td>### Instruction:\\nUse the Task and Input given...</td>\n",
       "      <td>[{'role': 'system', 'content': '\n",
       "INSTRUCTIONS:...</td>\n",
       "      <td>\\n&lt;response &gt;\\n    &lt;browser-command&gt;search new...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Your job is to discern the intent of a human v...</td>\n",
       "      <td>\\nBelow is the raw human voice command transcr...</td>\n",
       "      <td>\\n&lt;response&gt;\\n    &lt;browser-command&gt;search new ...</td>\n",
       "      <td>### Instruction:\\nUse the Task and Input given...</td>\n",
       "      <td>[{'role': 'system', 'content': '\n",
       "INSTRUCTIONS:...</td>\n",
       "      <td>\\n&lt;response&gt;\\n    &lt;browser-command&gt;search new ...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          instruction  \\\n",
       "10  Your job is to discern the intent of a human v...   \n",
       "17  Your job is to discern the intent of a human v...   \n",
       "22  Your job is to discern the intent of a human v...   \n",
       "26  Your job is to discern the intent of a human v...   \n",
       "28  Your job is to discern the intent of a human v...   \n",
       "31  Your job is to discern the intent of a human v...   \n",
       "32  Your job is to discern the intent of a human v...   \n",
       "34  Your job is to discern the intent of a human v...   \n",
       "36  Your job is to discern the intent of a human v...   \n",
       "44  Your job is to discern the intent of a human v...   \n",
       "\n",
       "                                                input  \\\n",
       "10  \\nBelow is the raw human voice command transcr...   \n",
       "17  \\nBelow is the raw human voice command transcr...   \n",
       "22  \\nBelow is the raw human voice command transcr...   \n",
       "26  \\nBelow is the raw human voice command transcr...   \n",
       "28  \\nBelow is the raw human voice command transcr...   \n",
       "31  \\nBelow is the raw human voice command transcr...   \n",
       "32  \\nBelow is the raw human voice command transcr...   \n",
       "34  \\nBelow is the raw human voice command transcr...   \n",
       "36  \\nBelow is the raw human voice command transcr...   \n",
       "44  \\nBelow is the raw human voice command transcr...   \n",
       "\n",
       "                                               output  \\\n",
       "10  \\n<response>\\n    <browser-command>search new ...   \n",
       "17  \\n<response>\\n    <browser-command>search new ...   \n",
       "22  \\n<response>\\n    <browser-command>search new ...   \n",
       "26  \\n<response>\\n    <browser-command>search new ...   \n",
       "28  \\n<response>\\n    <browser-command>search new ...   \n",
       "31  \\n<response>\\n    <browser-command>search new ...   \n",
       "32  \\n<response>\\n    <browser-command>search new ...   \n",
       "34  \\n<response>\\n    <browser-command>search new ...   \n",
       "36  \\n<response>\\n    <browser-command>search new ...   \n",
       "44  \\n<response>\\n    <browser-command>search new ...   \n",
       "\n",
       "                                               prompt  \\\n",
       "10  ### Instruction:\\nUse the Task and Input given...   \n",
       "17  ### Instruction:\\nUse the Task and Input given...   \n",
       "22  ### Instruction:\\nUse the Task and Input given...   \n",
       "26  ### Instruction:\\nUse the Task and Input given...   \n",
       "28  ### Instruction:\\nUse the Task and Input given...   \n",
       "31  ### Instruction:\\nUse the Task and Input given...   \n",
       "32  ### Instruction:\\nUse the Task and Input given...   \n",
       "34  ### Instruction:\\nUse the Task and Input given...   \n",
       "36  ### Instruction:\\nUse the Task and Input given...   \n",
       "44  ### Instruction:\\nUse the Task and Input given...   \n",
       "\n",
       "                                         gpt_messages  \\\n",
       "10  [{'role': 'system', 'content': '\n",
       "INSTRUCTIONS:...   \n",
       "17  [{'role': 'system', 'content': '\n",
       "INSTRUCTIONS:...   \n",
       "22  [{'role': 'system', 'content': '\n",
       "INSTRUCTIONS:...   \n",
       "26  [{'role': 'system', 'content': '\n",
       "INSTRUCTIONS:...   \n",
       "28  [{'role': 'system', 'content': '\n",
       "INSTRUCTIONS:...   \n",
       "31  [{'role': 'system', 'content': '\n",
       "INSTRUCTIONS:...   \n",
       "32  [{'role': 'system', 'content': '\n",
       "INSTRUCTIONS:...   \n",
       "34  [{'role': 'system', 'content': '\n",
       "INSTRUCTIONS:...   \n",
       "36  [{'role': 'system', 'content': '\n",
       "INSTRUCTIONS:...   \n",
       "44  [{'role': 'system', 'content': '\n",
       "INSTRUCTIONS:...   \n",
       "\n",
       "                                             response  response_xml_is_valid  \\\n",
       "10  \\n<response>\\n    <browser-command>search new ...                   True   \n",
       "17  \\n<response>\\n    <browser-command>search new ...                   True   \n",
       "22  \\n<response>\\n    <browser-command>search new ...                   True   \n",
       "26  \\n<response>\\n    <browser-command>search new ...                   True   \n",
       "28  \\n<response>\\n    <browser-command>search new ...                   True   \n",
       "31  \\n<response>\\n    <browser-command>search new ...                   True   \n",
       "32  \\n<response>\\n    <browser-command>search new ...                   True   \n",
       "34  \\n<response>\\n    <browser-command>search  new...                   True   \n",
       "36  \\n<response >\\n    <browser-command>search new...                   True   \n",
       "44  \\n<response>\\n    <browser-command>search new ...                   True   \n",
       "\n",
       "    contains_response  contains_browser_command  contains_args  \\\n",
       "10               True                      True           True   \n",
       "17               True                      True           True   \n",
       "22               True                      True           True   \n",
       "26               True                      True           True   \n",
       "28               True                      True           True   \n",
       "31               True                      True           True   \n",
       "32               True                      True           True   \n",
       "34               True                      True           True   \n",
       "36              False                      True           True   \n",
       "44               True                      True           True   \n",
       "\n",
       "    response_is_correct  \n",
       "10                False  \n",
       "17                False  \n",
       "22                False  \n",
       "26                False  \n",
       "28                False  \n",
       "31                False  \n",
       "32                False  \n",
       "34                False  \n",
       "36                False  \n",
       "44                False  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qna_df[ ( qna_df.response_xml_is_valid == True ) & ( qna_df.response_is_correct == False ) ].head( 10 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9471509827a815a8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# What's the baseline behavior of the Vanilla model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81af6460ca025f6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T15:49:56.709073Z",
     "start_time": "2024-01-17T15:49:56.656996Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# !pip install openai\n",
    "# !pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c34ef51ad77d4f42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-19T18:27:25.005491Z",
     "start_time": "2023-12-19T18:27:24.975954Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%autoreload\n",
    "from huggingface_hub import InferenceClient\n",
    "from lib.utils.util_stopwatch import Stopwatch\n",
    "from lib.agents.agent import XXX_Agent\n",
    "\n",
    "def query_llm_phind( prompt, model=XXX_Agent.PHIND_34B_v2, max_new_tokens=1024, temperature=0.25, top_k=10, top_p=0.9, debug=False, verbose=False, silent=True ):\n",
    "    \n",
    "    timer = Stopwatch( msg=f\"Asking LLM [{model}]...\".format( model ), silent=silent )\n",
    "    \n",
    "    # Get the TGI server URL for this context\n",
    "    # default_url    = self.config_mgr.get( \"tgi_server_codegen_url\", default=None )\n",
    "    # tgi_server_url = du.get_tgi_server_url_for_this_context( default_url=default_url )\n",
    "    \n",
    "    client         = InferenceClient( \"http://172.17.0.4:3000\" )\n",
    "    token_list     = [ ]\n",
    "    ellipsis_count = 0\n",
    "    \n",
    "    if debug:\n",
    "        for line in prompt.split( \"\\n\" ):\n",
    "            print( line )\n",
    "    \n",
    "    for token in client.text_generation(\n",
    "        prompt, max_new_tokens=max_new_tokens, stream=True, temperature=temperature, top_k=top_k, top_p=top_p, stop_sequences=[ \"</response>\" ]\n",
    "    ):\n",
    "        if debug:\n",
    "            print( token, end=\"\" )\n",
    "        else:\n",
    "            if not silent: print( \".\", end=\"\" )\n",
    "            ellipsis_count += 1\n",
    "            if ellipsis_count == 120:\n",
    "                ellipsis_count = 0\n",
    "                print()\n",
    "            \n",
    "        token_list.append( token )\n",
    "        \n",
    "    response = \"\".join( token_list ).strip()\n",
    "    \n",
    "    timer.print( msg=\"Done!\", use_millis=True, prepend_nl=True, end=\"\\n\" )\n",
    "    tokens_per_second = len( token_list ) / ( timer.get_delta_ms() / 1000.0 )\n",
    "    print( f\"Tokens per second [{round( tokens_per_second, 1 )}]\" )\n",
    "            \n",
    "    if debug:\n",
    "        print( f\"Token list length [{len( token_list )}]\" )\n",
    "        if verbose:\n",
    "            for line in response.split( \"\\n\" ):\n",
    "                print( line )\n",
    "    \n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "e405fb4639f53174",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T01:31:22.756566Z",
     "start_time": "2024-01-17T01:31:22.745276Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "from lib.utils.util_stopwatch import Stopwatch\n",
    "\n",
    "def query_llm_openai( messages, model=\"gpt-3.5-turbo-1106\", debug=False, verbose=False ):\n",
    "    \n",
    "    openai.api_key = du.get_api_key( \"openai\", project_root=\"/var/model/genie-in-the-box\" )\n",
    "    \n",
    "    timer = Stopwatch( msg=f\"Asking LLM [{model}]...\".format( model ) )\n",
    "    \n",
    "    response = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "        max_tokens=256,\n",
    "        top_p=1.0,\n",
    "        frequency_penalty=0.0,\n",
    "        presence_penalty=0.0\n",
    "    )\n",
    "    \n",
    "    timer.print( \"Done!\", use_millis=True )\n",
    "    if debug and verbose:\n",
    "        # print( json.dumps( response.to_dict(), indent=4 ) )\n",
    "        print( response )\n",
    "    \n",
    "    return response.choices[ 0 ].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "51a242bf736ad53b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-19T18:21:27.518248Z",
     "start_time": "2023-12-19T18:21:25.804363Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: openai\r\n",
      "Version: 1.5.0\r\n",
      "Summary: The official Python library for the openai API\r\n",
      "Home-page: \r\n",
      "Author: \r\n",
      "Author-email: OpenAI <support@openai.com>\r\n",
      "License: \r\n",
      "Location: /usr/local/lib/python3.10/dist-packages\r\n",
      "Requires: anyio, distro, httpx, pydantic, sniffio, tqdm, typing-extensions\r\n",
      "Required-by: \r\n"
     ]
    }
   ],
   "source": [
    "!pip show openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c2b55c7f68001d05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T17:28:09.012038Z",
     "start_time": "2024-01-16T17:28:08.956798Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "call_counter = 0\n",
    "# rows         = df.shape[ 0 ]\n",
    "\n",
    "def get_response_to_question( prompt, rows ):\n",
    "    \n",
    "    global call_counter\n",
    "    \n",
    "    call_counter += 1\n",
    "    print( f\"On call [{call_counter:03d}] out of [{rows}] = [{round( call_counter / rows * 100.0, 1 )}%]... \", end=\"\" )\n",
    "    \n",
    "    return query_llm_phind( prompt )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "d398e1e54dcd06e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T01:33:09.420395Z",
     "start_time": "2024-01-17T01:33:09.349179Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "call_counter = 0\n",
    "\n",
    "def get_gpt_response_to_question( cell, rows, model=\"gpt-3.5-turbo-1106\" ):\n",
    "    \n",
    "    global call_counter    \n",
    "    call_counter += 1\n",
    "    \n",
    "    print( f\"On call [{call_counter:03d}] out of [{rows}] = [{round( call_counter / rows * 100.0, 1 )}%]... \", end=\"\" )\n",
    "    \n",
    "    return query_llm_openai( cell[ \"messages\" ], model=model )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d6a4435850065f6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T17:28:56.527250Z",
     "start_time": "2024-01-16T17:28:56.515672Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120000, 11)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qna_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "dc726f722b6f687d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T17:53:23.931169Z",
     "start_time": "2024-01-16T17:53:23.920391Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def print_stats( df ):\n",
    "\n",
    "    du.print_banner( \"Stats\" )\n",
    "    print( f\"               Is valid xml {df.response_xml_is_valid.mean() * 100:.1f}%\" )\n",
    "    print( f\"          Contains response {df.contains_response.mean() * 100:.1f}%\" )\n",
    "    print( f\"   Contains browser command {df.contains_browser_command.mean() * 100:.1f}%\" )\n",
    "    print( f\"              Contains args {df.contains_args.mean() * 100:.1f}%\" )\n",
    "    print( f\"          Response is exact {df.response_is_exact.mean() * 100:.1f}%\" )\n",
    "    print( f\"Response has correct values {df.response_has_correct_values.mean() * 100:.1f}%\" )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "9719cdf3539ecb28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T01:32:50.839912Z",
     "start_time": "2024-01-17T01:32:50.787104Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': \"INSTRUCTIONS:\\nYour job is to discern the intent of a human voice command transcription and translate it into a standardized command that a browser on your computer would understand.\\n\\nYou will be given a human voice command as INPUT as well as a list of possible standardized commands. You must choose the correct standardized command from the following list: `'search new tab', 'search current tab', 'search google new tab', 'search google current tab', 'search google scholar new tab', 'search google scholar current tab' and 'none'`.\\n\\nRESPONSE FORMAT: MUST be returned wrapped in simple, well-formed XML\\n<response>\\n    <browser-command></browser-command>\\n    <args></args>\\n</response>\\n\"},\n",
       " {'role': 'user',\n",
       "  'content': 'Big Data analysis techniques: What techniques are essential for effective big data analysis? new tab search'},\n",
       " {'role': 'assistant',\n",
       "  'content': '\\n<response>\\n    <browser-command>search new tab</browser-command>\\n    <args>Big Data analysis techniques: What techniques are essential for effective big data analysis?</args>\\n</response>'}]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_messages[ 0 ][ \"messages\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "56733d5c866f4894",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T18:39:43.473848Z",
     "start_time": "2024-01-16T18:39:43.371521Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 120000 entries, 0 to 119999\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   instruction   120000 non-null  object\n",
      " 1   input         120000 non-null  object\n",
      " 2   output        120000 non-null  object\n",
      " 3   prompt        120000 non-null  object\n",
      " 4   gpt_messages  120000 non-null  object\n",
      "dtypes: object(5)\n",
      "memory usage: 4.6+ MB\n"
     ]
    }
   ],
   "source": [
    "qna_df = pd.DataFrame( { \"instruction\": instructions, \"input\": inputs, \"output\": outputs, \"prompt\": prompts, \"gpt_message\": gpt_messages } )\n",
    "qna_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "c1efe69ae319c998",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T18:39:46.112672Z",
     "start_time": "2024-01-16T18:39:46.083958Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sampled_df = qna_df[ [ \"instruction\", \"input\", \"output\", \"prompt\", \"gpt_message\" ] ].sample( 1000, random_state=42 ).copy()\n",
    "\n",
    "# Split the dataframe into train and (test+validate)\n",
    "train_df, test_validate_df = train_test_split( sampled_df, test_size=0.2, random_state=42 )\n",
    "\n",
    "# Then split (test+validate) into test and validate\n",
    "test_df, validate_df = train_test_split( test_validate_df, test_size=0.5, random_state=42 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "868f24f396c7505a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T18:39:48.819579Z",
     "start_time": "2024-01-16T18:39:48.804756Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 5)\n",
      "(100, 5)\n",
      "(100, 5)\n"
     ]
    }
   ],
   "source": [
    "print( train_df.shape )\n",
    "print( test_df.shape )\n",
    "print( validate_df.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff6879acfe8210d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T01:34:21.033688Z",
     "start_time": "2024-01-17T01:33:21.707289Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "validate_df = validate_responses( validate_df )\n",
    "\n",
    "print_stats( validate_df )\n",
    "\n",
    "# First run before attempting to force valid XML\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# - Stats: Phind\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# \n",
    "#                Is valid xml 0.0%\n",
    "#           Contains response 98.0%\n",
    "#    Contains browser command 98.0%\n",
    "#               Contains args 98.0%\n",
    "#           Response is exact 0.0%\n",
    "# Response has correct values 43.0%\n",
    "\n",
    "# after attempting to force the first token output: <response>\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# - Stats: Phind\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# \n",
    "#                Is valid xml 0.0%\n",
    "#           Contains response 100.0%\n",
    "#    Contains browser command 100.0%\n",
    "#               Contains args 100.0%\n",
    "#           Response is exact 0.0%\n",
    "# Response has correct values 39.0%\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# - Stats: GPT 3.5, before training\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# \n",
    "#                Is valid xml 100.0%\n",
    "#           Contains response 100.0%\n",
    "#    Contains browser command 100.0%\n",
    "#               Contains args 100.0%\n",
    "#           Response is exact  88.0%\n",
    "# Response has correct values  88.0%\n",
    "# \n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# - Stats: GPT 3.5, after OVER training\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# \n",
    "#                Is valid xml 100.0%\n",
    "#           Contains response 100.0%\n",
    "#    Contains browser command 100.0%\n",
    "#               Contains args 100.0%\n",
    "#           Response is exact 57.0%\n",
    "# Response has correct values 57.0%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "f5411f1724fdf5ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T01:36:39.026478Z",
     "start_time": "2024-01-17T01:36:38.979540Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65397     <response>\\n    <browser-command>search google...\n",
       "72078     <response>\\n    <browser-command>search google...\n",
       "59206     <response>\\n    <browser-command>search google...\n",
       "48642     <response>\\n    <browser-command>search google...\n",
       "114768    <response>\\n    <browser-command>search google...\n",
       "Name: response, dtype: object"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_df.response.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "31e8d6c98720256",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T18:01:08.370748Z",
     "start_time": "2024-01-16T18:01:08.361117Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[42], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtrain_df\u001B[49m\u001B[38;5;241m.\u001B[39minfo()\n",
      "\u001B[0;31mNameError\u001B[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "c3ab6816ce6104f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T18:40:04.444143Z",
     "start_time": "2024-01-16T18:40:04.386663Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sampled_df = qna_df[ [ \"instruction\", \"input\", \"output\", \"prompt\", \"gpt_message\" ] ].sample( 1000, random_state=42 ).copy()\n",
    "\n",
    "# Split the dataframe into 80% train and (test+validate)\n",
    "train_df, test_validate_df = train_test_split( sampled_df, test_size=0.2, random_state=42 )\n",
    "\n",
    "# Then split (test+validate) into 10% test and 10% validate\n",
    "test_df, validate_df = train_test_split( test_validate_df, test_size=0.5, random_state=42 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "20d7308b41938b39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T18:26:04.842104Z",
     "start_time": "2024-01-16T18:26:04.772072Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[43], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m project_root \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/var/model/genie-in-the-box\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      2\u001B[0m path \u001B[38;5;241m=\u001B[39m du\u001B[38;5;241m.\u001B[39mget_project_root() \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/src/ephemera/prompts/data/search-xml-train.jsonl\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m----> 3\u001B[0m \u001B[43mtrain_df\u001B[49m\u001B[38;5;241m.\u001B[39mto_json( path, orient\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrecords\u001B[39m\u001B[38;5;124m\"\u001B[39m, lines\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m )\n\u001B[1;32m      4\u001B[0m os\u001B[38;5;241m.\u001B[39mchmod( path, \u001B[38;5;241m0o666\u001B[39m )\n\u001B[1;32m      6\u001B[0m path \u001B[38;5;241m=\u001B[39m du\u001B[38;5;241m.\u001B[39mget_project_root() \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/src/ephemera/prompts/data/search-xml-test.jsonl\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "project_root = \"/var/model/genie-in-the-box\"\n",
    "path = du.get_project_root() + \"/src/ephemera/prompts/data/search-xml-train.jsonl\"\n",
    "train_df.to_json( path, orient=\"records\", lines=True )\n",
    "os.chmod( path, 0o666 )\n",
    "\n",
    "path = du.get_project_root() + \"/src/ephemera/prompts/data/search-xml-test.jsonl\"\n",
    "test_df.to_json( path, orient=\"records\", lines=True )\n",
    "os.chmod( path, 0o666 )\n",
    "\n",
    "path = du.get_project_root() + \"/src/ephemera/prompts/data/search-xml-validate.jsonl\"\n",
    "validate_df.to_json( path, orient=\"records\", lines=True )\n",
    "os.chmod( path, 0o666 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "641d9378e6ea3044",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T18:43:43.897333Z",
     "start_time": "2024-01-16T18:43:43.831279Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[44], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m path \u001B[38;5;241m=\u001B[39m project_root \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/src/ephemera/prompts/data/search-xml-train-gpt.jsonl\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m----> 2\u001B[0m \u001B[43mtrain_df\u001B[49m\u001B[38;5;241m.\u001B[39mgpt_messages\u001B[38;5;241m.\u001B[39mto_json( path, orient\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrecords\u001B[39m\u001B[38;5;124m\"\u001B[39m, lines\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m )\n\u001B[1;32m      3\u001B[0m os\u001B[38;5;241m.\u001B[39mchmod( path, \u001B[38;5;241m0o666\u001B[39m )\n\u001B[1;32m      5\u001B[0m path \u001B[38;5;241m=\u001B[39m project_root \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/src/ephemera/prompts/data/search-xml-test-gpt.jsonl\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "path = project_root + \"/src/ephemera/prompts/data/search-xml-train-gpt.jsonl\"\n",
    "train_df.gpt_messages.to_json( path, orient=\"records\", lines=True )\n",
    "os.chmod( path, 0o666 )\n",
    "\n",
    "path = project_root + \"/src/ephemera/prompts/data/search-xml-test-gpt.jsonl\"\n",
    "test_df.gpt_messages.to_json( path, orient=\"records\", lines=True )\n",
    "os.chmod( path, 0o666 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "40448bc22d1becc1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T18:53:52.783709Z",
     "start_time": "2024-01-16T18:53:52.759257Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[45], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtrain_df\u001B[49m\u001B[38;5;241m.\u001B[39mgpt_messages\u001B[38;5;241m.\u001B[39mhead()\n",
      "\u001B[0;31mNameError\u001B[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "train_df.gpt_messages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "28dd26783fccfbaf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T18:31:59.514393Z",
     "start_time": "2024-01-16T18:31:59.364614Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 88M\n",
      "drwxr--r-- 4 1001 1001 4.0K Sep 21 19:44 .\n",
      "drwxr--r-- 5 1001 1001 4.0K Sep 21 17:01 ..\n",
      "-rwxr--r-- 1 1001 1001 6.1K Jun 20  2023 .DS_Store\n",
      "-rwxr--r-- 1 1001 1001 4.0K Jun 20  2023 ._.DS_Store\n",
      "-rw-rw-r-- 1 1001 1001    0 Oct 13  2023 __init__.py\n",
      "drwxrwxr-x 2 1001 1001 4.0K Oct 13  2023 fine-tuning-results\n",
      "drwxrwxr-x 2 1001 1001 4.0K Oct 13  2023 jsonl\n",
      "-rw-rw-r-- 1 1001 1001 3.2K Feb 28  2024 munger.py\n",
      "-rw-rw-r-- 1 1001 1001 2.6K Feb 28  2024 placeholders-calendaring-dates-and-times.txt\n",
      "-rw-rw-r-- 1 1001 1001  414 Feb 28  2024 placeholders-calendaring-events.txt\n",
      "-rw-rw-r-- 1 1001 1001 5.4K Feb 28  2024 placeholders-calendaring-locations.txt\n",
      "-rw-rw-r-- 1 1001 1001 1.4K Feb 28  2024 placeholders-calendaring-people.txt\n",
      "-rw-rw-r-- 1 1001 1001 3.6K Feb 28  2024 placeholders-cities-and-countries.txt\n",
      "-rw-rw-r-- 1 1001 1001  360 Apr 23 02:06 placeholders-interjections-um-er-uh-etc.txt\n",
      "-rw-rw-r-- 1 1001 1001  342 Apr 23 02:06 placeholders-receptionist-names.txt\n",
      "-rw-rw-r-- 1 1001 1001 1.7K Apr 23 02:06 placeholders-receptionist-salutations.txt\n",
      "-rw-rw-r-- 1 1001 1001  982 Mar  7  2024 placeholders-receptionist-titles.txt\n",
      "-rw-rw-r-- 1 1001 1001  36K Feb 28  2024 placeholders-search-terms.txt\n",
      "-rw-rw-r-- 1 1001 1001  18K Feb 28  2024 synthetic-data-agent-routing-calendaring.txt\n",
      "-rw-rw-r-- 1 1001 1001 5.6K Feb 28  2024 synthetic-data-agent-routing-date-and-time.txt\n",
      "-rwxr--r-- 1 1001 1001  28K Sep 21 02:03 synthetic-data-agent-routing-math.txt\n",
      "-rw-rw-r-- 1 1001 1001  13K Apr 23 02:06 synthetic-data-agent-routing-receptionist.txt\n",
      "-rw-rw-r-- 1 1001 1001  25K Feb 28  2024 synthetic-data-agent-routing-todo-lists.txt\n",
      "-rw-rw-r-- 1 1001 1001  11K Feb 28  2024 synthetic-data-agent-routing-weather.txt\n",
      "-rw-rw-r-- 1 1001 1001  20K Apr 23 02:06 synthetic-data-agent-search-static-vs-dynamic.txt\n",
      "-rw-rw-r-- 1 1001 1001  15K Apr 23 02:06 synthetic-data-agent-search-static-vs-dynamic.xml\n",
      "-rw-rw-r-- 1 1001 1001 7.0K Jan 24  2024 synthetic-data-load-url-current-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001 7.3K Oct 13  2023 synthetic-data-load-url-new-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001  11K Feb 28  2024 synthetic-data-none-of-the-above.txt\n",
      "-rw-rw-r-- 1 1001 1001  12K Jan 24  2024 synthetic-data-search-clipboard-GENERIC-current-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001  12K Jan 24  2024 synthetic-data-search-clipboard-GENERIC-new-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001  11K Jan 24  2024 synthetic-data-search-clipboard-google-in-current-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001  11K Jan 24  2024 synthetic-data-search-clipboard-google-in-new-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001  12K Jan 24  2024 synthetic-data-search-clipboard-google-scholar-in-current-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001  12K Jan 24  2024 synthetic-data-search-clipboard-google-scholar-in-new-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001  14K Jan 24  2024 synthetic-data-search-clipboard-in-current-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001  13K Jan 24  2024 synthetic-data-search-clipboard-in-new-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001  11K Apr 23 02:06 synthetic-data-search-clipboard-kagi-in-current-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001 9.8K Apr 23 02:06 synthetic-data-search-clipboard-kagi-in-new-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001  12K Jan 24  2024 synthetic-data-search-clipboard-perplexity-in-current-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001  12K Jan 24  2024 synthetic-data-search-clipboard-perplexity-in-new-tab.txt\n",
      "-rw-rw-rw- 1 1001 1001  11K Sep 21 20:06 synthetic-data-search-clipboard-phind-in-current-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001  11K Feb 28  2024 synthetic-data-search-clipboard-phind-in-new-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001 8.5K Jan 24  2024 synthetic-data-search-google-in-current-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001  11K Oct 13  2023 synthetic-data-search-google-in-new-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001 8.5K Oct 13  2023 synthetic-data-search-google-scholar-in-current-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001  11K Oct 13  2023 synthetic-data-search-google-scholar-in-new-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001 7.6K Oct 13  2023 synthetic-data-search-in-current-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001 8.9K Jan 24  2024 synthetic-data-search-in-new-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001 7.9K Apr 23 02:06 synthetic-data-search-kagi-in-current-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001 9.5K Apr 23 02:06 synthetic-data-search-kagi-in-new-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001 4.6K Jan 24  2024 synthetic-data-search-perplexity-in-current-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001 5.5K Jan 24  2024 synthetic-data-search-perplexity-in-new-tab.txt\n",
      "-rw-rw-rw- 1 1001 1001 4.2K Sep 21 17:51 synthetic-data-search-phind-in-current-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001 5.2K Feb 28  2024 synthetic-data-search-phind-in-new-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001  723 Oct 13  2023 training-commands.map\n",
      "-rw-rw-rw- 1 1001 1001 7.0M Feb  5  2024 voice-commands-xml-test-gpt.jsonl\n",
      "-rw-rw-rw- 1 1001 1001  56M Feb  5  2024 voice-commands-xml-train-gpt.jsonl\n",
      "-rw-rw-rw- 1 1001 1001  25M Feb  5  2024 voice-commands-xml-validate-gpt.jsonl\n"
     ]
    }
   ],
   "source": [
    "! ls -alh /var/model/genie-in-the-box/src/ephemera/prompts/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80327281-e62f-43f4-a627-04b0a509463e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 88M\n",
      "drwxr--r-- 4 1001 1001 4.0K Sep 21 19:44 .\n",
      "drwxr--r-- 5 1001 1001 4.0K Sep 21 17:01 ..\n",
      "-rwxr--r-- 1 1001 1001 6.1K Jun 20  2023 .DS_Store\n",
      "-rwxr--r-- 1 1001 1001 4.0K Jun 20  2023 ._.DS_Store\n",
      "-rw-rw-r-- 1 1001 1001    0 Oct 13  2023 __init__.py\n",
      "drwxrwxr-x 2 1001 1001 4.0K Oct 13  2023 fine-tuning-results\n",
      "drwxrwxr-x 2 1001 1001 4.0K Oct 13  2023 jsonl\n",
      "-rw-rw-r-- 1 1001 1001 3.2K Feb 28  2024 munger.py\n",
      "-rw-rw-r-- 1 1001 1001 2.6K Feb 28  2024 placeholders-calendaring-dates-and-times.txt\n",
      "-rw-rw-r-- 1 1001 1001  414 Feb 28  2024 placeholders-calendaring-events.txt\n",
      "-rw-rw-r-- 1 1001 1001 5.4K Feb 28  2024 placeholders-calendaring-locations.txt\n",
      "-rw-rw-r-- 1 1001 1001 1.4K Feb 28  2024 placeholders-calendaring-people.txt\n",
      "-rw-rw-r-- 1 1001 1001 3.6K Feb 28  2024 placeholders-cities-and-countries.txt\n",
      "-rw-rw-r-- 1 1001 1001  360 Apr 23 02:06 placeholders-interjections-um-er-uh-etc.txt\n",
      "-rw-rw-r-- 1 1001 1001  342 Apr 23 02:06 placeholders-receptionist-names.txt\n",
      "-rw-rw-r-- 1 1001 1001 1.7K Apr 23 02:06 placeholders-receptionist-salutations.txt\n",
      "-rw-rw-r-- 1 1001 1001  982 Mar  7  2024 placeholders-receptionist-titles.txt\n",
      "-rw-rw-r-- 1 1001 1001  36K Feb 28  2024 placeholders-search-terms.txt\n",
      "-rw-rw-r-- 1 1001 1001  18K Feb 28  2024 synthetic-data-agent-routing-calendaring.txt\n",
      "-rw-rw-r-- 1 1001 1001 5.6K Feb 28  2024 synthetic-data-agent-routing-date-and-time.txt\n",
      "-rwxr--r-- 1 1001 1001  28K Sep 21 02:03 synthetic-data-agent-routing-math.txt\n",
      "-rw-rw-r-- 1 1001 1001  13K Apr 23 02:06 synthetic-data-agent-routing-receptionist.txt\n",
      "-rw-rw-r-- 1 1001 1001  25K Feb 28  2024 synthetic-data-agent-routing-todo-lists.txt\n",
      "-rw-rw-r-- 1 1001 1001  11K Feb 28  2024 synthetic-data-agent-routing-weather.txt\n",
      "-rw-rw-r-- 1 1001 1001  20K Apr 23 02:06 synthetic-data-agent-search-static-vs-dynamic.txt\n",
      "-rw-rw-r-- 1 1001 1001  15K Apr 23 02:06 synthetic-data-agent-search-static-vs-dynamic.xml\n",
      "-rw-rw-r-- 1 1001 1001 7.0K Jan 24  2024 synthetic-data-load-url-current-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001 7.3K Oct 13  2023 synthetic-data-load-url-new-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001  11K Feb 28  2024 synthetic-data-none-of-the-above.txt\n",
      "-rw-rw-r-- 1 1001 1001  12K Jan 24  2024 synthetic-data-search-clipboard-GENERIC-current-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001  12K Jan 24  2024 synthetic-data-search-clipboard-GENERIC-new-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001  11K Jan 24  2024 synthetic-data-search-clipboard-google-in-current-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001  11K Jan 24  2024 synthetic-data-search-clipboard-google-in-new-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001  12K Jan 24  2024 synthetic-data-search-clipboard-google-scholar-in-current-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001  12K Jan 24  2024 synthetic-data-search-clipboard-google-scholar-in-new-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001  14K Jan 24  2024 synthetic-data-search-clipboard-in-current-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001  13K Jan 24  2024 synthetic-data-search-clipboard-in-new-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001  11K Apr 23 02:06 synthetic-data-search-clipboard-kagi-in-current-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001 9.8K Apr 23 02:06 synthetic-data-search-clipboard-kagi-in-new-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001  12K Jan 24  2024 synthetic-data-search-clipboard-perplexity-in-current-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001  12K Jan 24  2024 synthetic-data-search-clipboard-perplexity-in-new-tab.txt\n",
      "-rw-rw-rw- 1 1001 1001  11K Sep 21 17:51 synthetic-data-search-clipboard-phind-in-current-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001  11K Feb 28  2024 synthetic-data-search-clipboard-phind-in-new-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001 8.5K Jan 24  2024 synthetic-data-search-google-in-current-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001  11K Oct 13  2023 synthetic-data-search-google-in-new-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001 8.5K Oct 13  2023 synthetic-data-search-google-scholar-in-current-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001  11K Oct 13  2023 synthetic-data-search-google-scholar-in-new-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001 7.6K Oct 13  2023 synthetic-data-search-in-current-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001 8.9K Jan 24  2024 synthetic-data-search-in-new-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001 7.9K Apr 23 02:06 synthetic-data-search-kagi-in-current-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001 9.5K Apr 23 02:06 synthetic-data-search-kagi-in-new-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001 4.6K Jan 24  2024 synthetic-data-search-perplexity-in-current-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001 5.5K Jan 24  2024 synthetic-data-search-perplexity-in-new-tab.txt\n",
      "-rw-rw-rw- 1 1001 1001 4.2K Sep 21 17:51 synthetic-data-search-phind-in-current-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001 5.2K Feb 28  2024 synthetic-data-search-phind-in-new-tab.txt\n",
      "-rw-rw-r-- 1 1001 1001  723 Oct 13  2023 training-commands.map\n",
      "-rw-rw-rw- 1 1001 1001 7.0M Feb  5  2024 voice-commands-xml-test-gpt.jsonl\n",
      "-rw-rw-rw- 1 1001 1001  56M Feb  5  2024 voice-commands-xml-train-gpt.jsonl\n",
      "-rw-rw-rw- 1 1001 1001  25M Feb  5  2024 voice-commands-xml-validate-gpt.jsonl\n"
     ]
    }
   ],
   "source": [
    "! ls -alh /var/model/genie-in-the-box/src/ephemera/prompts/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "72ddbb19f1dd5c23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T22:38:44.204450Z",
     "start_time": "2024-01-16T22:38:42.648719Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float16"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "getattr(torch, \"float16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "2aa023022ebbaa6a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-17T01:33:12.883146Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def validate_responses( df ):\n",
    "    \n",
    "    rows = df.shape[ 0 ]\n",
    "    global call_counter\n",
    "    call_counter = 0\n",
    "    \n",
    "    # df[ \"response\" ]                    = df[ \"prompt\" ].apply( lambda cell: get_response_to_question( cell, rows ) )\n",
    "    df[ \"response\" ]                    = df[ \"gpt_message\" ].apply( lambda cell: get_gpt_response_to_question( cell, rows ) )\n",
    "    \n",
    "    # Validate the structure and content of the xml response\n",
    "    df[ \"response_xml_is_valid\" ]       = df[ \"response\" ].apply( lambda cell: is_valid_xml( cell, schema ) )\n",
    "    df[ \"contains_response\" ]           = df[ \"response\" ].apply( lambda cell: contains_valid_xml_tag( cell, \"response\" ) )\n",
    "    df[ \"contains_browser_command\" ]    = df[ \"response\" ].apply( lambda cell: contains_valid_xml_tag( cell, \"browser-command\" ) )\n",
    "    df[ \"contains_args\" ]               = df[ \"response\" ].apply( lambda cell: contains_valid_xml_tag( cell, \"args\" ) )\n",
    "    df[ \"response_is_exact\" ]           = df.apply( lambda row: is_response_exact( row[ \"response\" ], row[ \"output\" ] ), axis=1 )\n",
    "    df[ \"response_has_correct_values\" ] = df.apply( lambda row: contains_correct_response_values( row[ \"response\" ], row[ \"output\" ] ), axis=1 )\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ab93c26c44ea720",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-29T16:04:47.153274Z",
     "start_time": "2024-01-29T16:04:47.089336Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/var/model/genie-in-the-box/src'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir( \"/var/model/genie-in-the-box/src\" )\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "87d38127f5eb8029",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-29T16:05:45.839427Z",
     "start_time": "2024-01-29T16:05:44.708769Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from lib.utils.util_stopwatch import Stopwatch\n",
    "from ephemera.prompts.xml_fine_tuning_prompt_generator import XmlFineTuningPromptGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "61fb4e14-8891-4aeb-9bc6-b0e887b404c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reusing ConfigurationManager() singleton...\n",
      "Inserting DUPLICATE placeholders into the list. Requested length [500] > list length [74]\n",
      "Inserting DUPLICATE placeholders into the list. Requested length [500] > list length [148]\n",
      "Inserting DUPLICATE placeholders into the list. Requested length [500] > list length [296]\n",
      "Commands file for command [go to current tab] exists: True\n",
      "Commands file for command [go to new tab] exists: True\n",
      "Commands file for command [search current tab] exists: True\n",
      "Commands file for command [search new tab] exists: True\n",
      "Commands file for command [search google current tab] exists: True\n",
      "Commands file for command [search google new tab] exists: True\n",
      "Commands file for command [search google scholar current tab] exists: True\n",
      "Commands file for command [search google scholar new tab] exists: True\n",
      "Commands file for command [search kagi new tab] exists: True\n",
      "Commands file for command [search kagi current tab] exists: True\n",
      "Commands file for command [search perplexity current tab] exists: True\n",
      "Commands file for command [search perplexity new tab] exists: True\n",
      "Commands file for command [search phind current tab] exists: True\n",
      "Commands file for command [search phind new tab] exists: True\n",
      "\n",
      "Commands file for command [search using clipboard current tab] exists: True\n",
      "Commands file for command [search using clipboard new tab] exists: True\n",
      "Commands file for command [search google using clipboard current tab] exists: True\n",
      "Commands file for command [search google using clipboard new tab] exists: True\n",
      "Commands file for command [search google scholar using clipboard current tab] exists: True\n",
      "Commands file for command [search google scholar using clipboard new tab] exists: True\n",
      "Commands file for command [search kagi using clipboard current tab] exists: True\n",
      "Commands file for command [search kagi using clipboard new tab] exists: True\n",
      "Commands file for command [search perplexity using clipboard current tab] exists: True\n",
      "Commands file for command [search perplexity using clipboard new tab] exists: True\n",
      "Commands file for command [search phind using clipboard current tab] exists: True\n",
      "Commands file for command [search phind using clipboard new tab] exists: True\n",
      "Commands file for command [none] exists: True\n",
      "\n",
      "Commands file for command [agent router go to search function mapping] exists: True\n",
      "\n",
      "Commands file for command [agent router go to date and time] exists: True\n",
      "Commands file for command [agent router go to weather] exists: True\n",
      "Commands file for command [agent router go to calendar] exists: True\n",
      "Commands file for command [agent router go to receptionist] exists: True\n",
      "\n",
      "Commands file for command [agent router go to todo list] exists: True\n",
      "Commands file for command [agent router go to math] exists: True\n",
      "Commands file for command [none] exists: True\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Building prompts for compound VOX command [go to current tab]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Building prompts for compound VOX command [go to new tab]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Building prompts for compound VOX command [search current tab]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Building prompts for compound VOX command [search new tab]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Building prompts for compound VOX command [search google current tab]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Building prompts for compound VOX command [search google new tab]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Building prompts for compound VOX command [search google scholar current tab]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Building prompts for compound VOX command [search google scholar new tab]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Building prompts for compound VOX command [search kagi new tab]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Building prompts for compound VOX command [search kagi current tab]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Building prompts for compound VOX command [search perplexity current tab]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Building prompts for compound VOX command [search perplexity new tab]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Building prompts for compound VOX command [search phind current tab]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Building prompts for compound VOX command [search phind new tab]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Pruning potential duplicates by 'input' values...\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      " PRE 140,000 training inputs...\n",
      "POST 138,700 training inputs. Deleted 1,300 rows = 0.9% duplicate questions\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Sampling 28,000 rows/command from the pruned dataframe using the following weights:\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "command\n",
      "go to current tab                    0.072098\n",
      "go to new tab                        0.071377\n",
      "search current tab                   0.071377\n",
      "search new tab                       0.071377\n",
      "search google current tab            0.071377\n",
      "search google new tab                0.071377\n",
      "search google scholar current tab    0.071377\n",
      "search google scholar new tab        0.071377\n",
      "search kagi new tab                  0.071377\n",
      "search kagi current tab              0.071377\n",
      "search perplexity current tab        0.071377\n",
      "search perplexity new tab            0.071377\n",
      "search phind current tab             0.071377\n",
      "search phind new tab                 0.071377\n",
      "Name: proportion, dtype: float64\n",
      "command\n",
      "go to current tab                    10000\n",
      "go to new tab                         9900\n",
      "search current tab                    9900\n",
      "search new tab                        9900\n",
      "search google current tab             9900\n",
      "search google new tab                 9900\n",
      "search google scholar current tab     9900\n",
      "search google scholar new tab         9900\n",
      "search kagi new tab                   9900\n",
      "search kagi current tab               9900\n",
      "search perplexity current tab         9900\n",
      "search perplexity new tab             9900\n",
      "search phind current tab              9900\n",
      "search phind new tab                  9900\n",
      "Name: count, dtype: int64\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Building prompts for simple VOX command [search using clipboard current tab]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "....................\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Building prompts for simple VOX command [search using clipboard new tab]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "....................\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Building prompts for simple VOX command [search google using clipboard current tab]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "....................\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Building prompts for simple VOX command [search google using clipboard new tab]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "....................\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Building prompts for simple VOX command [search google scholar using clipboard current tab]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "....................\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Building prompts for simple VOX command [search google scholar using clipboard new tab]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "....................\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Building prompts for simple VOX command [search kagi using clipboard current tab]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "....................\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Building prompts for simple VOX command [search kagi using clipboard new tab]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "....................\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Building prompts for simple VOX command [search perplexity using clipboard current tab]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "....................\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Building prompts for simple VOX command [search perplexity using clipboard new tab]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "....................\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Building prompts for simple VOX command [search phind using clipboard current tab]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "....................\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Building prompts for simple VOX command [search phind using clipboard new tab]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "....................\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Building prompts for simple VOX command [none]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "....................\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Pruning potential duplicates by 'input' values...\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      " PRE 2,600 training inputs...\n",
      "POST 2,498 training inputs. Deleted 102 rows = 3.9% duplicate questions\n",
      "WARNING: Sample size [5,200] > rows_post [2,498]. Returning all [2,498] rows.\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Building prompts for compound AGENT ROUTER command [agent router go to date and time]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Building prompts for compound AGENT ROUTER command [agent router go to weather]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Building prompts for compound AGENT ROUTER command [agent router go to calendar]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Building prompts for compound AGENT ROUTER command [agent router go to receptionist]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Inserting DUPLICATE placeholders into the list. Requested length [100] > list length [54]\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Pruning potential duplicates by 'input' values...\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      " PRE 40,000 training inputs...\n",
      "POST 39,827 training inputs. Deleted 173 rows = 0.4% duplicate questions\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Sampling 28,000 rows/command from the pruned dataframe using the following weights:\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "command\n",
      "agent router go to date and time    0.251086\n",
      "agent router go to weather          0.251086\n",
      "agent router go to calendar         0.249429\n",
      "agent router go to receptionist     0.248399\n",
      "Name: proportion, dtype: float64\n",
      "command\n",
      "agent router go to date and time    10000\n",
      "agent router go to weather          10000\n",
      "agent router go to calendar          9934\n",
      "agent router go to receptionist      9893\n",
      "Name: count, dtype: int64\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Building prompts for simple AGENT ROUTER command [agent router go to todo list]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "........................................\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Building prompts for simple AGENT ROUTER command [agent router go to math]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "........................................\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Building prompts for simple AGENT ROUTER command [none]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "....................\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Pruning potential duplicates by 'input' values...\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      " PRE 1,010 training inputs...\n",
      "POST 1,010 training inputs. Deleted 0 rows = 0.0% duplicate questions\n",
      "WARNING: Sample size [5,200] > rows_post [1,010]. Returning all [1,010] rows.\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Command counts for all 39,508 training prompts\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "                                              command  input\n",
      "0                         agent router go to calendar   2000\n",
      "1                    agent router go to date and time   2000\n",
      "2                             agent router go to math    407\n",
      "3                     agent router go to receptionist   2000\n",
      "4                        agent router go to todo list    403\n",
      "5                          agent router go to weather   2000\n",
      "6                                   go to current tab   2000\n",
      "7                                       go to new tab   2000\n",
      "8                                                none    400\n",
      "9                                  search current tab   2000\n",
      "10                          search google current tab   2000\n",
      "11                              search google new tab   2000\n",
      "12                  search google scholar current tab   2000\n",
      "13                      search google scholar new tab   2000\n",
      "14  search google scholar using clipboard current tab    199\n",
      "15      search google scholar using clipboard new tab    104\n",
      "16          search google using clipboard current tab    198\n",
      "17              search google using clipboard new tab    200\n",
      "18                            search kagi current tab   2000\n",
      "19                                search kagi new tab   2000\n",
      "20            search kagi using clipboard current tab    199\n",
      "21                search kagi using clipboard new tab    200\n",
      "22                                     search new tab   2000\n",
      "23                      search perplexity current tab   2000\n",
      "24                          search perplexity new tab   2000\n",
      "25      search perplexity using clipboard current tab    199\n",
      "26          search perplexity using clipboard new tab    200\n",
      "27                           search phind current tab   2000\n",
      "28                               search phind new tab   2000\n",
      "29           search phind using clipboard current tab    200\n",
      "30               search phind using clipboard new tab    200\n",
      "31                 search using clipboard current tab    199\n",
      "32                     search using clipboard new tab    200\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Max, min, and mean prompt CHARACTER counts for all 39,508 training prompts\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Max  prompt length [3,073] characters\n",
      "Min  prompt length [1,727] characters\n",
      "Mean prompt length [2,703.1] characters\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Max, min, and mean prompt WORD counts for all 39,508 training prompts\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Max  prompt length [703] words\n",
      "Min  prompt length [439] words\n",
      "Mean prompt length [631.9] words\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "path_prefix = \"/var/model/genie-in-the-box\"\n",
    "\n",
    "xml_ftp_generator     = XmlFineTuningPromptGenerator( path_prefix=path_prefix, tgi_url=\"http://172.17.0.4:3000\", debug=False )\n",
    "all_qna_df            = xml_ftp_generator.build_all_training_prompts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ed0548558d410b92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-29T16:10:11.178289Z",
     "start_time": "2024-01-29T16:10:11.012298Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reusing ConfigurationManager() singleton...\n",
      "Inserting DUPLICATE placeholders into the list. Requested length [500] > list length [74]\n",
      "Inserting DUPLICATE placeholders into the list. Requested length [500] > list length [148]\n",
      "Inserting DUPLICATE placeholders into the list. Requested length [500] > list length [296]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path_prefix = \"/var/model/genie-in-the-box\"\n",
    "xml_ftp_generator     = XmlFineTuningPromptGenerator( path_prefix=path_prefix, tgi_url=\"http://172.17.0.4:3000\", debug=False, silent=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42a80b0-c2ab-4e76-b96f-a4fb8ffcadb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%autoreload\n",
    "train_df, test_df, validate_df = xml_ftp_generator.get_train_test_validate_split( all_qna_df, sample_size=all_qna_df.shape[ 0 ], test_size=0.2, test_validate_size=0.5 )\n",
    "\n",
    "xml_ftp_generator.write_ttv_split_to_jsonl( train_df, test_df, validate_df )\n",
    "\n",
    "# validation block\n",
    "# validate_df    = pd.read_json( xml_ftp_generator.path_prefix + \"/src/ephemera/prompts/data/voice-commands-xml-validate.jsonl\", lines=True ).sample( 100, random_state=42 )\n",
    "# timer          = Stopwatch( msg=f\"Validating {validate_df.shape[ 0 ]:,} responses...\", silent=False )\n",
    "\n",
    "model_name     = \"mistralai/Mistral-7B-Instruct-v0.2-AWQ\"\n",
    "validate_df    = xml_ftp_generator.generate_responses( validate_df, switch=\"tgi\", model_name=model_name )\n",
    "validate_df    = xml_ftp_generator.validate_responses( validate_df )\n",
    "#\n",
    "# # model_name     = \"gpt-3.5-turbo-1106\"\n",
    "# # validate_df    = xml_ftp_generator.generate_responses( validate_df, switch=\"openai\", model_name= )\n",
    "# # validate_df    = xml_ftp_generator.validate_responses( validate_df )\n",
    "#\n",
    "xml_ftp_generator.print_validation_stats( validate_df, title=f\"Validation Stats for `{model_name}`\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22e8032eee2a36d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-29T16:30:55.817913Z",
     "start_time": "2024-01-29T16:18:45.599229Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Writing train, test, validate splits to jsonl...\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "   train_df.shape: 11,260 x 6\n",
      "    test_df.shape: 1,407 x 6\n",
      "validate_df.shape: 1,408 x 6\n",
      "Generating responses for 1,408 rows...\n",
      "Using TGI w/ model_name [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "Processing call [001] out of [1408] = [0.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 494 ms\n",
      "Tokens per second [78.9]\n",
      "Processing call [002] out of [1408] = [0.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 467 ms\n",
      "Tokens per second [79.2]\n",
      "Processing call [003] out of [1408] = [0.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 478 ms\n",
      "Tokens per second [79.5]\n",
      "Processing call [004] out of [1408] = [0.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 476 ms\n",
      "Tokens per second [79.8]\n",
      "Processing call [005] out of [1408] = [0.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 666 ms\n",
      "Tokens per second [85.6]\n",
      "Processing call [006] out of [1408] = [0.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 496 ms\n",
      "Tokens per second [80.6]\n",
      "Processing call [007] out of [1408] = [0.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 556 ms\n",
      "Tokens per second [82.7]\n",
      "Processing call [008] out of [1408] = [0.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 488 ms\n",
      "Tokens per second [79.9]\n",
      "Processing call [009] out of [1408] = [0.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 545 ms\n",
      "Tokens per second [82.6]\n",
      "Processing call [010] out of [1408] = [0.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 467 ms\n",
      "Tokens per second [79.2]\n",
      "Processing call [011] out of [1408] = [0.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 534 ms\n",
      "Tokens per second [82.4]\n",
      "Processing call [012] out of [1408] = [0.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 563 ms\n",
      "Tokens per second [83.5]\n",
      "Processing call [013] out of [1408] = [0.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 531 ms\n",
      "Tokens per second [82.9]\n",
      "Processing call [014] out of [1408] = [1.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 517 ms\n",
      "Tokens per second [81.2]\n",
      "Processing call [015] out of [1408] = [1.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 460 ms\n",
      "Tokens per second [80.4]\n",
      "Processing call [016] out of [1408] = [1.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 471 ms\n",
      "Tokens per second [78.6]\n",
      "Processing call [017] out of [1408] = [1.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 523 ms\n",
      "Tokens per second [82.2]\n",
      "Processing call [018] out of [1408] = [1.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 469 ms\n",
      "Tokens per second [78.9]\n",
      "Processing call [019] out of [1408] = [1.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 531 ms\n",
      "Tokens per second [82.9]\n",
      "Processing call [020] out of [1408] = [1.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 498 ms\n",
      "Tokens per second [80.3]\n",
      "Processing call [021] out of [1408] = [1.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 533 ms\n",
      "Tokens per second [82.6]\n",
      "Processing call [022] out of [1408] = [1.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 506 ms\n",
      "Tokens per second [81.0]\n",
      "Processing call [023] out of [1408] = [1.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 573 ms\n",
      "Tokens per second [83.8]\n",
      "Processing call [024] out of [1408] = [1.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 621 ms\n",
      "Tokens per second [85.3]\n",
      "Processing call [025] out of [1408] = [1.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 497 ms\n",
      "Tokens per second [80.5]\n",
      "Processing call [026] out of [1408] = [1.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 618 ms\n",
      "Tokens per second [84.1]\n",
      "Processing call [027] out of [1408] = [1.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 533 ms\n",
      "Tokens per second [82.6]\n",
      "Processing call [028] out of [1408] = [2.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 489 ms\n",
      "Tokens per second [79.8]\n",
      "Processing call [029] out of [1408] = [2.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 456 ms\n",
      "Tokens per second [78.9]\n",
      "Processing call [030] out of [1408] = [2.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 447 ms\n",
      "Tokens per second [78.3]\n",
      "Processing call [031] out of [1408] = [2.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 614 ms\n",
      "Tokens per second [84.7]\n",
      "Processing call [032] out of [1408] = [2.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 544 ms\n",
      "Tokens per second [82.7]\n",
      "Processing call [033] out of [1408] = [2.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "................................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 734 ms\n",
      "Tokens per second [87.2]\n",
      "Processing call [034] out of [1408] = [2.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 475 ms\n",
      "Tokens per second [80.0]\n",
      "Processing call [035] out of [1408] = [2.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 525 ms\n",
      "Tokens per second [81.9]\n",
      "Processing call [036] out of [1408] = [2.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 466 ms\n",
      "Tokens per second [79.4]\n",
      "Processing call [037] out of [1408] = [2.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 454 ms\n",
      "Tokens per second [79.3]\n",
      "Processing call [038] out of [1408] = [2.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 472 ms\n",
      "Tokens per second [80.5]\n",
      "Processing call [039] out of [1408] = [2.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 474 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [040] out of [1408] = [2.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 478 ms\n",
      "Tokens per second [79.5]\n",
      "Processing call [041] out of [1408] = [2.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 539 ms\n",
      "Tokens per second [83.5]\n",
      "Processing call [042] out of [1408] = [3.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 519 ms\n",
      "Tokens per second [80.9]\n",
      "Processing call [043] out of [1408] = [3.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 453 ms\n",
      "Tokens per second [79.5]\n",
      "Processing call [044] out of [1408] = [3.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 486 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [045] out of [1408] = [3.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 495 ms\n",
      "Tokens per second [80.8]\n",
      "Processing call [046] out of [1408] = [3.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 471 ms\n",
      "Tokens per second [78.6]\n",
      "Processing call [047] out of [1408] = [3.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 515 ms\n",
      "Tokens per second [81.6]\n",
      "Processing call [048] out of [1408] = [3.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 449 ms\n",
      "Tokens per second [78.0]\n",
      "Processing call [049] out of [1408] = [3.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 507 ms\n",
      "Tokens per second [80.9]\n",
      "Processing call [050] out of [1408] = [3.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 661 ms\n",
      "Tokens per second [86.2]\n",
      "Processing call [051] out of [1408] = [3.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 497 ms\n",
      "Tokens per second [80.5]\n",
      "Processing call [052] out of [1408] = [3.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 526 ms\n",
      "Tokens per second [81.7]\n",
      "Processing call [053] out of [1408] = [3.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 521 ms\n",
      "Tokens per second [82.5]\n",
      "Processing call [054] out of [1408] = [3.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 537 ms\n",
      "Tokens per second [81.9]\n",
      "Processing call [055] out of [1408] = [3.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 610 ms\n",
      "Tokens per second [85.2]\n",
      "Processing call [056] out of [1408] = [4.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 615 ms\n",
      "Tokens per second [84.6]\n",
      "Processing call [057] out of [1408] = [4.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 468 ms\n",
      "Tokens per second [79.1]\n",
      "Processing call [058] out of [1408] = [4.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 528 ms\n",
      "Tokens per second [81.4]\n",
      "Processing call [059] out of [1408] = [4.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 518 ms\n",
      "Tokens per second [81.1]\n",
      "Processing call [060] out of [1408] = [4.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 502 ms\n",
      "Tokens per second [79.7]\n",
      "Processing call [061] out of [1408] = [4.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 476 ms\n",
      "Tokens per second [79.8]\n",
      "Processing call [062] out of [1408] = [4.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 608 ms\n",
      "Tokens per second [83.9]\n",
      "Processing call [063] out of [1408] = [4.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 593 ms\n",
      "Tokens per second [84.3]\n",
      "Processing call [064] out of [1408] = [4.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 518 ms\n",
      "Tokens per second [81.1]\n",
      "Processing call [065] out of [1408] = [4.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 525 ms\n",
      "Tokens per second [81.9]\n",
      "Processing call [066] out of [1408] = [4.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 481 ms\n",
      "Tokens per second [79.0]\n",
      "Processing call [067] out of [1408] = [4.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 519 ms\n",
      "Tokens per second [80.9]\n",
      "Processing call [068] out of [1408] = [4.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 529 ms\n",
      "Tokens per second [81.3]\n",
      "Processing call [069] out of [1408] = [4.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 469 ms\n",
      "Tokens per second [78.9]\n",
      "Processing call [070] out of [1408] = [5.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 537 ms\n",
      "Tokens per second [81.9]\n",
      "Processing call [071] out of [1408] = [5.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 548 ms\n",
      "Tokens per second [82.1]\n",
      "Processing call [072] out of [1408] = [5.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 519 ms\n",
      "Tokens per second [80.9]\n",
      "Processing call [073] out of [1408] = [5.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 515 ms\n",
      "Tokens per second [81.6]\n",
      "Processing call [074] out of [1408] = [5.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 495 ms\n",
      "Tokens per second [78.8]\n",
      "Processing call [075] out of [1408] = [5.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 501 ms\n",
      "Tokens per second [79.8]\n",
      "Processing call [076] out of [1408] = [5.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 466 ms\n",
      "Tokens per second [79.4]\n",
      "Processing call [077] out of [1408] = [5.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..............................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 401 ms\n",
      "Tokens per second [74.8]\n",
      "Processing call [078] out of [1408] = [5.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 644 ms\n",
      "Tokens per second [85.4]\n",
      "Processing call [079] out of [1408] = [5.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 471 ms\n",
      "Tokens per second [78.6]\n",
      "Processing call [080] out of [1408] = [5.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 615 ms\n",
      "Tokens per second [84.6]\n",
      "Processing call [081] out of [1408] = [5.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 489 ms\n",
      "Tokens per second [79.8]\n",
      "Processing call [082] out of [1408] = [5.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 503 ms\n",
      "Tokens per second [81.5]\n",
      "Processing call [083] out of [1408] = [5.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 465 ms\n",
      "Tokens per second [79.6]\n",
      "Processing call [084] out of [1408] = [6.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 468 ms\n",
      "Tokens per second [79.1]\n",
      "Processing call [085] out of [1408] = [6.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 461 ms\n",
      "Tokens per second [80.3]\n",
      "Processing call [086] out of [1408] = [6.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 468 ms\n",
      "Tokens per second [79.1]\n",
      "Processing call [087] out of [1408] = [6.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 524 ms\n",
      "Tokens per second [82.1]\n",
      "Processing call [088] out of [1408] = [6.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 495 ms\n",
      "Tokens per second [80.6]\n",
      "Processing call [089] out of [1408] = [6.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 546 ms\n",
      "Tokens per second [82.4]\n",
      "Processing call [090] out of [1408] = [6.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 601 ms\n",
      "Tokens per second [84.9]\n",
      "Processing call [091] out of [1408] = [6.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 527 ms\n",
      "Tokens per second [81.6]\n",
      "Processing call [092] out of [1408] = [6.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 485 ms\n",
      "Tokens per second [80.4]\n",
      "Processing call [093] out of [1408] = [6.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 612 ms\n",
      "Tokens per second [85.0]\n",
      "Processing call [094] out of [1408] = [6.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 603 ms\n",
      "Tokens per second [84.6]\n",
      "Processing call [095] out of [1408] = [6.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 477 ms\n",
      "Tokens per second [79.7]\n",
      "Processing call [096] out of [1408] = [6.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 628 ms\n",
      "Tokens per second [86.0]\n",
      "Processing call [097] out of [1408] = [6.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 612 ms\n",
      "Tokens per second [85.0]\n",
      "Processing call [098] out of [1408] = [7.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 494 ms\n",
      "Tokens per second [81.0]\n",
      "Processing call [099] out of [1408] = [7.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 475 ms\n",
      "Tokens per second [80.0]\n",
      "Processing call [100] out of [1408] = [7.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 448 ms\n",
      "Tokens per second [78.1]\n",
      "Processing call [101] out of [1408] = [7.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 477 ms\n",
      "Tokens per second [79.7]\n",
      "Processing call [102] out of [1408] = [7.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 502 ms\n",
      "Tokens per second [81.7]\n",
      "Processing call [103] out of [1408] = [7.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 541 ms\n",
      "Tokens per second [81.3]\n",
      "Processing call [104] out of [1408] = [7.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 529 ms\n",
      "Tokens per second [81.3]\n",
      "Processing call [105] out of [1408] = [7.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 498 ms\n",
      "Tokens per second [80.3]\n",
      "Processing call [106] out of [1408] = [7.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 511 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [107] out of [1408] = [7.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 516 ms\n",
      "Tokens per second [81.4]\n",
      "Processing call [108] out of [1408] = [7.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 519 ms\n",
      "Tokens per second [80.9]\n",
      "Processing call [109] out of [1408] = [7.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 493 ms\n",
      "Tokens per second [79.1]\n",
      "Processing call [110] out of [1408] = [7.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..............................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 404 ms\n",
      "Tokens per second [74.3]\n",
      "Processing call [111] out of [1408] = [7.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 467 ms\n",
      "Tokens per second [77.1]\n",
      "Processing call [112] out of [1408] = [8.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 522 ms\n",
      "Tokens per second [80.5]\n",
      "Processing call [113] out of [1408] = [8.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 615 ms\n",
      "Tokens per second [82.9]\n",
      "Processing call [114] out of [1408] = [8.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 516 ms\n",
      "Tokens per second [77.5]\n",
      "Processing call [115] out of [1408] = [8.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 559 ms\n",
      "Tokens per second [80.5]\n",
      "Processing call [116] out of [1408] = [8.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 476 ms\n",
      "Tokens per second [77.7]\n",
      "Processing call [117] out of [1408] = [8.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 617 ms\n",
      "Tokens per second [80.9]\n",
      "Processing call [118] out of [1408] = [8.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 472 ms\n",
      "Tokens per second [76.3]\n",
      "Processing call [119] out of [1408] = [8.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 484 ms\n",
      "Tokens per second [78.5]\n",
      "Processing call [120] out of [1408] = [8.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 462 ms\n",
      "Tokens per second [77.9]\n",
      "Processing call [121] out of [1408] = [8.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 523 ms\n",
      "Tokens per second [80.3]\n",
      "Processing call [122] out of [1408] = [8.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 606 ms\n",
      "Tokens per second [84.2]\n",
      "Processing call [123] out of [1408] = [8.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 468 ms\n",
      "Tokens per second [79.1]\n",
      "Processing call [124] out of [1408] = [8.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 534 ms\n",
      "Tokens per second [80.5]\n",
      "Processing call [125] out of [1408] = [8.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 468 ms\n",
      "Tokens per second [79.1]\n",
      "Processing call [126] out of [1408] = [8.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 476 ms\n",
      "Tokens per second [77.7]\n",
      "Processing call [127] out of [1408] = [9.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 504 ms\n",
      "Tokens per second [77.4]\n",
      "Processing call [128] out of [1408] = [9.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 601 ms\n",
      "Tokens per second [79.9]\n",
      "Processing call [129] out of [1408] = [9.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 543 ms\n",
      "Tokens per second [79.2]\n",
      "Processing call [130] out of [1408] = [9.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 473 ms\n",
      "Tokens per second [78.2]\n",
      "Processing call [131] out of [1408] = [9.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 558 ms\n",
      "Tokens per second [80.6]\n",
      "Processing call [132] out of [1408] = [9.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 462 ms\n",
      "Tokens per second [75.8]\n",
      "Processing call [133] out of [1408] = [9.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 534 ms\n",
      "Tokens per second [78.7]\n",
      "Processing call [134] out of [1408] = [9.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 476 ms\n",
      "Tokens per second [77.7]\n",
      "Processing call [135] out of [1408] = [9.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 484 ms\n",
      "Tokens per second [78.5]\n",
      "Processing call [136] out of [1408] = [9.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 553 ms\n",
      "Tokens per second [81.4]\n",
      "Processing call [137] out of [1408] = [9.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 476 ms\n",
      "Tokens per second [77.7]\n",
      "Processing call [138] out of [1408] = [9.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 472 ms\n",
      "Tokens per second [78.4]\n",
      "Processing call [139] out of [1408] = [9.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 454 ms\n",
      "Tokens per second [77.1]\n",
      "Processing call [140] out of [1408] = [9.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 552 ms\n",
      "Tokens per second [81.5]\n",
      "Processing call [141] out of [1408] = [10.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 522 ms\n",
      "Tokens per second [80.5]\n",
      "Processing call [142] out of [1408] = [10.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 643 ms\n",
      "Tokens per second [84.0]\n",
      "Processing call [143] out of [1408] = [10.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 492 ms\n",
      "Tokens per second [79.3]\n",
      "Processing call [144] out of [1408] = [10.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 542 ms\n",
      "Tokens per second [81.2]\n",
      "Processing call [145] out of [1408] = [10.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 613 ms\n",
      "Tokens per second [83.2]\n",
      "Processing call [146] out of [1408] = [10.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 611 ms\n",
      "Tokens per second [83.5]\n",
      "Processing call [147] out of [1408] = [10.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 543 ms\n",
      "Tokens per second [81.0]\n",
      "Processing call [148] out of [1408] = [10.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 522 ms\n",
      "Tokens per second [80.5]\n",
      "Processing call [149] out of [1408] = [10.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 492 ms\n",
      "Tokens per second [79.3]\n",
      "Processing call [150] out of [1408] = [10.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 538 ms\n",
      "Tokens per second [81.8]\n",
      "Processing call [151] out of [1408] = [10.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 481 ms\n",
      "Tokens per second [79.0]\n",
      "Processing call [152] out of [1408] = [10.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 539 ms\n",
      "Tokens per second [81.6]\n",
      "Processing call [153] out of [1408] = [10.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 481 ms\n",
      "Tokens per second [79.0]\n",
      "Processing call [154] out of [1408] = [10.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 637 ms\n",
      "Tokens per second [84.8]\n",
      "Processing call [155] out of [1408] = [11.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 533 ms\n",
      "Tokens per second [80.7]\n",
      "Processing call [156] out of [1408] = [11.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 580 ms\n",
      "Tokens per second [82.8]\n",
      "Processing call [157] out of [1408] = [11.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 553 ms\n",
      "Tokens per second [81.4]\n",
      "Processing call [158] out of [1408] = [11.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 532 ms\n",
      "Tokens per second [80.8]\n",
      "Processing call [159] out of [1408] = [11.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 597 ms\n",
      "Tokens per second [83.8]\n",
      "Processing call [160] out of [1408] = [11.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 452 ms\n",
      "Tokens per second [77.4]\n",
      "Processing call [161] out of [1408] = [11.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 517 ms\n",
      "Tokens per second [79.3]\n",
      "Processing call [162] out of [1408] = [11.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 628 ms\n",
      "Tokens per second [84.4]\n",
      "Processing call [163] out of [1408] = [11.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 490 ms\n",
      "Tokens per second [79.6]\n",
      "Processing call [164] out of [1408] = [11.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 530 ms\n",
      "Tokens per second [81.1]\n",
      "Processing call [165] out of [1408] = [11.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 539 ms\n",
      "Tokens per second [81.6]\n",
      "Processing call [166] out of [1408] = [11.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 489 ms\n",
      "Tokens per second [79.8]\n",
      "Processing call [167] out of [1408] = [11.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 489 ms\n",
      "Tokens per second [79.8]\n",
      "Processing call [168] out of [1408] = [11.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 468 ms\n",
      "Tokens per second [79.1]\n",
      "Processing call [169] out of [1408] = [12.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 449 ms\n",
      "Tokens per second [78.0]\n",
      "Processing call [170] out of [1408] = [12.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 491 ms\n",
      "Tokens per second [79.4]\n",
      "Processing call [171] out of [1408] = [12.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 529 ms\n",
      "Tokens per second [81.3]\n",
      "Processing call [172] out of [1408] = [12.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 634 ms\n",
      "Tokens per second [85.2]\n",
      "Processing call [173] out of [1408] = [12.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 453 ms\n",
      "Tokens per second [77.3]\n",
      "Processing call [174] out of [1408] = [12.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 479 ms\n",
      "Tokens per second [79.3]\n",
      "Processing call [175] out of [1408] = [12.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 649 ms\n",
      "Tokens per second [84.7]\n",
      "Processing call [176] out of [1408] = [12.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 450 ms\n",
      "Tokens per second [77.8]\n",
      "Processing call [177] out of [1408] = [12.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 594 ms\n",
      "Tokens per second [84.2]\n",
      "Processing call [178] out of [1408] = [12.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 633 ms\n",
      "Tokens per second [85.3]\n",
      "Processing call [179] out of [1408] = [12.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 474 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [180] out of [1408] = [12.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 526 ms\n",
      "Tokens per second [81.7]\n",
      "Processing call [181] out of [1408] = [12.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 557 ms\n",
      "Tokens per second [82.6]\n",
      "Processing call [182] out of [1408] = [12.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 476 ms\n",
      "Tokens per second [79.8]\n",
      "Processing call [183] out of [1408] = [13.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 476 ms\n",
      "Tokens per second [79.8]\n",
      "Processing call [184] out of [1408] = [13.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 616 ms\n",
      "Tokens per second [84.4]\n",
      "Processing call [185] out of [1408] = [13.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 496 ms\n",
      "Tokens per second [80.6]\n",
      "Processing call [186] out of [1408] = [13.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 526 ms\n",
      "Tokens per second [81.7]\n",
      "Processing call [187] out of [1408] = [13.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 488 ms\n",
      "Tokens per second [79.9]\n",
      "Processing call [188] out of [1408] = [13.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 522 ms\n",
      "Tokens per second [82.4]\n",
      "Processing call [189] out of [1408] = [13.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 536 ms\n",
      "Tokens per second [82.1]\n",
      "Processing call [190] out of [1408] = [13.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 466 ms\n",
      "Tokens per second [79.4]\n",
      "Processing call [191] out of [1408] = [13.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 535 ms\n",
      "Tokens per second [82.2]\n",
      "Processing call [192] out of [1408] = [13.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 449 ms\n",
      "Tokens per second [78.0]\n",
      "Processing call [193] out of [1408] = [13.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 489 ms\n",
      "Tokens per second [79.8]\n",
      "Processing call [194] out of [1408] = [13.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 469 ms\n",
      "Tokens per second [78.9]\n",
      "Processing call [195] out of [1408] = [13.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 622 ms\n",
      "Tokens per second [85.2]\n",
      "Processing call [196] out of [1408] = [13.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".............................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 703 ms\n",
      "Tokens per second [86.8]\n",
      "Processing call [197] out of [1408] = [14.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 496 ms\n",
      "Tokens per second [80.6]\n",
      "Processing call [198] out of [1408] = [14.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 477 ms\n",
      "Tokens per second [77.6]\n",
      "Processing call [199] out of [1408] = [14.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 645 ms\n",
      "Tokens per second [85.3]\n",
      "Processing call [200] out of [1408] = [14.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 483 ms\n",
      "Tokens per second [78.7]\n",
      "Processing call [201] out of [1408] = [14.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 477 ms\n",
      "Tokens per second [79.7]\n",
      "Processing call [202] out of [1408] = [14.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 470 ms\n",
      "Tokens per second [78.7]\n",
      "Processing call [203] out of [1408] = [14.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 475 ms\n",
      "Tokens per second [80.0]\n",
      "Processing call [204] out of [1408] = [14.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 512 ms\n",
      "Tokens per second [80.1]\n",
      "Processing call [205] out of [1408] = [14.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 469 ms\n",
      "Tokens per second [78.9]\n",
      "Processing call [206] out of [1408] = [14.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 537 ms\n",
      "Tokens per second [81.9]\n",
      "Processing call [207] out of [1408] = [14.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 488 ms\n",
      "Tokens per second [79.9]\n",
      "Processing call [208] out of [1408] = [14.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 515 ms\n",
      "Tokens per second [81.6]\n",
      "Processing call [209] out of [1408] = [14.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 496 ms\n",
      "Tokens per second [80.6]\n",
      "Processing call [210] out of [1408] = [14.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 475 ms\n",
      "Tokens per second [80.0]\n",
      "Processing call [211] out of [1408] = [15.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 446 ms\n",
      "Tokens per second [78.5]\n",
      "Processing call [212] out of [1408] = [15.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 492 ms\n",
      "Tokens per second [79.3]\n",
      "Processing call [213] out of [1408] = [15.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 514 ms\n",
      "Tokens per second [79.8]\n",
      "Processing call [214] out of [1408] = [15.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 492 ms\n",
      "Tokens per second [79.3]\n",
      "Processing call [215] out of [1408] = [15.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 500 ms\n",
      "Tokens per second [80.0]\n",
      "Processing call [216] out of [1408] = [15.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 623 ms\n",
      "Tokens per second [83.5]\n",
      "Processing call [217] out of [1408] = [15.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 513 ms\n",
      "Tokens per second [79.9]\n",
      "Processing call [218] out of [1408] = [15.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 482 ms\n",
      "Tokens per second [78.8]\n",
      "Processing call [219] out of [1408] = [15.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 541 ms\n",
      "Tokens per second [81.3]\n",
      "Processing call [220] out of [1408] = [15.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 482 ms\n",
      "Tokens per second [78.8]\n",
      "Processing call [221] out of [1408] = [15.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 448 ms\n",
      "Tokens per second [78.1]\n",
      "Processing call [222] out of [1408] = [15.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 493 ms\n",
      "Tokens per second [79.1]\n",
      "Processing call [223] out of [1408] = [15.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 522 ms\n",
      "Tokens per second [80.5]\n",
      "Processing call [224] out of [1408] = [15.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 545 ms\n",
      "Tokens per second [80.7]\n",
      "Processing call [225] out of [1408] = [16.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 502 ms\n",
      "Tokens per second [79.7]\n",
      "Processing call [226] out of [1408] = [16.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 545 ms\n",
      "Tokens per second [80.7]\n",
      "Processing call [227] out of [1408] = [16.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 480 ms\n",
      "Tokens per second [79.2]\n",
      "Processing call [228] out of [1408] = [16.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 475 ms\n",
      "Tokens per second [77.9]\n",
      "Processing call [229] out of [1408] = [16.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 500 ms\n",
      "Tokens per second [80.0]\n",
      "Processing call [230] out of [1408] = [16.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 504 ms\n",
      "Tokens per second [79.4]\n",
      "Processing call [231] out of [1408] = [16.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 551 ms\n",
      "Tokens per second [81.7]\n",
      "Processing call [232] out of [1408] = [16.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 465 ms\n",
      "Tokens per second [77.4]\n",
      "Processing call [233] out of [1408] = [16.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 503 ms\n",
      "Tokens per second [79.5]\n",
      "Processing call [234] out of [1408] = [16.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 646 ms\n",
      "Tokens per second [83.6]\n",
      "Processing call [235] out of [1408] = [16.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 492 ms\n",
      "Tokens per second [79.3]\n",
      "Processing call [236] out of [1408] = [16.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 531 ms\n",
      "Tokens per second [81.0]\n",
      "Processing call [237] out of [1408] = [16.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 524 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [238] out of [1408] = [16.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 504 ms\n",
      "Tokens per second [79.4]\n",
      "Processing call [239] out of [1408] = [17.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 490 ms\n",
      "Tokens per second [79.6]\n",
      "Processing call [240] out of [1408] = [17.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 555 ms\n",
      "Tokens per second [81.1]\n",
      "Processing call [241] out of [1408] = [17.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 471 ms\n",
      "Tokens per second [78.6]\n",
      "Processing call [242] out of [1408] = [17.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 483 ms\n",
      "Tokens per second [78.7]\n",
      "Processing call [243] out of [1408] = [17.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 511 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [244] out of [1408] = [17.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 545 ms\n",
      "Tokens per second [80.7]\n",
      "Processing call [245] out of [1408] = [17.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 590 ms\n",
      "Tokens per second [83.1]\n",
      "Processing call [246] out of [1408] = [17.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 501 ms\n",
      "Tokens per second [79.8]\n",
      "Processing call [247] out of [1408] = [17.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 600 ms\n",
      "Tokens per second [83.3]\n",
      "Processing call [248] out of [1408] = [17.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 503 ms\n",
      "Tokens per second [79.5]\n",
      "Processing call [249] out of [1408] = [17.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 520 ms\n",
      "Tokens per second [80.8]\n",
      "Processing call [250] out of [1408] = [17.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 486 ms\n",
      "Tokens per second [78.2]\n",
      "Processing call [251] out of [1408] = [17.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 482 ms\n",
      "Tokens per second [78.8]\n",
      "Processing call [252] out of [1408] = [17.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 530 ms\n",
      "Tokens per second [81.1]\n",
      "Processing call [253] out of [1408] = [18.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 532 ms\n",
      "Tokens per second [80.8]\n",
      "Processing call [254] out of [1408] = [18.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 512 ms\n",
      "Tokens per second [80.1]\n",
      "Processing call [255] out of [1408] = [18.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 460 ms\n",
      "Tokens per second [78.3]\n",
      "Processing call [256] out of [1408] = [18.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 525 ms\n",
      "Tokens per second [80.0]\n",
      "Processing call [257] out of [1408] = [18.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 470 ms\n",
      "Tokens per second [78.7]\n",
      "Processing call [258] out of [1408] = [18.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 535 ms\n",
      "Tokens per second [80.4]\n",
      "Processing call [259] out of [1408] = [18.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 642 ms\n",
      "Tokens per second [84.1]\n",
      "Processing call [260] out of [1408] = [18.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 514 ms\n",
      "Tokens per second [79.8]\n",
      "Processing call [261] out of [1408] = [18.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 501 ms\n",
      "Tokens per second [79.8]\n",
      "Processing call [262] out of [1408] = [18.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 631 ms\n",
      "Tokens per second [84.0]\n",
      "Processing call [263] out of [1408] = [18.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 531 ms\n",
      "Tokens per second [81.0]\n",
      "Processing call [264] out of [1408] = [18.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 484 ms\n",
      "Tokens per second [78.5]\n",
      "Processing call [265] out of [1408] = [18.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 471 ms\n",
      "Tokens per second [78.6]\n",
      "Processing call [266] out of [1408] = [18.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 474 ms\n",
      "Tokens per second [78.1]\n",
      "Processing call [267] out of [1408] = [19.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 544 ms\n",
      "Tokens per second [80.9]\n",
      "Processing call [268] out of [1408] = [19.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 496 ms\n",
      "Tokens per second [78.6]\n",
      "Processing call [269] out of [1408] = [19.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 471 ms\n",
      "Tokens per second [78.6]\n",
      "Processing call [270] out of [1408] = [19.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 625 ms\n",
      "Tokens per second [83.2]\n",
      "Processing call [271] out of [1408] = [19.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 514 ms\n",
      "Tokens per second [79.8]\n",
      "Processing call [272] out of [1408] = [19.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 464 ms\n",
      "Tokens per second [77.6]\n",
      "Processing call [273] out of [1408] = [19.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 525 ms\n",
      "Tokens per second [80.0]\n",
      "Processing call [274] out of [1408] = [19.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 473 ms\n",
      "Tokens per second [78.2]\n",
      "Processing call [275] out of [1408] = [19.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..............................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 402 ms\n",
      "Tokens per second [74.6]\n",
      "Processing call [276] out of [1408] = [19.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 501 ms\n",
      "Tokens per second [79.8]\n",
      "Processing call [277] out of [1408] = [19.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 472 ms\n",
      "Tokens per second [78.4]\n",
      "Processing call [278] out of [1408] = [19.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 532 ms\n",
      "Tokens per second [80.8]\n",
      "Processing call [279] out of [1408] = [19.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 473 ms\n",
      "Tokens per second [78.2]\n",
      "Processing call [280] out of [1408] = [19.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 461 ms\n",
      "Tokens per second [78.1]\n",
      "Processing call [281] out of [1408] = [20.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 609 ms\n",
      "Tokens per second [83.7]\n",
      "Processing call [282] out of [1408] = [20.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 543 ms\n",
      "Tokens per second [81.0]\n",
      "Processing call [283] out of [1408] = [20.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 470 ms\n",
      "Tokens per second [78.7]\n",
      "Processing call [284] out of [1408] = [20.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 495 ms\n",
      "Tokens per second [80.8]\n",
      "Processing call [285] out of [1408] = [20.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 456 ms\n",
      "Tokens per second [78.9]\n",
      "Processing call [286] out of [1408] = [20.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 534 ms\n",
      "Tokens per second [82.4]\n",
      "Processing call [287] out of [1408] = [20.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 513 ms\n",
      "Tokens per second [81.9]\n",
      "Processing call [288] out of [1408] = [20.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 605 ms\n",
      "Tokens per second [84.3]\n",
      "Processing call [289] out of [1408] = [20.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 497 ms\n",
      "Tokens per second [80.5]\n",
      "Processing call [290] out of [1408] = [20.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 459 ms\n",
      "Tokens per second [78.4]\n",
      "Processing call [291] out of [1408] = [20.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 483 ms\n",
      "Tokens per second [78.7]\n",
      "Processing call [292] out of [1408] = [20.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 610 ms\n",
      "Tokens per second [83.6]\n",
      "Processing call [293] out of [1408] = [20.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 480 ms\n",
      "Tokens per second [79.2]\n",
      "Processing call [294] out of [1408] = [20.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 533 ms\n",
      "Tokens per second [80.7]\n",
      "Processing call [295] out of [1408] = [21.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 493 ms\n",
      "Tokens per second [79.1]\n",
      "Processing call [296] out of [1408] = [21.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 539 ms\n",
      "Tokens per second [81.6]\n",
      "Processing call [297] out of [1408] = [21.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 471 ms\n",
      "Tokens per second [78.6]\n",
      "Processing call [298] out of [1408] = [21.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 512 ms\n",
      "Tokens per second [80.1]\n",
      "Processing call [299] out of [1408] = [21.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 450 ms\n",
      "Tokens per second [77.8]\n",
      "Processing call [300] out of [1408] = [21.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 482 ms\n",
      "Tokens per second [78.8]\n",
      "Processing call [301] out of [1408] = [21.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 503 ms\n",
      "Tokens per second [79.5]\n",
      "Processing call [302] out of [1408] = [21.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 504 ms\n",
      "Tokens per second [79.4]\n",
      "Processing call [303] out of [1408] = [21.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 474 ms\n",
      "Tokens per second [78.1]\n",
      "Processing call [304] out of [1408] = [21.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 645 ms\n",
      "Tokens per second [83.7]\n",
      "Processing call [305] out of [1408] = [21.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 480 ms\n",
      "Tokens per second [79.2]\n",
      "Processing call [306] out of [1408] = [21.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 644 ms\n",
      "Tokens per second [83.9]\n",
      "Processing call [307] out of [1408] = [21.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..............................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 405 ms\n",
      "Tokens per second [74.1]\n",
      "Processing call [308] out of [1408] = [21.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 472 ms\n",
      "Tokens per second [78.4]\n",
      "Processing call [309] out of [1408] = [21.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 493 ms\n",
      "Tokens per second [79.1]\n",
      "Processing call [310] out of [1408] = [22.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 488 ms\n",
      "Tokens per second [79.9]\n",
      "Processing call [311] out of [1408] = [22.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 599 ms\n",
      "Tokens per second [83.5]\n",
      "Processing call [312] out of [1408] = [22.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 449 ms\n",
      "Tokens per second [78.0]\n",
      "Processing call [313] out of [1408] = [22.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 511 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [314] out of [1408] = [22.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 637 ms\n",
      "Tokens per second [84.8]\n",
      "Processing call [315] out of [1408] = [22.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 588 ms\n",
      "Tokens per second [83.3]\n",
      "Processing call [316] out of [1408] = [22.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 480 ms\n",
      "Tokens per second [79.2]\n",
      "Processing call [317] out of [1408] = [22.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 459 ms\n",
      "Tokens per second [78.4]\n",
      "Processing call [318] out of [1408] = [22.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 473 ms\n",
      "Tokens per second [78.2]\n",
      "Processing call [319] out of [1408] = [22.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 470 ms\n",
      "Tokens per second [78.7]\n",
      "Processing call [320] out of [1408] = [22.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 538 ms\n",
      "Tokens per second [81.8]\n",
      "Processing call [321] out of [1408] = [22.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 529 ms\n",
      "Tokens per second [81.3]\n",
      "Processing call [322] out of [1408] = [22.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 629 ms\n",
      "Tokens per second [84.3]\n",
      "Processing call [323] out of [1408] = [22.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 479 ms\n",
      "Tokens per second [79.3]\n",
      "Processing call [324] out of [1408] = [23.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 550 ms\n",
      "Tokens per second [81.8]\n",
      "Processing call [325] out of [1408] = [23.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 616 ms\n",
      "Tokens per second [84.4]\n",
      "Processing call [326] out of [1408] = [23.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 451 ms\n",
      "Tokens per second [77.6]\n",
      "Processing call [327] out of [1408] = [23.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 483 ms\n",
      "Tokens per second [78.7]\n",
      "Processing call [328] out of [1408] = [23.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 528 ms\n",
      "Tokens per second [81.4]\n",
      "Processing call [329] out of [1408] = [23.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 479 ms\n",
      "Tokens per second [79.3]\n",
      "Processing call [330] out of [1408] = [23.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 500 ms\n",
      "Tokens per second [80.0]\n",
      "Processing call [331] out of [1408] = [23.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 502 ms\n",
      "Tokens per second [79.7]\n",
      "Processing call [332] out of [1408] = [23.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 511 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [333] out of [1408] = [23.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 489 ms\n",
      "Tokens per second [79.8]\n",
      "Processing call [334] out of [1408] = [23.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 490 ms\n",
      "Tokens per second [79.6]\n",
      "Processing call [335] out of [1408] = [23.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 491 ms\n",
      "Tokens per second [79.4]\n",
      "Processing call [336] out of [1408] = [23.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 511 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [337] out of [1408] = [23.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 493 ms\n",
      "Tokens per second [79.1]\n",
      "Processing call [338] out of [1408] = [24.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 652 ms\n",
      "Tokens per second [84.4]\n",
      "Processing call [339] out of [1408] = [24.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 504 ms\n",
      "Tokens per second [79.4]\n",
      "Processing call [340] out of [1408] = [24.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 470 ms\n",
      "Tokens per second [78.7]\n",
      "Processing call [341] out of [1408] = [24.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 528 ms\n",
      "Tokens per second [81.4]\n",
      "Processing call [342] out of [1408] = [24.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 602 ms\n",
      "Tokens per second [83.1]\n",
      "Processing call [343] out of [1408] = [24.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 522 ms\n",
      "Tokens per second [80.5]\n",
      "Processing call [344] out of [1408] = [24.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 465 ms\n",
      "Tokens per second [77.4]\n",
      "Processing call [345] out of [1408] = [24.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 619 ms\n",
      "Tokens per second [84.0]\n",
      "Processing call [346] out of [1408] = [24.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 551 ms\n",
      "Tokens per second [81.7]\n",
      "Processing call [347] out of [1408] = [24.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 528 ms\n",
      "Tokens per second [81.4]\n",
      "Processing call [348] out of [1408] = [24.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 534 ms\n",
      "Tokens per second [80.5]\n",
      "Processing call [349] out of [1408] = [24.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 533 ms\n",
      "Tokens per second [80.7]\n",
      "Processing call [350] out of [1408] = [24.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 603 ms\n",
      "Tokens per second [82.9]\n",
      "Processing call [351] out of [1408] = [24.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 460 ms\n",
      "Tokens per second [78.3]\n",
      "Processing call [352] out of [1408] = [25.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 473 ms\n",
      "Tokens per second [78.2]\n",
      "Processing call [353] out of [1408] = [25.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 486 ms\n",
      "Tokens per second [78.2]\n",
      "Processing call [354] out of [1408] = [25.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 521 ms\n",
      "Tokens per second [80.6]\n",
      "Processing call [355] out of [1408] = [25.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 631 ms\n",
      "Tokens per second [84.0]\n",
      "Processing call [356] out of [1408] = [25.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 592 ms\n",
      "Tokens per second [82.8]\n",
      "Processing call [357] out of [1408] = [25.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 504 ms\n",
      "Tokens per second [79.4]\n",
      "Processing call [358] out of [1408] = [25.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 479 ms\n",
      "Tokens per second [79.3]\n",
      "Processing call [359] out of [1408] = [25.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 493 ms\n",
      "Tokens per second [79.1]\n",
      "Processing call [360] out of [1408] = [25.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 542 ms\n",
      "Tokens per second [81.2]\n",
      "Processing call [361] out of [1408] = [25.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 482 ms\n",
      "Tokens per second [78.8]\n",
      "Processing call [362] out of [1408] = [25.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 453 ms\n",
      "Tokens per second [77.3]\n",
      "Processing call [363] out of [1408] = [25.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 504 ms\n",
      "Tokens per second [79.4]\n",
      "Processing call [364] out of [1408] = [25.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 502 ms\n",
      "Tokens per second [79.7]\n",
      "Processing call [365] out of [1408] = [25.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 550 ms\n",
      "Tokens per second [81.8]\n",
      "Processing call [366] out of [1408] = [26.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 533 ms\n",
      "Tokens per second [80.7]\n",
      "Processing call [367] out of [1408] = [26.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 502 ms\n",
      "Tokens per second [79.7]\n",
      "Processing call [368] out of [1408] = [26.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 530 ms\n",
      "Tokens per second [81.1]\n",
      "Processing call [369] out of [1408] = [26.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 624 ms\n",
      "Tokens per second [84.9]\n",
      "Processing call [370] out of [1408] = [26.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 521 ms\n",
      "Tokens per second [80.6]\n",
      "Processing call [371] out of [1408] = [26.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 476 ms\n",
      "Tokens per second [79.8]\n",
      "Processing call [372] out of [1408] = [26.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 491 ms\n",
      "Tokens per second [79.4]\n",
      "Processing call [373] out of [1408] = [26.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 511 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [374] out of [1408] = [26.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 479 ms\n",
      "Tokens per second [79.3]\n",
      "Processing call [375] out of [1408] = [26.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 521 ms\n",
      "Tokens per second [80.6]\n",
      "Processing call [376] out of [1408] = [26.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 657 ms\n",
      "Tokens per second [83.7]\n",
      "Processing call [377] out of [1408] = [26.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 584 ms\n",
      "Tokens per second [83.9]\n",
      "Processing call [378] out of [1408] = [26.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 478 ms\n",
      "Tokens per second [79.5]\n",
      "Processing call [379] out of [1408] = [26.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 447 ms\n",
      "Tokens per second [78.3]\n",
      "Processing call [380] out of [1408] = [27.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 517 ms\n",
      "Tokens per second [81.2]\n",
      "Processing call [381] out of [1408] = [27.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 467 ms\n",
      "Tokens per second [79.2]\n",
      "Processing call [382] out of [1408] = [27.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 538 ms\n",
      "Tokens per second [81.8]\n",
      "Processing call [383] out of [1408] = [27.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 506 ms\n",
      "Tokens per second [81.0]\n",
      "Processing call [384] out of [1408] = [27.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 596 ms\n",
      "Tokens per second [83.9]\n",
      "Processing call [385] out of [1408] = [27.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 519 ms\n",
      "Tokens per second [80.9]\n",
      "Processing call [386] out of [1408] = [27.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 472 ms\n",
      "Tokens per second [78.4]\n",
      "Processing call [387] out of [1408] = [27.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 538 ms\n",
      "Tokens per second [81.8]\n",
      "Processing call [388] out of [1408] = [27.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 638 ms\n",
      "Tokens per second [84.6]\n",
      "Processing call [389] out of [1408] = [27.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 500 ms\n",
      "Tokens per second [80.0]\n",
      "Processing call [390] out of [1408] = [27.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 470 ms\n",
      "Tokens per second [78.7]\n",
      "Processing call [391] out of [1408] = [27.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 511 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [392] out of [1408] = [27.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 549 ms\n",
      "Tokens per second [82.0]\n",
      "Processing call [393] out of [1408] = [27.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..............................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 400 ms\n",
      "Tokens per second [75.0]\n",
      "Processing call [394] out of [1408] = [28.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 489 ms\n",
      "Tokens per second [79.8]\n",
      "Processing call [395] out of [1408] = [28.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 501 ms\n",
      "Tokens per second [79.8]\n",
      "Processing call [396] out of [1408] = [28.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 480 ms\n",
      "Tokens per second [79.2]\n",
      "Processing call [397] out of [1408] = [28.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 472 ms\n",
      "Tokens per second [78.4]\n",
      "Processing call [398] out of [1408] = [28.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 628 ms\n",
      "Tokens per second [84.4]\n",
      "Processing call [399] out of [1408] = [28.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 530 ms\n",
      "Tokens per second [81.1]\n",
      "Processing call [400] out of [1408] = [28.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 530 ms\n",
      "Tokens per second [81.1]\n",
      "Processing call [401] out of [1408] = [28.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 491 ms\n",
      "Tokens per second [79.4]\n",
      "Processing call [402] out of [1408] = [28.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 471 ms\n",
      "Tokens per second [78.6]\n",
      "Processing call [403] out of [1408] = [28.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 476 ms\n",
      "Tokens per second [79.8]\n",
      "Processing call [404] out of [1408] = [28.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 480 ms\n",
      "Tokens per second [79.2]\n",
      "Processing call [405] out of [1408] = [28.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 537 ms\n",
      "Tokens per second [81.9]\n",
      "Processing call [406] out of [1408] = [28.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 511 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [407] out of [1408] = [28.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 479 ms\n",
      "Tokens per second [79.3]\n",
      "Processing call [408] out of [1408] = [29.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 512 ms\n",
      "Tokens per second [80.1]\n",
      "Processing call [409] out of [1408] = [29.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 470 ms\n",
      "Tokens per second [78.7]\n",
      "Processing call [410] out of [1408] = [29.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 458 ms\n",
      "Tokens per second [78.4]\n",
      "Processing call [411] out of [1408] = [29.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 468 ms\n",
      "Tokens per second [79.1]\n",
      "Processing call [412] out of [1408] = [29.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 606 ms\n",
      "Tokens per second [84.2]\n",
      "Processing call [413] out of [1408] = [29.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 459 ms\n",
      "Tokens per second [78.4]\n",
      "Processing call [414] out of [1408] = [29.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 537 ms\n",
      "Tokens per second [81.9]\n",
      "Processing call [415] out of [1408] = [29.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 477 ms\n",
      "Tokens per second [79.7]\n",
      "Processing call [416] out of [1408] = [29.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 490 ms\n",
      "Tokens per second [79.6]\n",
      "Processing call [417] out of [1408] = [29.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 491 ms\n",
      "Tokens per second [79.4]\n",
      "Processing call [418] out of [1408] = [29.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 520 ms\n",
      "Tokens per second [80.8]\n",
      "Processing call [419] out of [1408] = [29.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 460 ms\n",
      "Tokens per second [78.3]\n",
      "Processing call [420] out of [1408] = [29.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 522 ms\n",
      "Tokens per second [80.5]\n",
      "Processing call [421] out of [1408] = [29.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 619 ms\n",
      "Tokens per second [84.0]\n",
      "Processing call [422] out of [1408] = [30.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 452 ms\n",
      "Tokens per second [77.4]\n",
      "Processing call [423] out of [1408] = [30.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 451 ms\n",
      "Tokens per second [77.6]\n",
      "Processing call [424] out of [1408] = [30.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 462 ms\n",
      "Tokens per second [77.9]\n",
      "Processing call [425] out of [1408] = [30.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 510 ms\n",
      "Tokens per second [80.4]\n",
      "Processing call [426] out of [1408] = [30.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 480 ms\n",
      "Tokens per second [79.2]\n",
      "Processing call [427] out of [1408] = [30.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 467 ms\n",
      "Tokens per second [79.2]\n",
      "Processing call [428] out of [1408] = [30.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 478 ms\n",
      "Tokens per second [79.5]\n",
      "Processing call [429] out of [1408] = [30.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 452 ms\n",
      "Tokens per second [77.4]\n",
      "Processing call [430] out of [1408] = [30.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 480 ms\n",
      "Tokens per second [79.2]\n",
      "Processing call [431] out of [1408] = [30.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 511 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [432] out of [1408] = [30.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 478 ms\n",
      "Tokens per second [79.5]\n",
      "Processing call [433] out of [1408] = [30.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 458 ms\n",
      "Tokens per second [78.6]\n",
      "Processing call [434] out of [1408] = [30.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 451 ms\n",
      "Tokens per second [77.6]\n",
      "Processing call [435] out of [1408] = [30.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 527 ms\n",
      "Tokens per second [81.6]\n",
      "Processing call [436] out of [1408] = [31.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 523 ms\n",
      "Tokens per second [80.3]\n",
      "Processing call [437] out of [1408] = [31.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 531 ms\n",
      "Tokens per second [81.0]\n",
      "Processing call [438] out of [1408] = [31.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 482 ms\n",
      "Tokens per second [78.8]\n",
      "Processing call [439] out of [1408] = [31.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 532 ms\n",
      "Tokens per second [80.8]\n",
      "Processing call [440] out of [1408] = [31.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 503 ms\n",
      "Tokens per second [79.5]\n",
      "Processing call [441] out of [1408] = [31.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 610 ms\n",
      "Tokens per second [83.6]\n",
      "Processing call [442] out of [1408] = [31.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 526 ms\n",
      "Tokens per second [79.8]\n",
      "Processing call [443] out of [1408] = [31.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 517 ms\n",
      "Tokens per second [79.3]\n",
      "Processing call [444] out of [1408] = [31.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 639 ms\n",
      "Tokens per second [84.5]\n",
      "Processing call [445] out of [1408] = [31.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 516 ms\n",
      "Tokens per second [79.5]\n",
      "Processing call [446] out of [1408] = [31.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 525 ms\n",
      "Tokens per second [80.0]\n",
      "Processing call [447] out of [1408] = [31.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 482 ms\n",
      "Tokens per second [78.8]\n",
      "Processing call [448] out of [1408] = [31.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 522 ms\n",
      "Tokens per second [80.5]\n",
      "Processing call [449] out of [1408] = [31.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 603 ms\n",
      "Tokens per second [82.9]\n",
      "Processing call [450] out of [1408] = [32.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 480 ms\n",
      "Tokens per second [79.2]\n",
      "Processing call [451] out of [1408] = [32.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 514 ms\n",
      "Tokens per second [79.8]\n",
      "Processing call [452] out of [1408] = [32.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 514 ms\n",
      "Tokens per second [79.8]\n",
      "Processing call [453] out of [1408] = [32.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 511 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [454] out of [1408] = [32.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 549 ms\n",
      "Tokens per second [82.0]\n",
      "Processing call [455] out of [1408] = [32.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 446 ms\n",
      "Tokens per second [78.5]\n",
      "Processing call [456] out of [1408] = [32.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 519 ms\n",
      "Tokens per second [80.8]\n",
      "Processing call [457] out of [1408] = [32.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 470 ms\n",
      "Tokens per second [78.7]\n",
      "Processing call [458] out of [1408] = [32.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 521 ms\n",
      "Tokens per second [80.6]\n",
      "Processing call [459] out of [1408] = [32.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 615 ms\n",
      "Tokens per second [84.6]\n",
      "Processing call [460] out of [1408] = [32.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 494 ms\n",
      "Tokens per second [78.9]\n",
      "Processing call [461] out of [1408] = [32.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 465 ms\n",
      "Tokens per second [79.6]\n",
      "Processing call [462] out of [1408] = [32.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 609 ms\n",
      "Tokens per second [83.7]\n",
      "Processing call [463] out of [1408] = [32.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 508 ms\n",
      "Tokens per second [80.7]\n",
      "Processing call [464] out of [1408] = [33.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 551 ms\n",
      "Tokens per second [81.7]\n",
      "Processing call [465] out of [1408] = [33.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 448 ms\n",
      "Tokens per second [78.1]\n",
      "Processing call [466] out of [1408] = [33.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 530 ms\n",
      "Tokens per second [81.1]\n",
      "Processing call [467] out of [1408] = [33.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 529 ms\n",
      "Tokens per second [81.3]\n",
      "Processing call [468] out of [1408] = [33.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 474 ms\n",
      "Tokens per second [78.1]\n",
      "Processing call [469] out of [1408] = [33.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..............................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 721 ms\n",
      "Tokens per second [86.0]\n",
      "Processing call [470] out of [1408] = [33.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 662 ms\n",
      "Tokens per second [84.6]\n",
      "Processing call [471] out of [1408] = [33.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 521 ms\n",
      "Tokens per second [80.6]\n",
      "Processing call [472] out of [1408] = [33.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 480 ms\n",
      "Tokens per second [79.2]\n",
      "Processing call [473] out of [1408] = [33.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 472 ms\n",
      "Tokens per second [78.4]\n",
      "Processing call [474] out of [1408] = [33.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 642 ms\n",
      "Tokens per second [84.1]\n",
      "Processing call [475] out of [1408] = [33.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 611 ms\n",
      "Tokens per second [83.5]\n",
      "Processing call [476] out of [1408] = [33.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 490 ms\n",
      "Tokens per second [79.6]\n",
      "Processing call [477] out of [1408] = [33.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 671 ms\n",
      "Tokens per second [84.9]\n",
      "Processing call [478] out of [1408] = [33.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 620 ms\n",
      "Tokens per second [83.9]\n",
      "Processing call [479] out of [1408] = [34.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 453 ms\n",
      "Tokens per second [77.3]\n",
      "Processing call [480] out of [1408] = [34.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 511 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [481] out of [1408] = [34.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 502 ms\n",
      "Tokens per second [79.7]\n",
      "Processing call [482] out of [1408] = [34.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 481 ms\n",
      "Tokens per second [79.0]\n",
      "Processing call [483] out of [1408] = [34.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 480 ms\n",
      "Tokens per second [79.2]\n",
      "Processing call [484] out of [1408] = [34.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 453 ms\n",
      "Tokens per second [77.3]\n",
      "Processing call [485] out of [1408] = [34.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 509 ms\n",
      "Tokens per second [80.6]\n",
      "Processing call [486] out of [1408] = [34.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 621 ms\n",
      "Tokens per second [83.7]\n",
      "Processing call [487] out of [1408] = [34.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 589 ms\n",
      "Tokens per second [83.2]\n",
      "Processing call [488] out of [1408] = [34.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 479 ms\n",
      "Tokens per second [77.2]\n",
      "Processing call [489] out of [1408] = [34.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 528 ms\n",
      "Tokens per second [81.4]\n",
      "Processing call [490] out of [1408] = [34.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 516 ms\n",
      "Tokens per second [81.4]\n",
      "Processing call [491] out of [1408] = [34.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 470 ms\n",
      "Tokens per second [78.7]\n",
      "Processing call [492] out of [1408] = [34.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 497 ms\n",
      "Tokens per second [80.5]\n",
      "Processing call [493] out of [1408] = [35.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 525 ms\n",
      "Tokens per second [80.0]\n",
      "Processing call [494] out of [1408] = [35.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 512 ms\n",
      "Tokens per second [80.1]\n",
      "Processing call [495] out of [1408] = [35.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 458 ms\n",
      "Tokens per second [78.6]\n",
      "Processing call [496] out of [1408] = [35.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 520 ms\n",
      "Tokens per second [80.8]\n",
      "Processing call [497] out of [1408] = [35.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 588 ms\n",
      "Tokens per second [83.3]\n",
      "Processing call [498] out of [1408] = [35.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 549 ms\n",
      "Tokens per second [82.0]\n",
      "Processing call [499] out of [1408] = [35.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 459 ms\n",
      "Tokens per second [78.4]\n",
      "Processing call [500] out of [1408] = [35.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 509 ms\n",
      "Tokens per second [80.6]\n",
      "Processing call [501] out of [1408] = [35.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 448 ms\n",
      "Tokens per second [78.1]\n",
      "Processing call [502] out of [1408] = [35.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 460 ms\n",
      "Tokens per second [78.3]\n",
      "Processing call [503] out of [1408] = [35.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 479 ms\n",
      "Tokens per second [79.3]\n",
      "Processing call [504] out of [1408] = [35.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 540 ms\n",
      "Tokens per second [81.5]\n",
      "Processing call [505] out of [1408] = [35.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 458 ms\n",
      "Tokens per second [78.6]\n",
      "Processing call [506] out of [1408] = [35.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 520 ms\n",
      "Tokens per second [80.8]\n",
      "Processing call [507] out of [1408] = [36.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 488 ms\n",
      "Tokens per second [79.9]\n",
      "Processing call [508] out of [1408] = [36.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 449 ms\n",
      "Tokens per second [78.0]\n",
      "Processing call [509] out of [1408] = [36.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 515 ms\n",
      "Tokens per second [81.6]\n",
      "Processing call [510] out of [1408] = [36.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 540 ms\n",
      "Tokens per second [81.5]\n",
      "Processing call [511] out of [1408] = [36.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 697 ms\n",
      "Tokens per second [86.1]\n",
      "Processing call [512] out of [1408] = [36.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 540 ms\n",
      "Tokens per second [81.5]\n",
      "Processing call [513] out of [1408] = [36.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 656 ms\n",
      "Tokens per second [85.4]\n",
      "Processing call [514] out of [1408] = [36.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 538 ms\n",
      "Tokens per second [81.8]\n",
      "Processing call [515] out of [1408] = [36.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 585 ms\n",
      "Tokens per second [83.8]\n",
      "Processing call [516] out of [1408] = [36.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 645 ms\n",
      "Tokens per second [85.3]\n",
      "Processing call [517] out of [1408] = [36.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 474 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [518] out of [1408] = [36.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 467 ms\n",
      "Tokens per second [79.2]\n",
      "Processing call [519] out of [1408] = [36.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 467 ms\n",
      "Tokens per second [79.2]\n",
      "Processing call [520] out of [1408] = [36.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 539 ms\n",
      "Tokens per second [81.6]\n",
      "Processing call [521] out of [1408] = [37.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 525 ms\n",
      "Tokens per second [81.9]\n",
      "Processing call [522] out of [1408] = [37.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 538 ms\n",
      "Tokens per second [81.8]\n",
      "Processing call [523] out of [1408] = [37.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 538 ms\n",
      "Tokens per second [81.8]\n",
      "Processing call [524] out of [1408] = [37.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 479 ms\n",
      "Tokens per second [79.3]\n",
      "Processing call [525] out of [1408] = [37.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 606 ms\n",
      "Tokens per second [84.2]\n",
      "Processing call [526] out of [1408] = [37.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 471 ms\n",
      "Tokens per second [78.6]\n",
      "Processing call [527] out of [1408] = [37.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 452 ms\n",
      "Tokens per second [77.4]\n",
      "Processing call [528] out of [1408] = [37.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 569 ms\n",
      "Tokens per second [82.6]\n",
      "Processing call [529] out of [1408] = [37.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 614 ms\n",
      "Tokens per second [84.7]\n",
      "Processing call [530] out of [1408] = [37.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 491 ms\n",
      "Tokens per second [79.4]\n",
      "Processing call [531] out of [1408] = [37.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 478 ms\n",
      "Tokens per second [79.5]\n",
      "Processing call [532] out of [1408] = [37.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 470 ms\n",
      "Tokens per second [78.7]\n",
      "Processing call [533] out of [1408] = [37.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 465 ms\n",
      "Tokens per second [79.6]\n",
      "Processing call [534] out of [1408] = [37.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 520 ms\n",
      "Tokens per second [80.8]\n",
      "Processing call [535] out of [1408] = [38.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 603 ms\n",
      "Tokens per second [84.6]\n",
      "Processing call [536] out of [1408] = [38.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 529 ms\n",
      "Tokens per second [81.3]\n",
      "Processing call [537] out of [1408] = [38.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 472 ms\n",
      "Tokens per second [78.4]\n",
      "Processing call [538] out of [1408] = [38.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 632 ms\n",
      "Tokens per second [83.9]\n",
      "Processing call [539] out of [1408] = [38.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 461 ms\n",
      "Tokens per second [78.1]\n",
      "Processing call [540] out of [1408] = [38.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 475 ms\n",
      "Tokens per second [77.9]\n",
      "Processing call [541] out of [1408] = [38.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 494 ms\n",
      "Tokens per second [78.9]\n",
      "Processing call [542] out of [1408] = [38.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 473 ms\n",
      "Tokens per second [78.2]\n",
      "Processing call [543] out of [1408] = [38.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 483 ms\n",
      "Tokens per second [78.7]\n",
      "Processing call [544] out of [1408] = [38.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 491 ms\n",
      "Tokens per second [79.4]\n",
      "Processing call [545] out of [1408] = [38.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 502 ms\n",
      "Tokens per second [79.7]\n",
      "Processing call [546] out of [1408] = [38.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 543 ms\n",
      "Tokens per second [81.0]\n",
      "Processing call [547] out of [1408] = [38.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 502 ms\n",
      "Tokens per second [79.7]\n",
      "Processing call [548] out of [1408] = [38.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 504 ms\n",
      "Tokens per second [79.4]\n",
      "Processing call [549] out of [1408] = [39.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 483 ms\n",
      "Tokens per second [78.7]\n",
      "Processing call [550] out of [1408] = [39.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 463 ms\n",
      "Tokens per second [77.8]\n",
      "Processing call [551] out of [1408] = [39.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 512 ms\n",
      "Tokens per second [80.1]\n",
      "Processing call [552] out of [1408] = [39.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 453 ms\n",
      "Tokens per second [77.3]\n",
      "Processing call [553] out of [1408] = [39.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 531 ms\n",
      "Tokens per second [81.0]\n",
      "Processing call [554] out of [1408] = [39.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 484 ms\n",
      "Tokens per second [78.5]\n",
      "Processing call [555] out of [1408] = [39.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 475 ms\n",
      "Tokens per second [77.9]\n",
      "Processing call [556] out of [1408] = [39.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 522 ms\n",
      "Tokens per second [80.5]\n",
      "Processing call [557] out of [1408] = [39.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 470 ms\n",
      "Tokens per second [78.7]\n",
      "Processing call [558] out of [1408] = [39.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 539 ms\n",
      "Tokens per second [81.6]\n",
      "Processing call [559] out of [1408] = [39.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 511 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [560] out of [1408] = [39.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 493 ms\n",
      "Tokens per second [79.1]\n",
      "Processing call [561] out of [1408] = [39.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 533 ms\n",
      "Tokens per second [80.7]\n",
      "Processing call [562] out of [1408] = [39.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 493 ms\n",
      "Tokens per second [79.1]\n",
      "Processing call [563] out of [1408] = [40.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 557 ms\n",
      "Tokens per second [82.6]\n",
      "Processing call [564] out of [1408] = [40.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 530 ms\n",
      "Tokens per second [81.1]\n",
      "Processing call [565] out of [1408] = [40.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 471 ms\n",
      "Tokens per second [78.6]\n",
      "Processing call [566] out of [1408] = [40.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 514 ms\n",
      "Tokens per second [79.8]\n",
      "Processing call [567] out of [1408] = [40.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 511 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [568] out of [1408] = [40.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 511 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [569] out of [1408] = [40.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 448 ms\n",
      "Tokens per second [78.1]\n",
      "Processing call [570] out of [1408] = [40.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 462 ms\n",
      "Tokens per second [77.9]\n",
      "Processing call [571] out of [1408] = [40.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 509 ms\n",
      "Tokens per second [80.6]\n",
      "Processing call [572] out of [1408] = [40.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 626 ms\n",
      "Tokens per second [84.7]\n",
      "Processing call [573] out of [1408] = [40.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 477 ms\n",
      "Tokens per second [79.7]\n",
      "Processing call [574] out of [1408] = [40.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 529 ms\n",
      "Tokens per second [81.3]\n",
      "Processing call [575] out of [1408] = [40.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 528 ms\n",
      "Tokens per second [81.4]\n",
      "Processing call [576] out of [1408] = [40.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 508 ms\n",
      "Tokens per second [80.7]\n",
      "Processing call [577] out of [1408] = [41.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 486 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [578] out of [1408] = [41.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 467 ms\n",
      "Tokens per second [79.2]\n",
      "Processing call [579] out of [1408] = [41.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 468 ms\n",
      "Tokens per second [79.1]\n",
      "Processing call [580] out of [1408] = [41.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 489 ms\n",
      "Tokens per second [79.8]\n",
      "Processing call [581] out of [1408] = [41.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 476 ms\n",
      "Tokens per second [79.8]\n",
      "Processing call [582] out of [1408] = [41.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 609 ms\n",
      "Tokens per second [83.7]\n",
      "Processing call [583] out of [1408] = [41.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 474 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [584] out of [1408] = [41.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 470 ms\n",
      "Tokens per second [78.7]\n",
      "Processing call [585] out of [1408] = [41.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 467 ms\n",
      "Tokens per second [79.2]\n",
      "Processing call [586] out of [1408] = [41.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 541 ms\n",
      "Tokens per second [81.3]\n",
      "Processing call [587] out of [1408] = [41.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 466 ms\n",
      "Tokens per second [79.4]\n",
      "Processing call [588] out of [1408] = [41.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 510 ms\n",
      "Tokens per second [80.4]\n",
      "Processing call [589] out of [1408] = [41.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 575 ms\n",
      "Tokens per second [83.5]\n",
      "Processing call [590] out of [1408] = [41.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 480 ms\n",
      "Tokens per second [79.2]\n",
      "Processing call [591] out of [1408] = [42.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 519 ms\n",
      "Tokens per second [80.9]\n",
      "Processing call [592] out of [1408] = [42.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 530 ms\n",
      "Tokens per second [81.1]\n",
      "Processing call [593] out of [1408] = [42.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 488 ms\n",
      "Tokens per second [79.9]\n",
      "Processing call [594] out of [1408] = [42.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 473 ms\n",
      "Tokens per second [78.2]\n",
      "Processing call [595] out of [1408] = [42.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 480 ms\n",
      "Tokens per second [79.2]\n",
      "Processing call [596] out of [1408] = [42.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 513 ms\n",
      "Tokens per second [79.9]\n",
      "Processing call [597] out of [1408] = [42.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 482 ms\n",
      "Tokens per second [78.8]\n",
      "Processing call [598] out of [1408] = [42.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 510 ms\n",
      "Tokens per second [80.4]\n",
      "Processing call [599] out of [1408] = [42.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 490 ms\n",
      "Tokens per second [79.6]\n",
      "Processing call [600] out of [1408] = [42.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 453 ms\n",
      "Tokens per second [77.3]\n",
      "Processing call [601] out of [1408] = [42.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 468 ms\n",
      "Tokens per second [79.1]\n",
      "Processing call [602] out of [1408] = [42.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 472 ms\n",
      "Tokens per second [78.4]\n",
      "Processing call [603] out of [1408] = [42.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 530 ms\n",
      "Tokens per second [81.1]\n",
      "Processing call [604] out of [1408] = [42.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 527 ms\n",
      "Tokens per second [81.6]\n",
      "Processing call [605] out of [1408] = [43.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 520 ms\n",
      "Tokens per second [80.8]\n",
      "Processing call [606] out of [1408] = [43.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 488 ms\n",
      "Tokens per second [79.9]\n",
      "Processing call [607] out of [1408] = [43.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 533 ms\n",
      "Tokens per second [82.6]\n",
      "Processing call [608] out of [1408] = [43.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 447 ms\n",
      "Tokens per second [78.3]\n",
      "Processing call [609] out of [1408] = [43.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 592 ms\n",
      "Tokens per second [84.5]\n",
      "Processing call [610] out of [1408] = [43.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 538 ms\n",
      "Tokens per second [81.8]\n",
      "Processing call [611] out of [1408] = [43.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 466 ms\n",
      "Tokens per second [79.4]\n",
      "Processing call [612] out of [1408] = [43.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 488 ms\n",
      "Tokens per second [79.9]\n",
      "Processing call [613] out of [1408] = [43.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 506 ms\n",
      "Tokens per second [81.0]\n",
      "Processing call [614] out of [1408] = [43.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 495 ms\n",
      "Tokens per second [80.8]\n",
      "Processing call [615] out of [1408] = [43.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 457 ms\n",
      "Tokens per second [78.8]\n",
      "Processing call [616] out of [1408] = [43.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 550 ms\n",
      "Tokens per second [80.0]\n",
      "Processing call [617] out of [1408] = [43.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 486 ms\n",
      "Tokens per second [78.2]\n",
      "Processing call [618] out of [1408] = [43.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 626 ms\n",
      "Tokens per second [83.1]\n",
      "Processing call [619] out of [1408] = [44.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 525 ms\n",
      "Tokens per second [80.0]\n",
      "Processing call [620] out of [1408] = [44.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 620 ms\n",
      "Tokens per second [83.9]\n",
      "Processing call [621] out of [1408] = [44.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 494 ms\n",
      "Tokens per second [78.9]\n",
      "Processing call [622] out of [1408] = [44.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 465 ms\n",
      "Tokens per second [77.4]\n",
      "Processing call [623] out of [1408] = [44.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 510 ms\n",
      "Tokens per second [80.4]\n",
      "Processing call [624] out of [1408] = [44.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 619 ms\n",
      "Tokens per second [84.0]\n",
      "Processing call [625] out of [1408] = [44.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 447 ms\n",
      "Tokens per second [78.3]\n",
      "Processing call [626] out of [1408] = [44.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 498 ms\n",
      "Tokens per second [80.3]\n",
      "Processing call [627] out of [1408] = [44.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 519 ms\n",
      "Tokens per second [80.9]\n",
      "Processing call [628] out of [1408] = [44.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 489 ms\n",
      "Tokens per second [79.8]\n",
      "Processing call [629] out of [1408] = [44.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 504 ms\n",
      "Tokens per second [81.3]\n",
      "Processing call [630] out of [1408] = [44.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 510 ms\n",
      "Tokens per second [80.4]\n",
      "Processing call [631] out of [1408] = [44.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 501 ms\n",
      "Tokens per second [81.8]\n",
      "Processing call [632] out of [1408] = [44.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 498 ms\n",
      "Tokens per second [80.3]\n",
      "Processing call [633] out of [1408] = [45.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 463 ms\n",
      "Tokens per second [79.9]\n",
      "Processing call [634] out of [1408] = [45.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 517 ms\n",
      "Tokens per second [81.2]\n",
      "Processing call [635] out of [1408] = [45.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 516 ms\n",
      "Tokens per second [81.4]\n",
      "Processing call [636] out of [1408] = [45.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 524 ms\n",
      "Tokens per second [82.1]\n",
      "Processing call [637] out of [1408] = [45.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 517 ms\n",
      "Tokens per second [81.2]\n",
      "Processing call [638] out of [1408] = [45.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 518 ms\n",
      "Tokens per second [81.1]\n",
      "Processing call [639] out of [1408] = [45.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 583 ms\n",
      "Tokens per second [84.0]\n",
      "Processing call [640] out of [1408] = [45.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 537 ms\n",
      "Tokens per second [81.9]\n",
      "Processing call [641] out of [1408] = [45.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 487 ms\n",
      "Tokens per second [80.1]\n",
      "Processing call [642] out of [1408] = [45.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 647 ms\n",
      "Tokens per second [85.0]\n",
      "Processing call [643] out of [1408] = [45.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 503 ms\n",
      "Tokens per second [81.5]\n",
      "Processing call [644] out of [1408] = [45.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 609 ms\n",
      "Tokens per second [83.7]\n",
      "Processing call [645] out of [1408] = [45.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 505 ms\n",
      "Tokens per second [81.2]\n",
      "Processing call [646] out of [1408] = [45.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 566 ms\n",
      "Tokens per second [83.0]\n",
      "Processing call [647] out of [1408] = [46.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 467 ms\n",
      "Tokens per second [79.2]\n",
      "Processing call [648] out of [1408] = [46.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 535 ms\n",
      "Tokens per second [82.2]\n",
      "Processing call [649] out of [1408] = [46.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 534 ms\n",
      "Tokens per second [82.4]\n",
      "Processing call [650] out of [1408] = [46.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 518 ms\n",
      "Tokens per second [81.1]\n",
      "Processing call [651] out of [1408] = [46.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 603 ms\n",
      "Tokens per second [84.6]\n",
      "Processing call [652] out of [1408] = [46.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 574 ms\n",
      "Tokens per second [83.6]\n",
      "Processing call [653] out of [1408] = [46.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 532 ms\n",
      "Tokens per second [82.7]\n",
      "Processing call [654] out of [1408] = [46.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 643 ms\n",
      "Tokens per second [85.5]\n",
      "Processing call [655] out of [1408] = [46.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 603 ms\n",
      "Tokens per second [84.6]\n",
      "Processing call [656] out of [1408] = [46.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 472 ms\n",
      "Tokens per second [78.4]\n",
      "Processing call [657] out of [1408] = [46.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 536 ms\n",
      "Tokens per second [82.1]\n",
      "Processing call [658] out of [1408] = [46.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 481 ms\n",
      "Tokens per second [79.0]\n",
      "Processing call [659] out of [1408] = [46.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 483 ms\n",
      "Tokens per second [80.7]\n",
      "Processing call [660] out of [1408] = [46.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 477 ms\n",
      "Tokens per second [79.7]\n",
      "Processing call [661] out of [1408] = [46.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 498 ms\n",
      "Tokens per second [80.3]\n",
      "Processing call [662] out of [1408] = [47.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 513 ms\n",
      "Tokens per second [81.9]\n",
      "Processing call [663] out of [1408] = [47.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 506 ms\n",
      "Tokens per second [81.0]\n",
      "Processing call [664] out of [1408] = [47.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 466 ms\n",
      "Tokens per second [79.4]\n",
      "Processing call [665] out of [1408] = [47.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 591 ms\n",
      "Tokens per second [84.6]\n",
      "Processing call [666] out of [1408] = [47.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 524 ms\n",
      "Tokens per second [82.1]\n",
      "Processing call [667] out of [1408] = [47.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 524 ms\n",
      "Tokens per second [82.1]\n",
      "Processing call [668] out of [1408] = [47.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 543 ms\n",
      "Tokens per second [82.9]\n",
      "Processing call [669] out of [1408] = [47.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 463 ms\n",
      "Tokens per second [79.9]\n",
      "Processing call [670] out of [1408] = [47.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 584 ms\n",
      "Tokens per second [83.9]\n",
      "Processing call [671] out of [1408] = [47.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 456 ms\n",
      "Tokens per second [78.9]\n",
      "Processing call [672] out of [1408] = [47.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 526 ms\n",
      "Tokens per second [81.7]\n",
      "Processing call [673] out of [1408] = [47.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..............................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 404 ms\n",
      "Tokens per second [74.3]\n",
      "Processing call [674] out of [1408] = [47.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 533 ms\n",
      "Tokens per second [80.7]\n",
      "Processing call [675] out of [1408] = [47.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 474 ms\n",
      "Tokens per second [78.1]\n",
      "Processing call [676] out of [1408] = [48.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 498 ms\n",
      "Tokens per second [78.3]\n",
      "Processing call [677] out of [1408] = [48.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 614 ms\n",
      "Tokens per second [83.1]\n",
      "Processing call [678] out of [1408] = [48.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 503 ms\n",
      "Tokens per second [79.5]\n",
      "Processing call [679] out of [1408] = [48.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 466 ms\n",
      "Tokens per second [77.3]\n",
      "Processing call [680] out of [1408] = [48.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 495 ms\n",
      "Tokens per second [78.8]\n",
      "Processing call [681] out of [1408] = [48.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 694 ms\n",
      "Tokens per second [85.0]\n",
      "Processing call [682] out of [1408] = [48.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 484 ms\n",
      "Tokens per second [78.5]\n",
      "Processing call [683] out of [1408] = [48.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 523 ms\n",
      "Tokens per second [80.3]\n",
      "Processing call [684] out of [1408] = [48.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 474 ms\n",
      "Tokens per second [78.1]\n",
      "Processing call [685] out of [1408] = [48.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 473 ms\n",
      "Tokens per second [78.2]\n",
      "Processing call [686] out of [1408] = [48.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 524 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [687] out of [1408] = [48.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 531 ms\n",
      "Tokens per second [81.0]\n",
      "Processing call [688] out of [1408] = [48.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 605 ms\n",
      "Tokens per second [82.6]\n",
      "Processing call [689] out of [1408] = [48.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 540 ms\n",
      "Tokens per second [81.5]\n",
      "Processing call [690] out of [1408] = [49.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 494 ms\n",
      "Tokens per second [78.9]\n",
      "Processing call [691] out of [1408] = [49.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 484 ms\n",
      "Tokens per second [78.5]\n",
      "Processing call [692] out of [1408] = [49.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 494 ms\n",
      "Tokens per second [78.9]\n",
      "Processing call [693] out of [1408] = [49.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 483 ms\n",
      "Tokens per second [78.7]\n",
      "Processing call [694] out of [1408] = [49.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 472 ms\n",
      "Tokens per second [78.4]\n",
      "Processing call [695] out of [1408] = [49.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 654 ms\n",
      "Tokens per second [84.1]\n",
      "Processing call [696] out of [1408] = [49.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 551 ms\n",
      "Tokens per second [81.7]\n",
      "Processing call [697] out of [1408] = [49.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 455 ms\n",
      "Tokens per second [76.9]\n",
      "Processing call [698] out of [1408] = [49.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 511 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [699] out of [1408] = [49.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 519 ms\n",
      "Tokens per second [79.0]\n",
      "Processing call [700] out of [1408] = [49.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 625 ms\n",
      "Tokens per second [83.2]\n",
      "Processing call [701] out of [1408] = [49.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 471 ms\n",
      "Tokens per second [78.6]\n",
      "Processing call [702] out of [1408] = [49.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 524 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [703] out of [1408] = [49.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 458 ms\n",
      "Tokens per second [76.4]\n",
      "Processing call [704] out of [1408] = [50.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 472 ms\n",
      "Tokens per second [78.4]\n",
      "Processing call [705] out of [1408] = [50.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 516 ms\n",
      "Tokens per second [79.5]\n",
      "Processing call [706] out of [1408] = [50.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 465 ms\n",
      "Tokens per second [77.4]\n",
      "Processing call [707] out of [1408] = [50.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 481 ms\n",
      "Tokens per second [79.0]\n",
      "Processing call [708] out of [1408] = [50.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 506 ms\n",
      "Tokens per second [79.1]\n",
      "Processing call [709] out of [1408] = [50.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 501 ms\n",
      "Tokens per second [79.8]\n",
      "Processing call [710] out of [1408] = [50.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 483 ms\n",
      "Tokens per second [78.7]\n",
      "Processing call [711] out of [1408] = [50.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 456 ms\n",
      "Tokens per second [76.8]\n",
      "Processing call [712] out of [1408] = [50.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 494 ms\n",
      "Tokens per second [78.9]\n",
      "Processing call [713] out of [1408] = [50.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 544 ms\n",
      "Tokens per second [80.9]\n",
      "Processing call [714] out of [1408] = [50.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 653 ms\n",
      "Tokens per second [84.2]\n",
      "Processing call [715] out of [1408] = [50.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 476 ms\n",
      "Tokens per second [77.7]\n",
      "Processing call [716] out of [1408] = [50.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 526 ms\n",
      "Tokens per second [79.8]\n",
      "Processing call [717] out of [1408] = [50.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 473 ms\n",
      "Tokens per second [78.2]\n",
      "Processing call [718] out of [1408] = [51.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 544 ms\n",
      "Tokens per second [80.9]\n",
      "Processing call [719] out of [1408] = [51.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 623 ms\n",
      "Tokens per second [83.5]\n",
      "Processing call [720] out of [1408] = [51.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 492 ms\n",
      "Tokens per second [79.3]\n",
      "Processing call [721] out of [1408] = [51.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 457 ms\n",
      "Tokens per second [76.6]\n",
      "Processing call [722] out of [1408] = [51.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 464 ms\n",
      "Tokens per second [77.6]\n",
      "Processing call [723] out of [1408] = [51.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 530 ms\n",
      "Tokens per second [81.1]\n",
      "Processing call [724] out of [1408] = [51.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 535 ms\n",
      "Tokens per second [80.4]\n",
      "Processing call [725] out of [1408] = [51.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 521 ms\n",
      "Tokens per second [80.6]\n",
      "Processing call [726] out of [1408] = [51.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 584 ms\n",
      "Tokens per second [82.2]\n",
      "Processing call [727] out of [1408] = [51.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 620 ms\n",
      "Tokens per second [82.3]\n",
      "Processing call [728] out of [1408] = [51.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 516 ms\n",
      "Tokens per second [79.5]\n",
      "Processing call [729] out of [1408] = [51.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 470 ms\n",
      "Tokens per second [76.6]\n",
      "Processing call [730] out of [1408] = [51.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 486 ms\n",
      "Tokens per second [78.2]\n",
      "Processing call [731] out of [1408] = [51.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 472 ms\n",
      "Tokens per second [78.4]\n",
      "Processing call [732] out of [1408] = [52.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 456 ms\n",
      "Tokens per second [76.8]\n",
      "Processing call [733] out of [1408] = [52.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 497 ms\n",
      "Tokens per second [78.5]\n",
      "Processing call [734] out of [1408] = [52.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 585 ms\n",
      "Tokens per second [82.1]\n",
      "Processing call [735] out of [1408] = [52.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 456 ms\n",
      "Tokens per second [76.8]\n",
      "Processing call [736] out of [1408] = [52.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 509 ms\n",
      "Tokens per second [80.6]\n",
      "Processing call [737] out of [1408] = [52.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 446 ms\n",
      "Tokens per second [78.5]\n",
      "Processing call [738] out of [1408] = [52.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 605 ms\n",
      "Tokens per second [84.3]\n",
      "Processing call [739] out of [1408] = [52.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 518 ms\n",
      "Tokens per second [81.1]\n",
      "Processing call [740] out of [1408] = [52.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 525 ms\n",
      "Tokens per second [81.9]\n",
      "Processing call [741] out of [1408] = [52.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 597 ms\n",
      "Tokens per second [83.8]\n",
      "Processing call [742] out of [1408] = [52.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 515 ms\n",
      "Tokens per second [81.6]\n",
      "Processing call [743] out of [1408] = [52.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 476 ms\n",
      "Tokens per second [79.8]\n",
      "Processing call [744] out of [1408] = [52.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 523 ms\n",
      "Tokens per second [82.2]\n",
      "Processing call [745] out of [1408] = [52.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 465 ms\n",
      "Tokens per second [79.6]\n",
      "Processing call [746] out of [1408] = [53.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 496 ms\n",
      "Tokens per second [80.6]\n",
      "Processing call [747] out of [1408] = [53.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 643 ms\n",
      "Tokens per second [85.5]\n",
      "Processing call [748] out of [1408] = [53.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 475 ms\n",
      "Tokens per second [80.0]\n",
      "Processing call [749] out of [1408] = [53.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 519 ms\n",
      "Tokens per second [80.9]\n",
      "Processing call [750] out of [1408] = [53.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 446 ms\n",
      "Tokens per second [78.5]\n",
      "Processing call [751] out of [1408] = [53.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 477 ms\n",
      "Tokens per second [77.6]\n",
      "Processing call [752] out of [1408] = [53.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 475 ms\n",
      "Tokens per second [77.9]\n",
      "Processing call [753] out of [1408] = [53.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 484 ms\n",
      "Tokens per second [78.5]\n",
      "Processing call [754] out of [1408] = [53.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 479 ms\n",
      "Tokens per second [79.3]\n",
      "Processing call [755] out of [1408] = [53.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 618 ms\n",
      "Tokens per second [84.1]\n",
      "Processing call [756] out of [1408] = [53.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 605 ms\n",
      "Tokens per second [84.3]\n",
      "Processing call [757] out of [1408] = [53.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 589 ms\n",
      "Tokens per second [84.9]\n",
      "Processing call [758] out of [1408] = [53.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 487 ms\n",
      "Tokens per second [80.1]\n",
      "Processing call [759] out of [1408] = [53.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 473 ms\n",
      "Tokens per second [80.3]\n",
      "Processing call [760] out of [1408] = [54.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 465 ms\n",
      "Tokens per second [79.6]\n",
      "Processing call [761] out of [1408] = [54.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 620 ms\n",
      "Tokens per second [85.5]\n",
      "Processing call [762] out of [1408] = [54.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 525 ms\n",
      "Tokens per second [81.9]\n",
      "Processing call [763] out of [1408] = [54.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 470 ms\n",
      "Tokens per second [80.9]\n",
      "Processing call [764] out of [1408] = [54.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..............................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 397 ms\n",
      "Tokens per second [75.6]\n",
      "Processing call [765] out of [1408] = [54.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 485 ms\n",
      "Tokens per second [80.4]\n",
      "Processing call [766] out of [1408] = [54.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 621 ms\n",
      "Tokens per second [85.3]\n",
      "Processing call [767] out of [1408] = [54.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 513 ms\n",
      "Tokens per second [81.9]\n",
      "Processing call [768] out of [1408] = [54.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 450 ms\n",
      "Tokens per second [77.8]\n",
      "Processing call [769] out of [1408] = [54.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 574 ms\n",
      "Tokens per second [83.6]\n",
      "Processing call [770] out of [1408] = [54.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 510 ms\n",
      "Tokens per second [80.4]\n",
      "Processing call [771] out of [1408] = [54.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 469 ms\n",
      "Tokens per second [78.9]\n",
      "Processing call [772] out of [1408] = [54.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 502 ms\n",
      "Tokens per second [81.7]\n",
      "Processing call [773] out of [1408] = [54.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 487 ms\n",
      "Tokens per second [80.1]\n",
      "Processing call [774] out of [1408] = [55.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 566 ms\n",
      "Tokens per second [83.0]\n",
      "Processing call [775] out of [1408] = [55.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 625 ms\n",
      "Tokens per second [84.8]\n",
      "Processing call [776] out of [1408] = [55.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 498 ms\n",
      "Tokens per second [80.3]\n",
      "Processing call [777] out of [1408] = [55.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 606 ms\n",
      "Tokens per second [84.2]\n",
      "Processing call [778] out of [1408] = [55.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 462 ms\n",
      "Tokens per second [80.1]\n",
      "Processing call [779] out of [1408] = [55.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 515 ms\n",
      "Tokens per second [81.6]\n",
      "Processing call [780] out of [1408] = [55.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 446 ms\n",
      "Tokens per second [78.5]\n",
      "Processing call [781] out of [1408] = [55.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 452 ms\n",
      "Tokens per second [79.6]\n",
      "Processing call [782] out of [1408] = [55.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 455 ms\n",
      "Tokens per second [79.1]\n",
      "Processing call [783] out of [1408] = [55.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 497 ms\n",
      "Tokens per second [80.5]\n",
      "Processing call [784] out of [1408] = [55.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 501 ms\n",
      "Tokens per second [81.8]\n",
      "Processing call [785] out of [1408] = [55.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 533 ms\n",
      "Tokens per second [80.7]\n",
      "Processing call [786] out of [1408] = [55.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 533 ms\n",
      "Tokens per second [80.7]\n",
      "Processing call [787] out of [1408] = [55.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 480 ms\n",
      "Tokens per second [79.2]\n",
      "Processing call [788] out of [1408] = [56.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 505 ms\n",
      "Tokens per second [79.2]\n",
      "Processing call [789] out of [1408] = [56.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 620 ms\n",
      "Tokens per second [83.9]\n",
      "Processing call [790] out of [1408] = [56.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 510 ms\n",
      "Tokens per second [80.4]\n",
      "Processing call [791] out of [1408] = [56.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 475 ms\n",
      "Tokens per second [77.9]\n",
      "Processing call [792] out of [1408] = [56.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 481 ms\n",
      "Tokens per second [79.0]\n",
      "Processing call [793] out of [1408] = [56.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 521 ms\n",
      "Tokens per second [80.6]\n",
      "Processing call [794] out of [1408] = [56.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 480 ms\n",
      "Tokens per second [79.2]\n",
      "Processing call [795] out of [1408] = [56.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 527 ms\n",
      "Tokens per second [81.6]\n",
      "Processing call [796] out of [1408] = [56.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 470 ms\n",
      "Tokens per second [78.7]\n",
      "Processing call [797] out of [1408] = [56.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 476 ms\n",
      "Tokens per second [79.8]\n",
      "Processing call [798] out of [1408] = [56.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 493 ms\n",
      "Tokens per second [79.1]\n",
      "Processing call [799] out of [1408] = [56.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 458 ms\n",
      "Tokens per second [78.6]\n",
      "Processing call [800] out of [1408] = [56.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 543 ms\n",
      "Tokens per second [81.0]\n",
      "Processing call [801] out of [1408] = [56.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 609 ms\n",
      "Tokens per second [83.7]\n",
      "Processing call [802] out of [1408] = [57.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 502 ms\n",
      "Tokens per second [79.7]\n",
      "Processing call [803] out of [1408] = [57.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 469 ms\n",
      "Tokens per second [78.9]\n",
      "Processing call [804] out of [1408] = [57.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 514 ms\n",
      "Tokens per second [79.8]\n",
      "Processing call [805] out of [1408] = [57.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 450 ms\n",
      "Tokens per second [77.8]\n",
      "Processing call [806] out of [1408] = [57.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 549 ms\n",
      "Tokens per second [82.0]\n",
      "Processing call [807] out of [1408] = [57.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 641 ms\n",
      "Tokens per second [84.2]\n",
      "Processing call [808] out of [1408] = [57.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 630 ms\n",
      "Tokens per second [84.1]\n",
      "Processing call [809] out of [1408] = [57.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 471 ms\n",
      "Tokens per second [78.6]\n",
      "Processing call [810] out of [1408] = [57.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 530 ms\n",
      "Tokens per second [81.1]\n",
      "Processing call [811] out of [1408] = [57.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 522 ms\n",
      "Tokens per second [80.5]\n",
      "Processing call [812] out of [1408] = [57.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 481 ms\n",
      "Tokens per second [79.0]\n",
      "Processing call [813] out of [1408] = [57.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 483 ms\n",
      "Tokens per second [78.7]\n",
      "Processing call [814] out of [1408] = [57.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 452 ms\n",
      "Tokens per second [77.4]\n",
      "Processing call [815] out of [1408] = [57.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 463 ms\n",
      "Tokens per second [77.8]\n",
      "Processing call [816] out of [1408] = [58.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 516 ms\n",
      "Tokens per second [79.5]\n",
      "Processing call [817] out of [1408] = [58.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 513 ms\n",
      "Tokens per second [79.9]\n",
      "Processing call [818] out of [1408] = [58.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..............................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 403 ms\n",
      "Tokens per second [74.4]\n",
      "Processing call [819] out of [1408] = [58.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 541 ms\n",
      "Tokens per second [81.3]\n",
      "Processing call [820] out of [1408] = [58.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 503 ms\n",
      "Tokens per second [79.5]\n",
      "Processing call [821] out of [1408] = [58.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 462 ms\n",
      "Tokens per second [77.9]\n",
      "Processing call [822] out of [1408] = [58.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 484 ms\n",
      "Tokens per second [78.5]\n",
      "Processing call [823] out of [1408] = [58.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 531 ms\n",
      "Tokens per second [81.0]\n",
      "Processing call [824] out of [1408] = [58.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 504 ms\n",
      "Tokens per second [79.4]\n",
      "Processing call [825] out of [1408] = [58.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 452 ms\n",
      "Tokens per second [77.4]\n",
      "Processing call [826] out of [1408] = [58.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 508 ms\n",
      "Tokens per second [78.7]\n",
      "Processing call [827] out of [1408] = [58.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 659 ms\n",
      "Tokens per second [85.0]\n",
      "Processing call [828] out of [1408] = [58.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 492 ms\n",
      "Tokens per second [79.3]\n",
      "Processing call [829] out of [1408] = [58.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 484 ms\n",
      "Tokens per second [78.5]\n",
      "Processing call [830] out of [1408] = [58.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 502 ms\n",
      "Tokens per second [79.7]\n",
      "Processing call [831] out of [1408] = [59.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 485 ms\n",
      "Tokens per second [78.4]\n",
      "Processing call [832] out of [1408] = [59.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 552 ms\n",
      "Tokens per second [81.5]\n",
      "Processing call [833] out of [1408] = [59.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 523 ms\n",
      "Tokens per second [80.3]\n",
      "Processing call [834] out of [1408] = [59.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 630 ms\n",
      "Tokens per second [84.1]\n",
      "Processing call [835] out of [1408] = [59.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 492 ms\n",
      "Tokens per second [79.3]\n",
      "Processing call [836] out of [1408] = [59.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 531 ms\n",
      "Tokens per second [80.8]\n",
      "Processing call [837] out of [1408] = [59.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 545 ms\n",
      "Tokens per second [80.7]\n",
      "Processing call [838] out of [1408] = [59.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 541 ms\n",
      "Tokens per second [81.3]\n",
      "Processing call [839] out of [1408] = [59.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 459 ms\n",
      "Tokens per second [78.4]\n",
      "Processing call [840] out of [1408] = [59.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 475 ms\n",
      "Tokens per second [77.9]\n",
      "Processing call [841] out of [1408] = [59.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 510 ms\n",
      "Tokens per second [80.4]\n",
      "Processing call [842] out of [1408] = [59.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 474 ms\n",
      "Tokens per second [78.1]\n",
      "Processing call [843] out of [1408] = [59.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 532 ms\n",
      "Tokens per second [80.8]\n",
      "Processing call [844] out of [1408] = [59.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 482 ms\n",
      "Tokens per second [78.8]\n",
      "Processing call [845] out of [1408] = [60.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 482 ms\n",
      "Tokens per second [78.8]\n",
      "Processing call [846] out of [1408] = [60.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 610 ms\n",
      "Tokens per second [83.6]\n",
      "Processing call [847] out of [1408] = [60.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 523 ms\n",
      "Tokens per second [80.3]\n",
      "Processing call [848] out of [1408] = [60.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 460 ms\n",
      "Tokens per second [78.3]\n",
      "Processing call [849] out of [1408] = [60.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 549 ms\n",
      "Tokens per second [82.0]\n",
      "Processing call [850] out of [1408] = [60.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 615 ms\n",
      "Tokens per second [84.6]\n",
      "Processing call [851] out of [1408] = [60.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 527 ms\n",
      "Tokens per second [81.6]\n",
      "Processing call [852] out of [1408] = [60.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 607 ms\n",
      "Tokens per second [84.0]\n",
      "Processing call [853] out of [1408] = [60.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 465 ms\n",
      "Tokens per second [79.6]\n",
      "Processing call [854] out of [1408] = [60.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 536 ms\n",
      "Tokens per second [82.1]\n",
      "Processing call [855] out of [1408] = [60.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 463 ms\n",
      "Tokens per second [79.9]\n",
      "Processing call [856] out of [1408] = [60.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 604 ms\n",
      "Tokens per second [84.4]\n",
      "Processing call [857] out of [1408] = [60.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 494 ms\n",
      "Tokens per second [81.0]\n",
      "Processing call [858] out of [1408] = [60.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 534 ms\n",
      "Tokens per second [82.4]\n",
      "Processing call [859] out of [1408] = [61.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 474 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [860] out of [1408] = [61.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 463 ms\n",
      "Tokens per second [79.9]\n",
      "Processing call [861] out of [1408] = [61.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 533 ms\n",
      "Tokens per second [82.6]\n",
      "Processing call [862] out of [1408] = [61.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 473 ms\n",
      "Tokens per second [80.3]\n",
      "Processing call [863] out of [1408] = [61.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 466 ms\n",
      "Tokens per second [79.4]\n",
      "Processing call [864] out of [1408] = [61.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 453 ms\n",
      "Tokens per second [79.5]\n",
      "Processing call [865] out of [1408] = [61.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 447 ms\n",
      "Tokens per second [78.3]\n",
      "Processing call [866] out of [1408] = [61.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 515 ms\n",
      "Tokens per second [81.6]\n",
      "Processing call [867] out of [1408] = [61.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 464 ms\n",
      "Tokens per second [79.7]\n",
      "Processing call [868] out of [1408] = [61.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 526 ms\n",
      "Tokens per second [81.7]\n",
      "Processing call [869] out of [1408] = [61.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 504 ms\n",
      "Tokens per second [81.3]\n",
      "Processing call [870] out of [1408] = [61.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 536 ms\n",
      "Tokens per second [82.1]\n",
      "Processing call [871] out of [1408] = [61.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 464 ms\n",
      "Tokens per second [79.7]\n",
      "Processing call [872] out of [1408] = [61.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 486 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [873] out of [1408] = [62.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 466 ms\n",
      "Tokens per second [79.4]\n",
      "Processing call [874] out of [1408] = [62.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 482 ms\n",
      "Tokens per second [80.9]\n",
      "Processing call [875] out of [1408] = [62.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 445 ms\n",
      "Tokens per second [78.7]\n",
      "Processing call [876] out of [1408] = [62.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 444 ms\n",
      "Tokens per second [78.8]\n",
      "Processing call [877] out of [1408] = [62.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 525 ms\n",
      "Tokens per second [81.9]\n",
      "Processing call [878] out of [1408] = [62.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 468 ms\n",
      "Tokens per second [79.1]\n",
      "Processing call [879] out of [1408] = [62.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 574 ms\n",
      "Tokens per second [83.6]\n",
      "Processing call [880] out of [1408] = [62.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 505 ms\n",
      "Tokens per second [81.2]\n",
      "Processing call [881] out of [1408] = [62.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 464 ms\n",
      "Tokens per second [79.7]\n",
      "Processing call [882] out of [1408] = [62.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 464 ms\n",
      "Tokens per second [79.7]\n",
      "Processing call [883] out of [1408] = [62.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 521 ms\n",
      "Tokens per second [82.5]\n",
      "Processing call [884] out of [1408] = [62.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 469 ms\n",
      "Tokens per second [78.9]\n",
      "Processing call [885] out of [1408] = [62.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 479 ms\n",
      "Tokens per second [79.3]\n",
      "Processing call [886] out of [1408] = [62.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 615 ms\n",
      "Tokens per second [84.6]\n",
      "Processing call [887] out of [1408] = [63.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 452 ms\n",
      "Tokens per second [79.6]\n",
      "Processing call [888] out of [1408] = [63.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 524 ms\n",
      "Tokens per second [82.1]\n",
      "Processing call [889] out of [1408] = [63.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 502 ms\n",
      "Tokens per second [81.7]\n",
      "Processing call [890] out of [1408] = [63.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 514 ms\n",
      "Tokens per second [81.7]\n",
      "Processing call [891] out of [1408] = [63.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 513 ms\n",
      "Tokens per second [81.9]\n",
      "Processing call [892] out of [1408] = [63.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 495 ms\n",
      "Tokens per second [80.8]\n",
      "Processing call [893] out of [1408] = [63.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 467 ms\n",
      "Tokens per second [79.2]\n",
      "Processing call [894] out of [1408] = [63.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 477 ms\n",
      "Tokens per second [79.7]\n",
      "Processing call [895] out of [1408] = [63.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 462 ms\n",
      "Tokens per second [80.1]\n",
      "Processing call [896] out of [1408] = [63.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 474 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [897] out of [1408] = [63.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 494 ms\n",
      "Tokens per second [81.0]\n",
      "Processing call [898] out of [1408] = [63.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 526 ms\n",
      "Tokens per second [81.7]\n",
      "Processing call [899] out of [1408] = [63.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 552 ms\n",
      "Tokens per second [83.3]\n",
      "Processing call [900] out of [1408] = [63.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 537 ms\n",
      "Tokens per second [81.9]\n",
      "Processing call [901] out of [1408] = [64.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 493 ms\n",
      "Tokens per second [81.1]\n",
      "Processing call [902] out of [1408] = [64.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 465 ms\n",
      "Tokens per second [79.6]\n",
      "Processing call [903] out of [1408] = [64.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 454 ms\n",
      "Tokens per second [79.3]\n",
      "Processing call [904] out of [1408] = [64.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 464 ms\n",
      "Tokens per second [79.7]\n",
      "Processing call [905] out of [1408] = [64.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 536 ms\n",
      "Tokens per second [82.1]\n",
      "Processing call [906] out of [1408] = [64.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 463 ms\n",
      "Tokens per second [79.9]\n",
      "Processing call [907] out of [1408] = [64.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 536 ms\n",
      "Tokens per second [82.1]\n",
      "Processing call [908] out of [1408] = [64.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 513 ms\n",
      "Tokens per second [81.7]\n",
      "Processing call [909] out of [1408] = [64.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 693 ms\n",
      "Tokens per second [86.6]\n",
      "Processing call [910] out of [1408] = [64.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 478 ms\n",
      "Tokens per second [79.5]\n",
      "Processing call [911] out of [1408] = [64.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 485 ms\n",
      "Tokens per second [80.4]\n",
      "Processing call [912] out of [1408] = [64.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 517 ms\n",
      "Tokens per second [81.2]\n",
      "Processing call [913] out of [1408] = [64.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 482 ms\n",
      "Tokens per second [80.9]\n",
      "Processing call [914] out of [1408] = [64.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 516 ms\n",
      "Tokens per second [81.4]\n",
      "Processing call [915] out of [1408] = [65.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 455 ms\n",
      "Tokens per second [79.1]\n",
      "Processing call [916] out of [1408] = [65.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 596 ms\n",
      "Tokens per second [83.9]\n",
      "Processing call [917] out of [1408] = [65.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 473 ms\n",
      "Tokens per second [80.3]\n",
      "Processing call [918] out of [1408] = [65.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 528 ms\n",
      "Tokens per second [81.4]\n",
      "Processing call [919] out of [1408] = [65.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 522 ms\n",
      "Tokens per second [82.4]\n",
      "Processing call [920] out of [1408] = [65.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 594 ms\n",
      "Tokens per second [84.2]\n",
      "Processing call [921] out of [1408] = [65.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 503 ms\n",
      "Tokens per second [81.5]\n",
      "Processing call [922] out of [1408] = [65.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 466 ms\n",
      "Tokens per second [79.4]\n",
      "Processing call [923] out of [1408] = [65.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 534 ms\n",
      "Tokens per second [82.4]\n",
      "Processing call [924] out of [1408] = [65.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 465 ms\n",
      "Tokens per second [79.6]\n",
      "Processing call [925] out of [1408] = [65.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 489 ms\n",
      "Tokens per second [79.8]\n",
      "Processing call [926] out of [1408] = [65.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 451 ms\n",
      "Tokens per second [77.6]\n",
      "Processing call [927] out of [1408] = [65.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 518 ms\n",
      "Tokens per second [81.1]\n",
      "Processing call [928] out of [1408] = [65.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 464 ms\n",
      "Tokens per second [79.7]\n",
      "Processing call [929] out of [1408] = [66.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 488 ms\n",
      "Tokens per second [79.9]\n",
      "Processing call [930] out of [1408] = [66.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 632 ms\n",
      "Tokens per second [85.4]\n",
      "Processing call [931] out of [1408] = [66.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 514 ms\n",
      "Tokens per second [81.7]\n",
      "Processing call [932] out of [1408] = [66.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 626 ms\n",
      "Tokens per second [84.7]\n",
      "Processing call [933] out of [1408] = [66.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 483 ms\n",
      "Tokens per second [80.7]\n",
      "Processing call [934] out of [1408] = [66.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 623 ms\n",
      "Tokens per second [85.1]\n",
      "Processing call [935] out of [1408] = [66.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 475 ms\n",
      "Tokens per second [80.0]\n",
      "Processing call [936] out of [1408] = [66.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 466 ms\n",
      "Tokens per second [79.4]\n",
      "Processing call [937] out of [1408] = [66.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 462 ms\n",
      "Tokens per second [80.1]\n",
      "Processing call [938] out of [1408] = [66.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 634 ms\n",
      "Tokens per second [85.2]\n",
      "Processing call [939] out of [1408] = [66.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 553 ms\n",
      "Tokens per second [83.2]\n",
      "Processing call [940] out of [1408] = [66.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 517 ms\n",
      "Tokens per second [81.2]\n",
      "Processing call [941] out of [1408] = [66.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..............................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 400 ms\n",
      "Tokens per second [75.0]\n",
      "Processing call [942] out of [1408] = [66.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 465 ms\n",
      "Tokens per second [79.6]\n",
      "Processing call [943] out of [1408] = [67.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 653 ms\n",
      "Tokens per second [85.8]\n",
      "Processing call [944] out of [1408] = [67.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 484 ms\n",
      "Tokens per second [80.6]\n",
      "Processing call [945] out of [1408] = [67.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 633 ms\n",
      "Tokens per second [85.3]\n",
      "Processing call [946] out of [1408] = [67.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 526 ms\n",
      "Tokens per second [81.7]\n",
      "Processing call [947] out of [1408] = [67.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 445 ms\n",
      "Tokens per second [78.7]\n",
      "Processing call [948] out of [1408] = [67.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 484 ms\n",
      "Tokens per second [80.6]\n",
      "Processing call [949] out of [1408] = [67.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 439 ms\n",
      "Tokens per second [77.4]\n",
      "Processing call [950] out of [1408] = [67.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 475 ms\n",
      "Tokens per second [80.0]\n",
      "Processing call [951] out of [1408] = [67.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 525 ms\n",
      "Tokens per second [81.9]\n",
      "Processing call [952] out of [1408] = [67.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 515 ms\n",
      "Tokens per second [81.6]\n",
      "Processing call [953] out of [1408] = [67.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 446 ms\n",
      "Tokens per second [78.5]\n",
      "Processing call [954] out of [1408] = [67.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 617 ms\n",
      "Tokens per second [84.3]\n",
      "Processing call [955] out of [1408] = [67.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 474 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [956] out of [1408] = [67.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 528 ms\n",
      "Tokens per second [81.4]\n",
      "Processing call [957] out of [1408] = [68.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 444 ms\n",
      "Tokens per second [78.8]\n",
      "Processing call [958] out of [1408] = [68.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 576 ms\n",
      "Tokens per second [83.3]\n",
      "Processing call [959] out of [1408] = [68.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 473 ms\n",
      "Tokens per second [80.3]\n",
      "Processing call [960] out of [1408] = [68.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 544 ms\n",
      "Tokens per second [82.7]\n",
      "Processing call [961] out of [1408] = [68.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 464 ms\n",
      "Tokens per second [79.7]\n",
      "Processing call [962] out of [1408] = [68.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 535 ms\n",
      "Tokens per second [82.2]\n",
      "Processing call [963] out of [1408] = [68.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 465 ms\n",
      "Tokens per second [79.6]\n",
      "Processing call [964] out of [1408] = [68.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 523 ms\n",
      "Tokens per second [82.2]\n",
      "Processing call [965] out of [1408] = [68.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 603 ms\n",
      "Tokens per second [84.6]\n",
      "Processing call [966] out of [1408] = [68.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 486 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [967] out of [1408] = [68.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 499 ms\n",
      "Tokens per second [80.0]\n",
      "Processing call [968] out of [1408] = [68.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 622 ms\n",
      "Tokens per second [85.2]\n",
      "Processing call [969] out of [1408] = [68.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 648 ms\n",
      "Tokens per second [84.9]\n",
      "Processing call [970] out of [1408] = [68.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 457 ms\n",
      "Tokens per second [78.8]\n",
      "Processing call [971] out of [1408] = [69.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 460 ms\n",
      "Tokens per second [78.3]\n",
      "Processing call [972] out of [1408] = [69.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 485 ms\n",
      "Tokens per second [80.4]\n",
      "Processing call [973] out of [1408] = [69.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 517 ms\n",
      "Tokens per second [81.2]\n",
      "Processing call [974] out of [1408] = [69.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 509 ms\n",
      "Tokens per second [80.6]\n",
      "Processing call [975] out of [1408] = [69.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 473 ms\n",
      "Tokens per second [80.3]\n",
      "Processing call [976] out of [1408] = [69.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 459 ms\n",
      "Tokens per second [78.4]\n",
      "Processing call [977] out of [1408] = [69.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 541 ms\n",
      "Tokens per second [81.3]\n",
      "Processing call [978] out of [1408] = [69.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 468 ms\n",
      "Tokens per second [79.1]\n",
      "Processing call [979] out of [1408] = [69.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 573 ms\n",
      "Tokens per second [82.0]\n",
      "Processing call [980] out of [1408] = [69.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 533 ms\n",
      "Tokens per second [80.7]\n",
      "Processing call [981] out of [1408] = [69.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 644 ms\n",
      "Tokens per second [83.9]\n",
      "Processing call [982] out of [1408] = [69.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 552 ms\n",
      "Tokens per second [81.5]\n",
      "Processing call [983] out of [1408] = [69.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 533 ms\n",
      "Tokens per second [80.7]\n",
      "Processing call [984] out of [1408] = [69.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 502 ms\n",
      "Tokens per second [79.7]\n",
      "Processing call [985] out of [1408] = [70.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 472 ms\n",
      "Tokens per second [78.4]\n",
      "Processing call [986] out of [1408] = [70.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 504 ms\n",
      "Tokens per second [79.4]\n",
      "Processing call [987] out of [1408] = [70.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 480 ms\n",
      "Tokens per second [79.2]\n",
      "Processing call [988] out of [1408] = [70.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 591 ms\n",
      "Tokens per second [82.8]\n",
      "Processing call [989] out of [1408] = [70.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 529 ms\n",
      "Tokens per second [81.3]\n",
      "Processing call [990] out of [1408] = [70.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 535 ms\n",
      "Tokens per second [82.2]\n",
      "Processing call [991] out of [1408] = [70.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 461 ms\n",
      "Tokens per second [78.1]\n",
      "Processing call [992] out of [1408] = [70.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 505 ms\n",
      "Tokens per second [81.2]\n",
      "Processing call [993] out of [1408] = [70.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 492 ms\n",
      "Tokens per second [81.3]\n",
      "Processing call [994] out of [1408] = [70.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 512 ms\n",
      "Tokens per second [82.0]\n",
      "Processing call [995] out of [1408] = [70.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 464 ms\n",
      "Tokens per second [79.7]\n",
      "Processing call [996] out of [1408] = [70.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 507 ms\n",
      "Tokens per second [75.0]\n",
      "Processing call [997] out of [1408] = [70.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 603 ms\n",
      "Tokens per second [82.9]\n",
      "Processing call [998] out of [1408] = [70.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 526 ms\n",
      "Tokens per second [79.8]\n",
      "Processing call [999] out of [1408] = [71.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 468 ms\n",
      "Tokens per second [79.1]\n",
      "Processing call [1000] out of [1408] = [71.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 507 ms\n",
      "Tokens per second [80.9]\n",
      "Processing call [1001] out of [1408] = [71.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 505 ms\n",
      "Tokens per second [81.2]\n",
      "Processing call [1002] out of [1408] = [71.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 444 ms\n",
      "Tokens per second [78.8]\n",
      "Processing call [1003] out of [1408] = [71.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 446 ms\n",
      "Tokens per second [78.5]\n",
      "Processing call [1004] out of [1408] = [71.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 606 ms\n",
      "Tokens per second [84.2]\n",
      "Processing call [1005] out of [1408] = [71.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..............................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 402 ms\n",
      "Tokens per second [74.6]\n",
      "Processing call [1006] out of [1408] = [71.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 575 ms\n",
      "Tokens per second [81.7]\n",
      "Processing call [1007] out of [1408] = [71.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 469 ms\n",
      "Tokens per second [78.9]\n",
      "Processing call [1008] out of [1408] = [71.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 651 ms\n",
      "Tokens per second [86.0]\n",
      "Processing call [1009] out of [1408] = [71.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..............................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 398 ms\n",
      "Tokens per second [75.4]\n",
      "Processing call [1010] out of [1408] = [71.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 475 ms\n",
      "Tokens per second [80.0]\n",
      "Processing call [1011] out of [1408] = [71.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 478 ms\n",
      "Tokens per second [79.5]\n",
      "Processing call [1012] out of [1408] = [71.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 450 ms\n",
      "Tokens per second [77.8]\n",
      "Processing call [1013] out of [1408] = [71.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 526 ms\n",
      "Tokens per second [81.7]\n",
      "Processing call [1014] out of [1408] = [72.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 617 ms\n",
      "Tokens per second [84.3]\n",
      "Processing call [1015] out of [1408] = [72.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 477 ms\n",
      "Tokens per second [77.6]\n",
      "Processing call [1016] out of [1408] = [72.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 483 ms\n",
      "Tokens per second [78.7]\n",
      "Processing call [1017] out of [1408] = [72.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 539 ms\n",
      "Tokens per second [81.6]\n",
      "Processing call [1018] out of [1408] = [72.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 482 ms\n",
      "Tokens per second [78.8]\n",
      "Processing call [1019] out of [1408] = [72.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 522 ms\n",
      "Tokens per second [80.5]\n",
      "Processing call [1020] out of [1408] = [72.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 623 ms\n",
      "Tokens per second [83.3]\n",
      "Processing call [1021] out of [1408] = [72.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 478 ms\n",
      "Tokens per second [79.5]\n",
      "Processing call [1022] out of [1408] = [72.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 509 ms\n",
      "Tokens per second [80.6]\n",
      "Processing call [1023] out of [1408] = [72.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 481 ms\n",
      "Tokens per second [79.0]\n",
      "Processing call [1024] out of [1408] = [72.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 454 ms\n",
      "Tokens per second [77.1]\n",
      "Processing call [1025] out of [1408] = [72.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 539 ms\n",
      "Tokens per second [81.6]\n",
      "Processing call [1026] out of [1408] = [72.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 499 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [1027] out of [1408] = [72.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..............................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 403 ms\n",
      "Tokens per second [74.4]\n",
      "Processing call [1028] out of [1408] = [73.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 605 ms\n",
      "Tokens per second [84.3]\n",
      "Processing call [1029] out of [1408] = [73.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 519 ms\n",
      "Tokens per second [80.9]\n",
      "Processing call [1030] out of [1408] = [73.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 488 ms\n",
      "Tokens per second [79.9]\n",
      "Processing call [1031] out of [1408] = [73.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 469 ms\n",
      "Tokens per second [78.9]\n",
      "Processing call [1032] out of [1408] = [73.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 528 ms\n",
      "Tokens per second [81.4]\n",
      "Processing call [1033] out of [1408] = [73.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 501 ms\n",
      "Tokens per second [79.8]\n",
      "Processing call [1034] out of [1408] = [73.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 539 ms\n",
      "Tokens per second [81.6]\n",
      "Processing call [1035] out of [1408] = [73.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 519 ms\n",
      "Tokens per second [80.9]\n",
      "Processing call [1036] out of [1408] = [73.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 608 ms\n",
      "Tokens per second [83.9]\n",
      "Processing call [1037] out of [1408] = [73.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 469 ms\n",
      "Tokens per second [78.9]\n",
      "Processing call [1038] out of [1408] = [73.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 620 ms\n",
      "Tokens per second [83.9]\n",
      "Processing call [1039] out of [1408] = [73.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 511 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [1040] out of [1408] = [73.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 478 ms\n",
      "Tokens per second [79.5]\n",
      "Processing call [1041] out of [1408] = [73.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 518 ms\n",
      "Tokens per second [81.1]\n",
      "Processing call [1042] out of [1408] = [74.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 537 ms\n",
      "Tokens per second [81.9]\n",
      "Processing call [1043] out of [1408] = [74.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 458 ms\n",
      "Tokens per second [78.6]\n",
      "Processing call [1044] out of [1408] = [74.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 451 ms\n",
      "Tokens per second [77.6]\n",
      "Processing call [1045] out of [1408] = [74.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 482 ms\n",
      "Tokens per second [78.8]\n",
      "Processing call [1046] out of [1408] = [74.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 510 ms\n",
      "Tokens per second [80.4]\n",
      "Processing call [1047] out of [1408] = [74.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 668 ms\n",
      "Tokens per second [85.3]\n",
      "Processing call [1048] out of [1408] = [74.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 471 ms\n",
      "Tokens per second [78.6]\n",
      "Processing call [1049] out of [1408] = [74.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 468 ms\n",
      "Tokens per second [79.1]\n",
      "Processing call [1050] out of [1408] = [74.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 607 ms\n",
      "Tokens per second [84.0]\n",
      "Processing call [1051] out of [1408] = [74.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 474 ms\n",
      "Tokens per second [78.1]\n",
      "Processing call [1052] out of [1408] = [74.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 560 ms\n",
      "Tokens per second [82.1]\n",
      "Processing call [1053] out of [1408] = [74.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 449 ms\n",
      "Tokens per second [78.0]\n",
      "Processing call [1054] out of [1408] = [74.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 500 ms\n",
      "Tokens per second [80.0]\n",
      "Processing call [1055] out of [1408] = [74.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 419 ms\n",
      "Tokens per second [76.4]\n",
      "Processing call [1056] out of [1408] = [75.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 497 ms\n",
      "Tokens per second [80.5]\n",
      "Processing call [1057] out of [1408] = [75.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 457 ms\n",
      "Tokens per second [78.6]\n",
      "Processing call [1058] out of [1408] = [75.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 590 ms\n",
      "Tokens per second [83.1]\n",
      "Processing call [1059] out of [1408] = [75.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 585 ms\n",
      "Tokens per second [83.8]\n",
      "Processing call [1060] out of [1408] = [75.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 533 ms\n",
      "Tokens per second [82.6]\n",
      "Processing call [1061] out of [1408] = [75.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 466 ms\n",
      "Tokens per second [79.4]\n",
      "Processing call [1062] out of [1408] = [75.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 505 ms\n",
      "Tokens per second [81.2]\n",
      "Processing call [1063] out of [1408] = [75.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 620 ms\n",
      "Tokens per second [85.5]\n",
      "Processing call [1064] out of [1408] = [75.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 647 ms\n",
      "Tokens per second [81.9]\n",
      "Processing call [1065] out of [1408] = [75.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 446 ms\n",
      "Tokens per second [78.5]\n",
      "Processing call [1066] out of [1408] = [75.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 510 ms\n",
      "Tokens per second [82.4]\n",
      "Processing call [1067] out of [1408] = [75.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 503 ms\n",
      "Tokens per second [79.5]\n",
      "Processing call [1068] out of [1408] = [75.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 492 ms\n",
      "Tokens per second [81.3]\n",
      "Processing call [1069] out of [1408] = [75.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 538 ms\n",
      "Tokens per second [81.8]\n",
      "Processing call [1070] out of [1408] = [76.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 514 ms\n",
      "Tokens per second [81.7]\n",
      "Processing call [1071] out of [1408] = [76.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 610 ms\n",
      "Tokens per second [85.2]\n",
      "Processing call [1072] out of [1408] = [76.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 523 ms\n",
      "Tokens per second [82.2]\n",
      "Processing call [1073] out of [1408] = [76.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 474 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [1074] out of [1408] = [76.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 464 ms\n",
      "Tokens per second [79.7]\n",
      "Processing call [1075] out of [1408] = [76.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 513 ms\n",
      "Tokens per second [81.9]\n",
      "Processing call [1076] out of [1408] = [76.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 552 ms\n",
      "Tokens per second [83.3]\n",
      "Processing call [1077] out of [1408] = [76.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 481 ms\n",
      "Tokens per second [79.0]\n",
      "Processing call [1078] out of [1408] = [76.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 498 ms\n",
      "Tokens per second [78.3]\n",
      "Processing call [1079] out of [1408] = [76.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 465 ms\n",
      "Tokens per second [79.6]\n",
      "Processing call [1080] out of [1408] = [76.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..............................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 397 ms\n",
      "Tokens per second [75.6]\n",
      "Processing call [1081] out of [1408] = [76.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 454 ms\n",
      "Tokens per second [79.3]\n",
      "Processing call [1082] out of [1408] = [76.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 544 ms\n",
      "Tokens per second [82.7]\n",
      "Processing call [1083] out of [1408] = [76.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 525 ms\n",
      "Tokens per second [81.9]\n",
      "Processing call [1084] out of [1408] = [77.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 503 ms\n",
      "Tokens per second [81.5]\n",
      "Processing call [1085] out of [1408] = [77.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 492 ms\n",
      "Tokens per second [81.3]\n",
      "Processing call [1086] out of [1408] = [77.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 458 ms\n",
      "Tokens per second [78.6]\n",
      "Processing call [1087] out of [1408] = [77.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 539 ms\n",
      "Tokens per second [81.6]\n",
      "Processing call [1088] out of [1408] = [77.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 478 ms\n",
      "Tokens per second [79.5]\n",
      "Processing call [1089] out of [1408] = [77.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 534 ms\n",
      "Tokens per second [82.4]\n",
      "Processing call [1090] out of [1408] = [77.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..............................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 398 ms\n",
      "Tokens per second [75.4]\n",
      "Processing call [1091] out of [1408] = [77.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 551 ms\n",
      "Tokens per second [83.5]\n",
      "Processing call [1092] out of [1408] = [77.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 535 ms\n",
      "Tokens per second [82.1]\n",
      "Processing call [1093] out of [1408] = [77.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 484 ms\n",
      "Tokens per second [80.6]\n",
      "Processing call [1094] out of [1408] = [77.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 506 ms\n",
      "Tokens per second [81.0]\n",
      "Processing call [1095] out of [1408] = [77.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 477 ms\n",
      "Tokens per second [77.6]\n",
      "Processing call [1096] out of [1408] = [77.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 625 ms\n",
      "Tokens per second [83.2]\n",
      "Processing call [1097] out of [1408] = [77.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 543 ms\n",
      "Tokens per second [81.0]\n",
      "Processing call [1098] out of [1408] = [78.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 624 ms\n",
      "Tokens per second [83.3]\n",
      "Processing call [1099] out of [1408] = [78.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 499 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [1100] out of [1408] = [78.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 454 ms\n",
      "Tokens per second [77.1]\n",
      "Processing call [1101] out of [1408] = [78.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 482 ms\n",
      "Tokens per second [78.8]\n",
      "Processing call [1102] out of [1408] = [78.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 483 ms\n",
      "Tokens per second [78.7]\n",
      "Processing call [1103] out of [1408] = [78.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 470 ms\n",
      "Tokens per second [78.7]\n",
      "Processing call [1104] out of [1408] = [78.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 531 ms\n",
      "Tokens per second [81.0]\n",
      "Processing call [1105] out of [1408] = [78.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 604 ms\n",
      "Tokens per second [82.8]\n",
      "Processing call [1106] out of [1408] = [78.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 535 ms\n",
      "Tokens per second [80.4]\n",
      "Processing call [1107] out of [1408] = [78.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 532 ms\n",
      "Tokens per second [80.8]\n",
      "Processing call [1108] out of [1408] = [78.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 503 ms\n",
      "Tokens per second [79.5]\n",
      "Processing call [1109] out of [1408] = [78.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 532 ms\n",
      "Tokens per second [80.8]\n",
      "Processing call [1110] out of [1408] = [78.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 484 ms\n",
      "Tokens per second [78.5]\n",
      "Processing call [1111] out of [1408] = [78.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 483 ms\n",
      "Tokens per second [78.7]\n",
      "Processing call [1112] out of [1408] = [79.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 476 ms\n",
      "Tokens per second [77.7]\n",
      "Processing call [1113] out of [1408] = [79.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 474 ms\n",
      "Tokens per second [78.1]\n",
      "Processing call [1114] out of [1408] = [79.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 491 ms\n",
      "Tokens per second [79.4]\n",
      "Processing call [1115] out of [1408] = [79.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 620 ms\n",
      "Tokens per second [83.9]\n",
      "Processing call [1116] out of [1408] = [79.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 473 ms\n",
      "Tokens per second [78.2]\n",
      "Processing call [1117] out of [1408] = [79.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 620 ms\n",
      "Tokens per second [83.9]\n",
      "Processing call [1118] out of [1408] = [79.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 474 ms\n",
      "Tokens per second [78.1]\n",
      "Processing call [1119] out of [1408] = [79.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 539 ms\n",
      "Tokens per second [81.6]\n",
      "Processing call [1120] out of [1408] = [79.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 533 ms\n",
      "Tokens per second [80.7]\n",
      "Processing call [1121] out of [1408] = [79.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 500 ms\n",
      "Tokens per second [80.0]\n",
      "Processing call [1122] out of [1408] = [79.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 643 ms\n",
      "Tokens per second [84.0]\n",
      "Processing call [1123] out of [1408] = [79.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 502 ms\n",
      "Tokens per second [79.7]\n",
      "Processing call [1124] out of [1408] = [79.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 603 ms\n",
      "Tokens per second [82.9]\n",
      "Processing call [1125] out of [1408] = [79.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 523 ms\n",
      "Tokens per second [80.3]\n",
      "Processing call [1126] out of [1408] = [80.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 493 ms\n",
      "Tokens per second [79.1]\n",
      "Processing call [1127] out of [1408] = [80.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 473 ms\n",
      "Tokens per second [78.2]\n",
      "Processing call [1128] out of [1408] = [80.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 475 ms\n",
      "Tokens per second [77.9]\n",
      "Processing call [1129] out of [1408] = [80.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 542 ms\n",
      "Tokens per second [81.2]\n",
      "Processing call [1130] out of [1408] = [80.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 494 ms\n",
      "Tokens per second [78.9]\n",
      "Processing call [1131] out of [1408] = [80.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 469 ms\n",
      "Tokens per second [78.9]\n",
      "Processing call [1132] out of [1408] = [80.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 522 ms\n",
      "Tokens per second [80.5]\n",
      "Processing call [1133] out of [1408] = [80.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 633 ms\n",
      "Tokens per second [83.6]\n",
      "Processing call [1134] out of [1408] = [80.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 501 ms\n",
      "Tokens per second [79.8]\n",
      "Processing call [1135] out of [1408] = [80.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 533 ms\n",
      "Tokens per second [80.7]\n",
      "Processing call [1136] out of [1408] = [80.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 523 ms\n",
      "Tokens per second [80.3]\n",
      "Processing call [1137] out of [1408] = [80.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 646 ms\n",
      "Tokens per second [83.6]\n",
      "Processing call [1138] out of [1408] = [80.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 543 ms\n",
      "Tokens per second [81.0]\n",
      "Processing call [1139] out of [1408] = [80.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 510 ms\n",
      "Tokens per second [80.4]\n",
      "Processing call [1140] out of [1408] = [81.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 459 ms\n",
      "Tokens per second [78.4]\n",
      "Processing call [1141] out of [1408] = [81.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 532 ms\n",
      "Tokens per second [80.8]\n",
      "Processing call [1142] out of [1408] = [81.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 612 ms\n",
      "Tokens per second [83.3]\n",
      "Processing call [1143] out of [1408] = [81.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 477 ms\n",
      "Tokens per second [79.7]\n",
      "Processing call [1144] out of [1408] = [81.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 461 ms\n",
      "Tokens per second [78.1]\n",
      "Processing call [1145] out of [1408] = [81.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 454 ms\n",
      "Tokens per second [77.1]\n",
      "Processing call [1146] out of [1408] = [81.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 632 ms\n",
      "Tokens per second [83.9]\n",
      "Processing call [1147] out of [1408] = [81.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 471 ms\n",
      "Tokens per second [78.6]\n",
      "Processing call [1148] out of [1408] = [81.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..............................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 401 ms\n",
      "Tokens per second [74.8]\n",
      "Processing call [1149] out of [1408] = [81.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 521 ms\n",
      "Tokens per second [80.6]\n",
      "Processing call [1150] out of [1408] = [81.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 477 ms\n",
      "Tokens per second [79.7]\n",
      "Processing call [1151] out of [1408] = [81.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 503 ms\n",
      "Tokens per second [79.5]\n",
      "Processing call [1152] out of [1408] = [81.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 482 ms\n",
      "Tokens per second [78.8]\n",
      "Processing call [1153] out of [1408] = [81.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 624 ms\n",
      "Tokens per second [83.3]\n",
      "Processing call [1154] out of [1408] = [82.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 524 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [1155] out of [1408] = [82.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 625 ms\n",
      "Tokens per second [83.2]\n",
      "Processing call [1156] out of [1408] = [82.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 525 ms\n",
      "Tokens per second [80.0]\n",
      "Processing call [1157] out of [1408] = [82.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 475 ms\n",
      "Tokens per second [77.9]\n",
      "Processing call [1158] out of [1408] = [82.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..............................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 405 ms\n",
      "Tokens per second [74.1]\n",
      "Processing call [1159] out of [1408] = [82.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 470 ms\n",
      "Tokens per second [78.7]\n",
      "Processing call [1160] out of [1408] = [82.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 683 ms\n",
      "Tokens per second [84.9]\n",
      "Processing call [1161] out of [1408] = [82.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 551 ms\n",
      "Tokens per second [81.7]\n",
      "Processing call [1162] out of [1408] = [82.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 471 ms\n",
      "Tokens per second [78.6]\n",
      "Processing call [1163] out of [1408] = [82.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 492 ms\n",
      "Tokens per second [79.3]\n",
      "Processing call [1164] out of [1408] = [82.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 545 ms\n",
      "Tokens per second [80.7]\n",
      "Processing call [1165] out of [1408] = [82.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 471 ms\n",
      "Tokens per second [78.6]\n",
      "Processing call [1166] out of [1408] = [82.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 492 ms\n",
      "Tokens per second [79.3]\n",
      "Processing call [1167] out of [1408] = [82.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 631 ms\n",
      "Tokens per second [84.0]\n",
      "Processing call [1168] out of [1408] = [83.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 513 ms\n",
      "Tokens per second [79.9]\n",
      "Processing call [1169] out of [1408] = [83.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 521 ms\n",
      "Tokens per second [80.6]\n",
      "Processing call [1170] out of [1408] = [83.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 552 ms\n",
      "Tokens per second [81.5]\n",
      "Processing call [1171] out of [1408] = [83.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 596 ms\n",
      "Tokens per second [82.2]\n",
      "Processing call [1172] out of [1408] = [83.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 525 ms\n",
      "Tokens per second [80.0]\n",
      "Processing call [1173] out of [1408] = [83.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 533 ms\n",
      "Tokens per second [80.7]\n",
      "Processing call [1174] out of [1408] = [83.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 685 ms\n",
      "Tokens per second [84.7]\n",
      "Processing call [1175] out of [1408] = [83.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 522 ms\n",
      "Tokens per second [80.5]\n",
      "Processing call [1176] out of [1408] = [83.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 464 ms\n",
      "Tokens per second [77.6]\n",
      "Processing call [1177] out of [1408] = [83.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 614 ms\n",
      "Tokens per second [83.1]\n",
      "Processing call [1178] out of [1408] = [83.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 662 ms\n",
      "Tokens per second [84.6]\n",
      "Processing call [1179] out of [1408] = [83.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 664 ms\n",
      "Tokens per second [84.3]\n",
      "Processing call [1180] out of [1408] = [83.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..............................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 404 ms\n",
      "Tokens per second [74.3]\n",
      "Processing call [1181] out of [1408] = [83.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 632 ms\n",
      "Tokens per second [83.9]\n",
      "Processing call [1182] out of [1408] = [83.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 506 ms\n",
      "Tokens per second [79.1]\n",
      "Processing call [1183] out of [1408] = [84.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 557 ms\n",
      "Tokens per second [80.8]\n",
      "Processing call [1184] out of [1408] = [84.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 474 ms\n",
      "Tokens per second [78.1]\n",
      "Processing call [1185] out of [1408] = [84.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 495 ms\n",
      "Tokens per second [78.8]\n",
      "Processing call [1186] out of [1408] = [84.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 571 ms\n",
      "Tokens per second [82.3]\n",
      "Processing call [1187] out of [1408] = [84.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 535 ms\n",
      "Tokens per second [80.4]\n",
      "Processing call [1188] out of [1408] = [84.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 613 ms\n",
      "Tokens per second [83.2]\n",
      "Processing call [1189] out of [1408] = [84.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 475 ms\n",
      "Tokens per second [77.9]\n",
      "Processing call [1190] out of [1408] = [84.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 523 ms\n",
      "Tokens per second [80.3]\n",
      "Processing call [1191] out of [1408] = [84.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 524 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [1192] out of [1408] = [84.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 542 ms\n",
      "Tokens per second [81.2]\n",
      "Processing call [1193] out of [1408] = [84.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 506 ms\n",
      "Tokens per second [79.1]\n",
      "Processing call [1194] out of [1408] = [84.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 543 ms\n",
      "Tokens per second [81.0]\n",
      "Processing call [1195] out of [1408] = [84.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 536 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [1196] out of [1408] = [84.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 543 ms\n",
      "Tokens per second [81.0]\n",
      "Processing call [1197] out of [1408] = [85.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 527 ms\n",
      "Tokens per second [79.7]\n",
      "Processing call [1198] out of [1408] = [85.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 475 ms\n",
      "Tokens per second [77.9]\n",
      "Processing call [1199] out of [1408] = [85.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 463 ms\n",
      "Tokens per second [77.8]\n",
      "Processing call [1200] out of [1408] = [85.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 524 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [1201] out of [1408] = [85.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 483 ms\n",
      "Tokens per second [78.7]\n",
      "Processing call [1202] out of [1408] = [85.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 475 ms\n",
      "Tokens per second [77.9]\n",
      "Processing call [1203] out of [1408] = [85.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..............................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 402 ms\n",
      "Tokens per second [74.6]\n",
      "Processing call [1204] out of [1408] = [85.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 495 ms\n",
      "Tokens per second [78.8]\n",
      "Processing call [1205] out of [1408] = [85.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 515 ms\n",
      "Tokens per second [79.6]\n",
      "Processing call [1206] out of [1408] = [85.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 450 ms\n",
      "Tokens per second [77.8]\n",
      "Processing call [1207] out of [1408] = [85.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 492 ms\n",
      "Tokens per second [79.1]\n",
      "Processing call [1208] out of [1408] = [85.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 544 ms\n",
      "Tokens per second [80.9]\n",
      "Processing call [1209] out of [1408] = [85.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 454 ms\n",
      "Tokens per second [77.1]\n",
      "Processing call [1210] out of [1408] = [85.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 473 ms\n",
      "Tokens per second [78.2]\n",
      "Processing call [1211] out of [1408] = [86.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 527 ms\n",
      "Tokens per second [79.7]\n",
      "Processing call [1212] out of [1408] = [86.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 487 ms\n",
      "Tokens per second [78.0]\n",
      "Processing call [1213] out of [1408] = [86.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 453 ms\n",
      "Tokens per second [77.3]\n",
      "Processing call [1214] out of [1408] = [86.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 486 ms\n",
      "Tokens per second [78.2]\n",
      "Processing call [1215] out of [1408] = [86.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 634 ms\n",
      "Tokens per second [83.6]\n",
      "Processing call [1216] out of [1408] = [86.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 482 ms\n",
      "Tokens per second [78.8]\n",
      "Processing call [1217] out of [1408] = [86.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 523 ms\n",
      "Tokens per second [80.3]\n",
      "Processing call [1218] out of [1408] = [86.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 517 ms\n",
      "Tokens per second [79.3]\n",
      "Processing call [1219] out of [1408] = [86.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 543 ms\n",
      "Tokens per second [81.0]\n",
      "Processing call [1220] out of [1408] = [86.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 512 ms\n",
      "Tokens per second [80.1]\n",
      "Processing call [1221] out of [1408] = [86.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 473 ms\n",
      "Tokens per second [78.2]\n",
      "Processing call [1222] out of [1408] = [86.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 491 ms\n",
      "Tokens per second [79.4]\n",
      "Processing call [1223] out of [1408] = [86.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 613 ms\n",
      "Tokens per second [83.2]\n",
      "Processing call [1224] out of [1408] = [86.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 499 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [1225] out of [1408] = [87.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 552 ms\n",
      "Tokens per second [81.5]\n",
      "Processing call [1226] out of [1408] = [87.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 554 ms\n",
      "Tokens per second [81.2]\n",
      "Processing call [1227] out of [1408] = [87.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 503 ms\n",
      "Tokens per second [79.5]\n",
      "Processing call [1228] out of [1408] = [87.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 511 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [1229] out of [1408] = [87.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 454 ms\n",
      "Tokens per second [77.1]\n",
      "Processing call [1230] out of [1408] = [87.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 631 ms\n",
      "Tokens per second [84.0]\n",
      "Processing call [1231] out of [1408] = [87.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 531 ms\n",
      "Tokens per second [81.0]\n",
      "Processing call [1232] out of [1408] = [87.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 580 ms\n",
      "Tokens per second [82.8]\n",
      "Processing call [1233] out of [1408] = [87.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 471 ms\n",
      "Tokens per second [78.6]\n",
      "Processing call [1234] out of [1408] = [87.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 502 ms\n",
      "Tokens per second [79.7]\n",
      "Processing call [1235] out of [1408] = [87.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 544 ms\n",
      "Tokens per second [80.9]\n",
      "Processing call [1236] out of [1408] = [87.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 511 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [1237] out of [1408] = [87.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 598 ms\n",
      "Tokens per second [83.6]\n",
      "Processing call [1238] out of [1408] = [87.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 504 ms\n",
      "Tokens per second [79.4]\n",
      "Processing call [1239] out of [1408] = [88.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 492 ms\n",
      "Tokens per second [79.3]\n",
      "Processing call [1240] out of [1408] = [88.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 493 ms\n",
      "Tokens per second [79.1]\n",
      "Processing call [1241] out of [1408] = [88.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 468 ms\n",
      "Tokens per second [79.1]\n",
      "Processing call [1242] out of [1408] = [88.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 475 ms\n",
      "Tokens per second [77.9]\n",
      "Processing call [1243] out of [1408] = [88.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 463 ms\n",
      "Tokens per second [77.8]\n",
      "Processing call [1244] out of [1408] = [88.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 476 ms\n",
      "Tokens per second [77.7]\n",
      "Processing call [1245] out of [1408] = [88.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 457 ms\n",
      "Tokens per second [76.6]\n",
      "Processing call [1246] out of [1408] = [88.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 461 ms\n",
      "Tokens per second [78.1]\n",
      "Processing call [1247] out of [1408] = [88.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 516 ms\n",
      "Tokens per second [79.5]\n",
      "Processing call [1248] out of [1408] = [88.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 601 ms\n",
      "Tokens per second [83.2]\n",
      "Processing call [1249] out of [1408] = [88.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 457 ms\n",
      "Tokens per second [76.6]\n",
      "Processing call [1250] out of [1408] = [88.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 486 ms\n",
      "Tokens per second [78.2]\n",
      "Processing call [1251] out of [1408] = [88.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 505 ms\n",
      "Tokens per second [79.2]\n",
      "Processing call [1252] out of [1408] = [88.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 483 ms\n",
      "Tokens per second [78.7]\n",
      "Processing call [1253] out of [1408] = [89.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 516 ms\n",
      "Tokens per second [79.5]\n",
      "Processing call [1254] out of [1408] = [89.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 465 ms\n",
      "Tokens per second [77.4]\n",
      "Processing call [1255] out of [1408] = [89.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 533 ms\n",
      "Tokens per second [80.7]\n",
      "Processing call [1256] out of [1408] = [89.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 513 ms\n",
      "Tokens per second [79.9]\n",
      "Processing call [1257] out of [1408] = [89.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 633 ms\n",
      "Tokens per second [83.7]\n",
      "Processing call [1258] out of [1408] = [89.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 464 ms\n",
      "Tokens per second [77.6]\n",
      "Processing call [1259] out of [1408] = [89.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 520 ms\n",
      "Tokens per second [80.8]\n",
      "Processing call [1260] out of [1408] = [89.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 544 ms\n",
      "Tokens per second [80.9]\n",
      "Processing call [1261] out of [1408] = [89.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 483 ms\n",
      "Tokens per second [78.7]\n",
      "Processing call [1262] out of [1408] = [89.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 621 ms\n",
      "Tokens per second [83.7]\n",
      "Processing call [1263] out of [1408] = [89.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 482 ms\n",
      "Tokens per second [78.8]\n",
      "Processing call [1264] out of [1408] = [89.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 502 ms\n",
      "Tokens per second [79.7]\n",
      "Processing call [1265] out of [1408] = [89.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 494 ms\n",
      "Tokens per second [78.9]\n",
      "Processing call [1266] out of [1408] = [89.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 482 ms\n",
      "Tokens per second [78.8]\n",
      "Processing call [1267] out of [1408] = [90.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 510 ms\n",
      "Tokens per second [80.4]\n",
      "Processing call [1268] out of [1408] = [90.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 474 ms\n",
      "Tokens per second [78.1]\n",
      "Processing call [1269] out of [1408] = [90.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 535 ms\n",
      "Tokens per second [80.4]\n",
      "Processing call [1270] out of [1408] = [90.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 544 ms\n",
      "Tokens per second [80.9]\n",
      "Processing call [1271] out of [1408] = [90.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 576 ms\n",
      "Tokens per second [81.6]\n",
      "Processing call [1272] out of [1408] = [90.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 515 ms\n",
      "Tokens per second [79.6]\n",
      "Processing call [1273] out of [1408] = [90.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 542 ms\n",
      "Tokens per second [81.2]\n",
      "Processing call [1274] out of [1408] = [90.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 479 ms\n",
      "Tokens per second [77.2]\n",
      "Processing call [1275] out of [1408] = [90.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 623 ms\n",
      "Tokens per second [83.5]\n",
      "Processing call [1276] out of [1408] = [90.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 473 ms\n",
      "Tokens per second [78.2]\n",
      "Processing call [1277] out of [1408] = [90.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 696 ms\n",
      "Tokens per second [84.8]\n",
      "Processing call [1278] out of [1408] = [90.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 466 ms\n",
      "Tokens per second [77.3]\n",
      "Processing call [1279] out of [1408] = [90.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 664 ms\n",
      "Tokens per second [84.3]\n",
      "Processing call [1280] out of [1408] = [90.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 512 ms\n",
      "Tokens per second [80.1]\n",
      "Processing call [1281] out of [1408] = [91.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 494 ms\n",
      "Tokens per second [78.9]\n",
      "Processing call [1282] out of [1408] = [91.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 498 ms\n",
      "Tokens per second [78.3]\n",
      "Processing call [1283] out of [1408] = [91.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 503 ms\n",
      "Tokens per second [79.5]\n",
      "Processing call [1284] out of [1408] = [91.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 457 ms\n",
      "Tokens per second [76.6]\n",
      "Processing call [1285] out of [1408] = [91.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 476 ms\n",
      "Tokens per second [77.7]\n",
      "Processing call [1286] out of [1408] = [91.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 523 ms\n",
      "Tokens per second [80.3]\n",
      "Processing call [1287] out of [1408] = [91.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 483 ms\n",
      "Tokens per second [78.7]\n",
      "Processing call [1288] out of [1408] = [91.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 465 ms\n",
      "Tokens per second [77.4]\n",
      "Processing call [1289] out of [1408] = [91.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 484 ms\n",
      "Tokens per second [78.5]\n",
      "Processing call [1290] out of [1408] = [91.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 583 ms\n",
      "Tokens per second [82.3]\n",
      "Processing call [1291] out of [1408] = [91.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 545 ms\n",
      "Tokens per second [80.7]\n",
      "Processing call [1292] out of [1408] = [91.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 642 ms\n",
      "Tokens per second [84.1]\n",
      "Processing call [1293] out of [1408] = [91.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 453 ms\n",
      "Tokens per second [79.5]\n",
      "Processing call [1294] out of [1408] = [91.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 528 ms\n",
      "Tokens per second [81.4]\n",
      "Processing call [1295] out of [1408] = [92.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 536 ms\n",
      "Tokens per second [82.1]\n",
      "Processing call [1296] out of [1408] = [92.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 459 ms\n",
      "Tokens per second [78.4]\n",
      "Processing call [1297] out of [1408] = [92.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 674 ms\n",
      "Tokens per second [86.1]\n",
      "Processing call [1298] out of [1408] = [92.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 494 ms\n",
      "Tokens per second [81.0]\n",
      "Processing call [1299] out of [1408] = [92.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 507 ms\n",
      "Tokens per second [80.9]\n",
      "Processing call [1300] out of [1408] = [92.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 473 ms\n",
      "Tokens per second [80.3]\n",
      "Processing call [1301] out of [1408] = [92.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 520 ms\n",
      "Tokens per second [80.8]\n",
      "Processing call [1302] out of [1408] = [92.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 643 ms\n",
      "Tokens per second [85.5]\n",
      "Processing call [1303] out of [1408] = [92.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 614 ms\n",
      "Tokens per second [84.7]\n",
      "Processing call [1304] out of [1408] = [92.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 481 ms\n",
      "Tokens per second [79.0]\n",
      "Processing call [1305] out of [1408] = [92.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 519 ms\n",
      "Tokens per second [80.9]\n",
      "Processing call [1306] out of [1408] = [92.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 468 ms\n",
      "Tokens per second [79.1]\n",
      "Processing call [1307] out of [1408] = [92.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 538 ms\n",
      "Tokens per second [81.8]\n",
      "Processing call [1308] out of [1408] = [92.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 458 ms\n",
      "Tokens per second [78.6]\n",
      "Processing call [1309] out of [1408] = [93.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 485 ms\n",
      "Tokens per second [80.4]\n",
      "Processing call [1310] out of [1408] = [93.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 466 ms\n",
      "Tokens per second [79.4]\n",
      "Processing call [1311] out of [1408] = [93.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 629 ms\n",
      "Tokens per second [85.9]\n",
      "Processing call [1312] out of [1408] = [93.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 511 ms\n",
      "Tokens per second [82.2]\n",
      "Processing call [1313] out of [1408] = [93.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 611 ms\n",
      "Tokens per second [85.1]\n",
      "Processing call [1314] out of [1408] = [93.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 604 ms\n",
      "Tokens per second [84.4]\n",
      "Processing call [1315] out of [1408] = [93.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 465 ms\n",
      "Tokens per second [79.6]\n",
      "Processing call [1316] out of [1408] = [93.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 508 ms\n",
      "Tokens per second [80.7]\n",
      "Processing call [1317] out of [1408] = [93.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 450 ms\n",
      "Tokens per second [77.8]\n",
      "Processing call [1318] out of [1408] = [93.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 489 ms\n",
      "Tokens per second [79.8]\n",
      "Processing call [1319] out of [1408] = [93.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 447 ms\n",
      "Tokens per second [78.3]\n",
      "Processing call [1320] out of [1408] = [93.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 457 ms\n",
      "Tokens per second [78.8]\n",
      "Processing call [1321] out of [1408] = [93.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 528 ms\n",
      "Tokens per second [81.4]\n",
      "Processing call [1322] out of [1408] = [93.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 617 ms\n",
      "Tokens per second [84.3]\n",
      "Processing call [1323] out of [1408] = [94.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 543 ms\n",
      "Tokens per second [81.0]\n",
      "Processing call [1324] out of [1408] = [94.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 467 ms\n",
      "Tokens per second [79.2]\n",
      "Processing call [1325] out of [1408] = [94.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 499 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [1326] out of [1408] = [94.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 468 ms\n",
      "Tokens per second [79.1]\n",
      "Processing call [1327] out of [1408] = [94.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 597 ms\n",
      "Tokens per second [83.8]\n",
      "Processing call [1328] out of [1408] = [94.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 477 ms\n",
      "Tokens per second [79.7]\n",
      "Processing call [1329] out of [1408] = [94.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 466 ms\n",
      "Tokens per second [79.4]\n",
      "Processing call [1330] out of [1408] = [94.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 458 ms\n",
      "Tokens per second [78.6]\n",
      "Processing call [1331] out of [1408] = [94.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 486 ms\n",
      "Tokens per second [80.1]\n",
      "Processing call [1332] out of [1408] = [94.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 469 ms\n",
      "Tokens per second [78.9]\n",
      "Processing call [1333] out of [1408] = [94.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 567 ms\n",
      "Tokens per second [82.9]\n",
      "Processing call [1334] out of [1408] = [94.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 488 ms\n",
      "Tokens per second [77.9]\n",
      "Processing call [1335] out of [1408] = [94.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 474 ms\n",
      "Tokens per second [78.1]\n",
      "Processing call [1336] out of [1408] = [94.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 675 ms\n",
      "Tokens per second [84.4]\n",
      "Processing call [1337] out of [1408] = [95.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 536 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [1338] out of [1408] = [95.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 535 ms\n",
      "Tokens per second [80.4]\n",
      "Processing call [1339] out of [1408] = [95.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 472 ms\n",
      "Tokens per second [78.4]\n",
      "Processing call [1340] out of [1408] = [95.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 533 ms\n",
      "Tokens per second [80.5]\n",
      "Processing call [1341] out of [1408] = [95.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 598 ms\n",
      "Tokens per second [83.6]\n",
      "Processing call [1342] out of [1408] = [95.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 573 ms\n",
      "Tokens per second [82.0]\n",
      "Processing call [1343] out of [1408] = [95.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 541 ms\n",
      "Tokens per second [81.3]\n",
      "Processing call [1344] out of [1408] = [95.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 543 ms\n",
      "Tokens per second [81.0]\n",
      "Processing call [1345] out of [1408] = [95.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 531 ms\n",
      "Tokens per second [81.0]\n",
      "Processing call [1346] out of [1408] = [95.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 682 ms\n",
      "Tokens per second [85.0]\n",
      "Processing call [1347] out of [1408] = [95.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 555 ms\n",
      "Tokens per second [79.3]\n",
      "Processing call [1348] out of [1408] = [95.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 520 ms\n",
      "Tokens per second [80.8]\n",
      "Processing call [1349] out of [1408] = [95.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 616 ms\n",
      "Tokens per second [84.4]\n",
      "Processing call [1350] out of [1408] = [95.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 648 ms\n",
      "Tokens per second [84.9]\n",
      "Processing call [1351] out of [1408] = [96.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 527 ms\n",
      "Tokens per second [81.6]\n",
      "Processing call [1352] out of [1408] = [96.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 506 ms\n",
      "Tokens per second [81.0]\n",
      "Processing call [1353] out of [1408] = [96.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 661 ms\n",
      "Tokens per second [84.7]\n",
      "Processing call [1354] out of [1408] = [96.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 456 ms\n",
      "Tokens per second [76.8]\n",
      "Processing call [1355] out of [1408] = [96.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 512 ms\n",
      "Tokens per second [80.1]\n",
      "Processing call [1356] out of [1408] = [96.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 475 ms\n",
      "Tokens per second [80.0]\n",
      "Processing call [1357] out of [1408] = [96.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 476 ms\n",
      "Tokens per second [77.7]\n",
      "Processing call [1358] out of [1408] = [96.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".............................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 701 ms\n",
      "Tokens per second [87.0]\n",
      "Processing call [1359] out of [1408] = [96.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 490 ms\n",
      "Tokens per second [79.6]\n",
      "Processing call [1360] out of [1408] = [96.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..............................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 398 ms\n",
      "Tokens per second [75.4]\n",
      "Processing call [1361] out of [1408] = [96.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 646 ms\n",
      "Tokens per second [85.1]\n",
      "Processing call [1362] out of [1408] = [96.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 697 ms\n",
      "Tokens per second [86.1]\n",
      "Processing call [1363] out of [1408] = [96.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 488 ms\n",
      "Tokens per second [79.9]\n",
      "Processing call [1364] out of [1408] = [96.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 448 ms\n",
      "Tokens per second [78.1]\n",
      "Processing call [1365] out of [1408] = [96.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 500 ms\n",
      "Tokens per second [80.0]\n",
      "Processing call [1366] out of [1408] = [97.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 530 ms\n",
      "Tokens per second [81.1]\n",
      "Processing call [1367] out of [1408] = [97.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 522 ms\n",
      "Tokens per second [80.5]\n",
      "Processing call [1368] out of [1408] = [97.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 469 ms\n",
      "Tokens per second [78.9]\n",
      "Processing call [1369] out of [1408] = [97.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 531 ms\n",
      "Tokens per second [81.0]\n",
      "Processing call [1370] out of [1408] = [97.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 470 ms\n",
      "Tokens per second [78.7]\n",
      "Processing call [1371] out of [1408] = [97.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 492 ms\n",
      "Tokens per second [79.3]\n",
      "Processing call [1372] out of [1408] = [97.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 620 ms\n",
      "Tokens per second [83.9]\n",
      "Processing call [1373] out of [1408] = [97.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 601 ms\n",
      "Tokens per second [83.2]\n",
      "Processing call [1374] out of [1408] = [97.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 493 ms\n",
      "Tokens per second [79.1]\n",
      "Processing call [1375] out of [1408] = [97.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 538 ms\n",
      "Tokens per second [81.8]\n",
      "Processing call [1376] out of [1408] = [97.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 488 ms\n",
      "Tokens per second [79.9]\n",
      "Processing call [1377] out of [1408] = [97.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 486 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [1378] out of [1408] = [97.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 458 ms\n",
      "Tokens per second [78.6]\n",
      "Processing call [1379] out of [1408] = [97.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 483 ms\n",
      "Tokens per second [78.7]\n",
      "Processing call [1380] out of [1408] = [98.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 470 ms\n",
      "Tokens per second [78.7]\n",
      "Processing call [1381] out of [1408] = [98.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 467 ms\n",
      "Tokens per second [79.2]\n",
      "Processing call [1382] out of [1408] = [98.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 511 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [1383] out of [1408] = [98.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 491 ms\n",
      "Tokens per second [79.4]\n",
      "Processing call [1384] out of [1408] = [98.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 671 ms\n",
      "Tokens per second [84.9]\n",
      "Processing call [1385] out of [1408] = [98.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 484 ms\n",
      "Tokens per second [78.5]\n",
      "Processing call [1386] out of [1408] = [98.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 503 ms\n",
      "Tokens per second [79.5]\n",
      "Processing call [1387] out of [1408] = [98.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 461 ms\n",
      "Tokens per second [78.1]\n",
      "Processing call [1388] out of [1408] = [98.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 624 ms\n",
      "Tokens per second [84.9]\n",
      "Processing call [1389] out of [1408] = [98.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 564 ms\n",
      "Tokens per second [83.3]\n",
      "Processing call [1390] out of [1408] = [98.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 626 ms\n",
      "Tokens per second [84.7]\n",
      "Processing call [1391] out of [1408] = [98.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 615 ms\n",
      "Tokens per second [84.6]\n",
      "Processing call [1392] out of [1408] = [98.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 462 ms\n",
      "Tokens per second [77.9]\n",
      "Processing call [1393] out of [1408] = [98.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 632 ms\n",
      "Tokens per second [83.9]\n",
      "Processing call [1394] out of [1408] = [99.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 691 ms\n",
      "Tokens per second [85.4]\n",
      "Processing call [1395] out of [1408] = [99.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 529 ms\n",
      "Tokens per second [81.3]\n",
      "Processing call [1396] out of [1408] = [99.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 508 ms\n",
      "Tokens per second [80.7]\n",
      "Processing call [1397] out of [1408] = [99.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 522 ms\n",
      "Tokens per second [80.5]\n",
      "Processing call [1398] out of [1408] = [99.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 665 ms\n",
      "Tokens per second [85.7]\n",
      "Processing call [1399] out of [1408] = [99.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 633 ms\n",
      "Tokens per second [83.7]\n",
      "Processing call [1400] out of [1408] = [99.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 491 ms\n",
      "Tokens per second [79.4]\n",
      "Processing call [1401] out of [1408] = [99.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 520 ms\n",
      "Tokens per second [80.8]\n",
      "Processing call [1402] out of [1408] = [99.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 578 ms\n",
      "Tokens per second [81.3]\n",
      "Processing call [1403] out of [1408] = [99.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 525 ms\n",
      "Tokens per second [81.9]\n",
      "Processing call [1404] out of [1408] = [99.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 472 ms\n",
      "Tokens per second [78.4]\n",
      "Processing call [1405] out of [1408] = [99.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 643 ms\n",
      "Tokens per second [84.0]\n",
      "Processing call [1406] out of [1408] = [99.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 469 ms\n",
      "Tokens per second [78.9]\n",
      "Processing call [1407] out of [1408] = [99.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 521 ms\n",
      "Tokens per second [80.6]\n",
      "Processing call [1408] out of [1408] = [100.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 447 ms\n",
      "Tokens per second [78.3]\n",
      "\n",
      "Generating responses for 1,408 rows... Done! in 12:08\n",
      "[517.5] ms per item\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Validation Stats for `mistralai/Mistral-7B-Instruct-v0.2-AWQ`\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "               Is valid xml 100.0%\n",
      "          Contains response 100.0%\n",
      " Contains <browser-command> 100.0%\n",
      "            Contains <args> 100.0%\n",
      "          Response is exact 99.6%\n",
      "Response has correct values 99.6%\n",
      " Browser command is correct 99.6%\n",
      "            Args is correct 99.9%\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "train_df, test_df, validate_df = xml_ftp_generator.get_train_test_validate_split( all_qna_df, sample_size=all_qna_df.shape[ 0 ], test_size=0.2, test_validate_size=0.5 )\n",
    "\n",
    "xml_ftp_generator.write_ttv_split_to_jsonl( train_df, test_df, validate_df )\n",
    "\n",
    "# validation block\n",
    "# validate_df    = pd.read_json( xml_ftp_generator.path_prefix + \"/src/ephemera/prompts/data/voice-commands-xml-validate.jsonl\", lines=True ).sample( 100, random_state=42 )\n",
    "# timer          = Stopwatch( msg=f\"Validating {validate_df.shape[ 0 ]:,} responses...\", silent=False )\n",
    "\n",
    "model_name     = \"mistralai/Mistral-7B-Instruct-v0.2-AWQ\"\n",
    "validate_df    = xml_ftp_generator.generate_responses( validate_df, switch=\"tgi\", model_name=model_name )\n",
    "validate_df    = xml_ftp_generator.validate_responses( validate_df )\n",
    "#\n",
    "# # model_name     = \"gpt-3.5-turbo-1106\"\n",
    "# # validate_df    = xml_ftp_generator.generate_responses( validate_df, switch=\"openai\", model_name= )\n",
    "# # validate_df    = xml_ftp_generator.validate_responses( validate_df )\n",
    "#\n",
    "xml_ftp_generator.print_validation_stats( validate_df, title=f\"Validation Stats for `{model_name}`\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "29753bb141448e9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-29T18:55:30.132988Z",
     "start_time": "2024-01-29T18:55:29.847604Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Validation Stats for `mistralai/Mistral-7B-Instruct-v0.2-AWQ`\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "               Is valid xml 100.0%\n",
      "          Contains response 100.0%\n",
      " Contains <browser-command> 100.0%\n",
      "            Contains <args> 100.0%\n",
      "          Response is exact 99.6%\n",
      "Response has correct values 99.6%\n",
      " Browser command is correct 99.6%\n",
      "            Args is correct 99.9%\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Validation Stats for `mistralai/Mistral-7B-Instruct-v0.2-AWQ`: Accuracy per command\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "                                            command     mean  sum  count\n",
      "                                 search current tab   99.08%  108    109\n",
      "                              search google new tab   98.96%   95     96\n",
      "                               search phind new tab   97.78%   44     45\n",
      "                 search using clipboard current tab   96.77%   30     31\n",
      "           search phind using clipboard current tab   95.83%   23     24\n",
      "      search google scholar using clipboard new tab   94.12%   16     17\n",
      "                                  go to current tab  100.00%  121    121\n",
      "                      search perplexity current tab  100.00%   48     48\n",
      "               search phind using clipboard new tab  100.00%   20     20\n",
      "                           search phind current tab  100.00%   56     56\n",
      "          search perplexity using clipboard new tab  100.00%   22     22\n",
      "      search perplexity using clipboard current tab  100.00%   22     22\n",
      "                          search perplexity new tab  100.00%   69     69\n",
      "              search google using clipboard new tab  100.00%   14     14\n",
      "                                     search new tab  100.00%  122    122\n",
      "                                      go to new tab  100.00%  122    122\n",
      "          search google using clipboard current tab  100.00%   11     11\n",
      "  search google scholar using clipboard current tab  100.00%   26     26\n",
      "                      search google scholar new tab  100.00%  144    144\n",
      "                  search google scholar current tab  100.00%  144    144\n",
      "                          search google current tab  100.00%  107    107\n",
      "                                               none  100.00%   19     19\n",
      "                     search using clipboard new tab  100.00%   19     19\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "foo = xml_ftp_generator.print_validation_stats( validate_df, title=f\"Validation Stats for `{model_name}`\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b12fc8c32d542752",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-29T16:38:23.672721Z",
     "start_time": "2024-01-29T16:38:23.646259Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1408, 15)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4e49e406f2a946bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-29T16:39:38.681971Z",
     "start_time": "2024-01-29T16:39:38.569748Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_df.command.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f7d40d97d1d57999",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-29T16:39:06.581622Z",
     "start_time": "2024-01-29T16:39:06.447661Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search google current tab\n",
      "search google scholar using clipboard new tab\n",
      "search perplexity using clipboard new tab\n",
      "search google new tab\n",
      "search new tab\n",
      "search google scholar current tab\n",
      "go to new tab\n",
      "search google scholar new tab\n",
      "search current tab\n",
      "search phind new tab\n",
      "search phind using clipboard new tab\n",
      "go to current tab\n",
      "search phind current tab\n",
      "search perplexity new tab\n",
      "search google using clipboard new tab\n",
      "search perplexity current tab\n",
      "search google using clipboard current tab\n",
      "search using clipboard new tab\n",
      "search perplexity using clipboard current tab\n",
      "none\n",
      "search phind using clipboard current tab\n",
      "search google scholar using clipboard current tab\n",
      "search using clipboard current tab\n"
     ]
    }
   ],
   "source": [
    "for cmd in validate_df.command.unique(): print( cmd ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c453baf6adf3b965",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Calculate stats on a grouped by command basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bf6af335cf491e88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-29T17:03:24.432492Z",
     "start_time": "2024-01-29T17:03:24.348918Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "cols = [ \"command\", \"response_is_exact\" ]\n",
    "stats_df = validate_df[ cols ].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2889cae7fafaabcc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-29T17:03:25.336746Z",
     "start_time": "2024-01-29T17:03:25.282918Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              command     mean  sum  count\n",
      "3                                  search current tab   99.08%  108    109\n",
      "5                               search google new tab   98.96%   95     96\n",
      "18                               search phind new tab   97.78%   44     45\n",
      "21                 search using clipboard current tab   96.77%   30     31\n",
      "19           search phind using clipboard current tab   95.83%   23     24\n",
      "9       search google scholar using clipboard new tab   94.12%   16     17\n",
      "0                                   go to current tab  100.00%  121    121\n",
      "13                      search perplexity current tab  100.00%   48     48\n",
      "20               search phind using clipboard new tab  100.00%   20     20\n",
      "17                           search phind current tab  100.00%   56     56\n",
      "16          search perplexity using clipboard new tab  100.00%   22     22\n",
      "15      search perplexity using clipboard current tab  100.00%   22     22\n",
      "14                          search perplexity new tab  100.00%   69     69\n",
      "11              search google using clipboard new tab  100.00%   14     14\n",
      "12                                     search new tab  100.00%  122    122\n",
      "1                                       go to new tab  100.00%  122    122\n",
      "10          search google using clipboard current tab  100.00%   11     11\n",
      "8   search google scholar using clipboard current tab  100.00%   26     26\n",
      "7                       search google scholar new tab  100.00%  144    144\n",
      "6                   search google scholar current tab  100.00%  144    144\n",
      "4                           search google current tab  100.00%  107    107\n",
      "2                                                none  100.00%   19     19\n",
      "22                     search using clipboard new tab  100.00%   19     19\n"
     ]
    }
   ],
   "source": [
    "# Group stats_df by command column And calculate 1) means for response_is_exact column, 2) sums for response_is_exact column, and 3) counts for response_is_exact column\n",
    "stats_df_grp = validate_df[ cols ].groupby( \"command\" )[ \"response_is_exact\" ].agg( [ \"mean\", \"sum\", \"count\" ] ).reset_index()\n",
    "stats_df_grp[ \"mean\" ] = stats_df_grp[ \"mean\" ].apply( lambda cell: f\"{ cell * 100:.2f}%\" )\n",
    "\n",
    "# Sort by response is exact ascending\n",
    "stats_df_grp = stats_df_grp.sort_values( \"mean\", ascending=False )\n",
    "\n",
    "print( stats_df_grp )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fb48024e63ddde90",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-23T21:58:41.208125Z",
     "start_time": "2024-01-23T21:58:41.108601Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>command</th>\n",
       "      <th>response_xml_is_valid</th>\n",
       "      <th>contains_response</th>\n",
       "      <th>contains_browser_command</th>\n",
       "      <th>contains_args</th>\n",
       "      <th>response_is_exact</th>\n",
       "      <th>response_has_correct_values</th>\n",
       "      <th>browser_command_is_correct</th>\n",
       "      <th>args_is_correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8539</th>\n",
       "      <td>search new tab</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13551</th>\n",
       "      <td>search phind using clipboard current tab</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14074</th>\n",
       "      <td>none</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12773</th>\n",
       "      <td>search google using clipboard new tab</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13388</th>\n",
       "      <td>search perplexity using clipboard new tab</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         command  response_xml_is_valid  \\\n",
       "8539                              search new tab                  False   \n",
       "13551   search phind using clipboard current tab                  False   \n",
       "14074                                       none                  False   \n",
       "12773      search google using clipboard new tab                  False   \n",
       "13388  search perplexity using clipboard new tab                  False   \n",
       "\n",
       "       contains_response  contains_browser_command  contains_args  \\\n",
       "8539                True                      True           True   \n",
       "13551               True                      True           True   \n",
       "14074               True                      True           True   \n",
       "12773               True                      True           True   \n",
       "13388               True                      True           True   \n",
       "\n",
       "       response_is_exact  response_has_correct_values  \\\n",
       "8539                True                         True   \n",
       "13551              False                        False   \n",
       "14074              False                        False   \n",
       "12773              False                        False   \n",
       "13388              False                        False   \n",
       "\n",
       "       browser_command_is_correct  args_is_correct  \n",
       "8539                         True             True  \n",
       "13551                       False            False  \n",
       "14074                       False            False  \n",
       "12773                       False            False  \n",
       "13388                       False            False  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = ['command', 'response_xml_is_valid', 'contains_response',\n",
    "       'contains_browser_command', 'contains_args', 'response_is_exact',\n",
    "       'response_has_correct_values', 'browser_command_is_correct',\n",
    "       'args_is_correct']\n",
    "stats_df = validate_df[ cols ]\n",
    "stats_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c71ef9c1a0544876",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-23T22:06:39.375889Z",
     "start_time": "2024-01-23T22:06:39.323163Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>command</th>\n",
       "      <th>response_is_exact</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>go to current tab</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>go to new tab</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>none</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>search current tab</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>search google current tab</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>search google new tab</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>search google scholar current tab</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>search google scholar new tab</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>search google scholar using clipboard current tab</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>search google using clipboard current tab</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>search google using clipboard new tab</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>search new tab</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>search perplexity current tab</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>search perplexity new tab</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>search perplexity using clipboard current tab</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>search perplexity using clipboard new tab</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>search phind current tab</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>search phind new tab</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>search phind using clipboard current tab</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>search phind using clipboard new tab</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>search using clipboard current tab</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>search using clipboard new tab</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              command  response_is_exact\n",
       "0                                   go to current tab                0.0\n",
       "1                                       go to new tab                0.0\n",
       "2                                                none                0.0\n",
       "3                                  search current tab                1.0\n",
       "4                           search google current tab                1.0\n",
       "5                               search google new tab                1.0\n",
       "6                   search google scholar current tab                1.0\n",
       "7                       search google scholar new tab                1.0\n",
       "8   search google scholar using clipboard current tab                0.0\n",
       "9           search google using clipboard current tab                0.0\n",
       "10              search google using clipboard new tab                0.0\n",
       "11                                     search new tab                1.0\n",
       "12                      search perplexity current tab                0.0\n",
       "13                          search perplexity new tab                0.0\n",
       "14      search perplexity using clipboard current tab                0.0\n",
       "15          search perplexity using clipboard new tab                0.0\n",
       "16                           search phind current tab                0.0\n",
       "17                               search phind new tab                0.0\n",
       "18           search phind using clipboard current tab                0.0\n",
       "19               search phind using clipboard new tab                0.0\n",
       "20                 search using clipboard current tab                0.0\n",
       "21                     search using clipboard new tab                0.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get_means_for = [ 'response_xml_is_valid', 'contains_response',\n",
    "#        'contains_browser_command', 'contains_args', 'response_is_exact',\n",
    "#        'response_has_correct_values', 'browser_command_is_correct',\n",
    "#        'args_is_correct']\n",
    "\n",
    "# Group stats_df by command column And calculate percentages for get_means_for columns\n",
    "stats_df.groupby( \"command\" )[ \"response_is_exact\" ].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319ca42267ada630",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
